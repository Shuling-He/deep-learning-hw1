{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate =  0.000001\n",
    "        self.epochs = 500\n",
    "        self.batch_size = 100\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.5  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "        \n",
    "        # calc feature-wise mean\n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        # calc feature-wise std\n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x_norm = x - mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x_norm = x_norm / std\n",
    "\n",
    "        return x_norm, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        \n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        temp = label - self.y_mean\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def train_loss(self, x, yt_sm):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.clip(np.exp(np.dot(x, self.weights)), 0, 120)\n",
    "        \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt_sm\n",
    "    \n",
    "        # calc the diff, and divide\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def test_loss(self, x, yt_sm):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        # this function adds the y mean back\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "\n",
    "        # predict\n",
    "#         pred_y = np.exp(np.dot(x, self.weights)) + self.y_mean\n",
    "        pred_y = np.clip(np.exp(np.dot(x, self.weights)), 0, 120)\n",
    "    \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # need to add the mean back to label\n",
    "#         yt = yt_sm + self.y_mean\n",
    "        yt = yt_sm\n",
    "    \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt\n",
    "        \n",
    "    \n",
    "        # calc the diff, divide, and add the mean\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "#         pred_y = np.exp(np.dot(x, self.weights))\n",
    "#         pred_y = np.clip(np.exp(np.dot(x, self.weights)), 0, 2020)\n",
    "        \n",
    "#         x_trans_dot_pred_y = np.dot(x.T, pred_y)\n",
    "        \n",
    "#         dW = x_trans_dot_pred_y - np.dot(x.T, yt_sm)\n",
    "\n",
    "        y_pred = np.clip(np.exp(np.dot(x, self.weights)), 0, 2020)\n",
    "\n",
    "        dW = np.dot(x.T, (y_pred - yt_sm).reshape(-1)).reshape(-1, 1)\n",
    "        \n",
    "#         print('dW shape: ', dW.shape)\n",
    "        \n",
    "        # return the avg dW\n",
    "        return dW / n_samples\n",
    "\n",
    "    def calc_mse(self, x, y_sm):\n",
    "        # preprocesses (adds the y_mean back to both x and y, and calls musicMSE)\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # add the y mean to the pred and convert to int to round\n",
    "#         pred_y += self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        pred_y = pred_y\n",
    "        \n",
    "        # add the y mean back to the labels\n",
    "#         y_labels = y_sm + self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        y_labels = y_sm\n",
    "        \n",
    "        # calc the MSE\n",
    "        mse = self.musicMSE(pred_y, y_labels)\n",
    "        \n",
    "        return mse, pred_y\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            \n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Epoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            test_loss = self.test_phase(x_test, y_test_sm)\n",
    "            \n",
    "            train_mse, train_preds = self.calc_mse(x_train, y_train_sm)\n",
    "            test_mse, test_preds = self.calc_mse(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "        \n",
    "#         return train_losses, test_losses\n",
    "\n",
    "        # return all the vals\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds\n",
    "    \n",
    "    def plot_graph(self, train_losses, test_losses, train_mse, test_mse):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Poisson: Loss vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse, label=\"Train MSE\")\n",
    "        plt.plot(test_mse, label=\"Test MSE\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Poisson: MSE vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_weights(self):\n",
    "        \n",
    "        plt.hist(self.weights, bins=12)\n",
    "        plt.xlabel('bins')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Lasso Regression Weights Histogram')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "# x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 500...\n",
      "train loss:  0.8919907322998095\n",
      "test loss:  0.7885432364405991\n",
      "train MSE:  119.25702413108561\n",
      "test MSE:  117.34769516358048\n",
      "Epoch 2 / 500...\n",
      "train loss:  0.6798859540525244\n",
      "test loss:  0.5840175068496358\n",
      "train MSE:  118.84966554917776\n",
      "test MSE:  116.9465707488222\n",
      "Epoch 3 / 500...\n",
      "train loss:  0.47488916057471275\n",
      "test loss:  0.3863420225871199\n",
      "train MSE:  118.45637167939202\n",
      "test MSE:  116.55962956502475\n",
      "Epoch 4 / 500...\n",
      "train loss:  0.2762670810462115\n",
      "test loss:  0.19514972013333368\n",
      "train MSE:  118.0763632534801\n",
      "test MSE:  116.18617683387501\n",
      "Epoch 5 / 500...\n",
      "train loss:  0.08421529836347495\n",
      "test loss:  0.010332549378801214\n",
      "train MSE:  117.70931296396721\n",
      "test MSE:  115.82592045765682\n",
      "Epoch 6 / 500...\n",
      "train loss:  -0.10138294926451093\n",
      "test loss:  -0.16817644270201532\n",
      "train MSE:  117.35491256917092\n",
      "test MSE:  115.4785796075101\n",
      "Epoch 7 / 500...\n",
      "train loss:  -0.28116359907335803\n",
      "test loss:  -0.3404737194022688\n",
      "train MSE:  117.01276002750171\n",
      "test MSE:  115.1438049455877\n",
      "Epoch 8 / 500...\n",
      "train loss:  -0.4550835253238772\n",
      "test loss:  -0.5065969268146799\n",
      "train MSE:  116.68261294578178\n",
      "test MSE:  114.8213588951011\n",
      "Epoch 9 / 500...\n",
      "train loss:  -0.6226094167693079\n",
      "test loss:  -0.6664990390438944\n",
      "train MSE:  116.36438319597177\n",
      "test MSE:  114.51122254932406\n",
      "Epoch 10 / 500...\n",
      "train loss:  -0.7841810425150859\n",
      "test loss:  -0.8202191565014\n",
      "train MSE:  116.05791447258686\n",
      "test MSE:  114.21323687549936\n",
      "Epoch 11 / 500...\n",
      "train loss:  -0.9401515014617559\n",
      "test loss:  -0.9678232166763565\n",
      "train MSE:  115.76302008145213\n",
      "test MSE:  113.92722629996672\n",
      "Epoch 12 / 500...\n",
      "train loss:  -1.0894647756608455\n",
      "test loss:  -1.1094175985625185\n",
      "train MSE:  115.47950377242593\n",
      "test MSE:  113.65298376111456\n",
      "Epoch 13 / 500...\n",
      "train loss:  -1.232889062967338\n",
      "test loss:  -1.2450287339776953\n",
      "train MSE:  115.2074483685359\n",
      "test MSE:  113.3906101147611\n",
      "Epoch 14 / 500...\n",
      "train loss:  -1.3708113836842521\n",
      "test loss:  -1.3749625517508999\n",
      "train MSE:  114.94635994603779\n",
      "test MSE:  113.13963437256817\n",
      "Epoch 15 / 500...\n",
      "train loss:  -1.5045258114915339\n",
      "test loss:  -1.4993842292396433\n",
      "train MSE:  114.69619665913255\n",
      "test MSE:  112.89999917273155\n",
      "Epoch 16 / 500...\n",
      "train loss:  -1.6312132255591845\n",
      "test loss:  -1.618676427425457\n",
      "train MSE:  114.45682306577089\n",
      "test MSE:  112.67148397698938\n",
      "Epoch 17 / 500...\n",
      "train loss:  -1.754204486043364\n",
      "test loss:  -1.7334742504642515\n",
      "train MSE:  114.22768936125671\n",
      "test MSE:  112.45356313726299\n",
      "Epoch 18 / 500...\n",
      "train loss:  -1.8725518682833904\n",
      "test loss:  -1.8436966537296526\n",
      "train MSE:  114.00862191389352\n",
      "test MSE:  112.24605533129994\n",
      "Epoch 19 / 500...\n",
      "train loss:  -1.987869774080705\n",
      "test loss:  -1.9497097858758803\n",
      "train MSE:  113.79906579776548\n",
      "test MSE:  112.04829304346433\n",
      "Epoch 20 / 500...\n",
      "train loss:  -2.0958659895650698\n",
      "test loss:  -2.051704894338861\n",
      "train MSE:  113.59892786551168\n",
      "test MSE:  111.86019859067918\n",
      "Epoch 21 / 500...\n",
      "train loss:  -2.2011772980162347\n",
      "test loss:  -2.1501170908362375\n",
      "train MSE:  113.40725450409441\n",
      "test MSE:  111.68071433588023\n",
      "Epoch 22 / 500...\n",
      "train loss:  -2.30419434956391\n",
      "test loss:  -2.245171356925302\n",
      "train MSE:  113.22417842394267\n",
      "test MSE:  111.50994879032564\n",
      "Epoch 23 / 500...\n",
      "train loss:  -2.402846240296554\n",
      "test loss:  -2.337183328824851\n",
      "train MSE:  113.04909030812074\n",
      "test MSE:  111.34728912813212\n",
      "Epoch 24 / 500...\n",
      "train loss:  -2.499113198381457\n",
      "test loss:  -2.4264712369869175\n",
      "train MSE:  112.88055754328049\n",
      "test MSE:  111.1911344667987\n",
      "Epoch 25 / 500...\n",
      "train loss:  -2.592441289485245\n",
      "test loss:  -2.5129530218275655\n",
      "train MSE:  112.71937492310298\n",
      "test MSE:  111.04239294410986\n",
      "Epoch 26 / 500...\n",
      "train loss:  -2.6842238908778295\n",
      "test loss:  -2.596941415477569\n",
      "train MSE:  112.56473577130981\n",
      "test MSE:  110.9001523887375\n",
      "Epoch 27 / 500...\n",
      "train loss:  -2.7719854016363565\n",
      "test loss:  -2.6786057943784685\n",
      "train MSE:  112.41602277138139\n",
      "test MSE:  110.76377354883078\n",
      "Epoch 28 / 500...\n",
      "train loss:  -2.857679972448792\n",
      "test loss:  -2.7580367409823987\n",
      "train MSE:  112.27335674291284\n",
      "test MSE:  110.63339360064234\n",
      "Epoch 29 / 500...\n",
      "train loss:  -2.9414802576774948\n",
      "test loss:  -2.835495707123533\n",
      "train MSE:  112.13584363840748\n",
      "test MSE:  110.50804617057717\n",
      "Epoch 30 / 500...\n",
      "train loss:  -3.024931872024496\n",
      "test loss:  -2.911140136410506\n",
      "train MSE:  112.00295107610039\n",
      "test MSE:  110.38715867285055\n",
      "Epoch 31 / 500...\n",
      "train loss:  -3.109341535893064\n",
      "test loss:  -2.9850940276057227\n",
      "train MSE:  111.87477773291442\n",
      "test MSE:  110.2708872450576\n",
      "Epoch 32 / 500...\n",
      "train loss:  -3.1827819817714547\n",
      "test loss:  -3.0575690260919397\n",
      "train MSE:  111.75020227332044\n",
      "test MSE:  110.15796698930752\n",
      "Epoch 33 / 500...\n",
      "train loss:  -3.260796300451018\n",
      "test loss:  -3.1285068212903946\n",
      "train MSE:  111.63021221989358\n",
      "test MSE:  110.04951981146957\n",
      "Epoch 34 / 500...\n",
      "train loss:  -3.337719143410088\n",
      "test loss:  -3.1981207223164088\n",
      "train MSE:  111.51432595507032\n",
      "test MSE:  109.94503458589512\n",
      "Epoch 35 / 500...\n",
      "train loss:  -3.410591813089586\n",
      "test loss:  -3.2663760106641724\n",
      "train MSE:  111.40197545114592\n",
      "test MSE:  109.8438926497435\n",
      "Epoch 36 / 500...\n",
      "train loss:  -3.4833131107716073\n",
      "test loss:  -3.3335892307910324\n",
      "train MSE:  111.29198326782468\n",
      "test MSE:  109.74485444214771\n",
      "Epoch 37 / 500...\n",
      "train loss:  -3.5546301083864886\n",
      "test loss:  -3.399504326501636\n",
      "train MSE:  111.18575364115398\n",
      "test MSE:  109.64945564662767\n",
      "Epoch 38 / 500...\n",
      "train loss:  -3.6249121693854587\n",
      "test loss:  -3.4645095630031784\n",
      "train MSE:  111.08188941324009\n",
      "test MSE:  109.55615459030071\n",
      "Epoch 39 / 500...\n",
      "train loss:  -3.6953232676942696\n",
      "test loss:  -3.5283619079805555\n",
      "train MSE:  110.98152335823863\n",
      "test MSE:  109.46621182711678\n",
      "Epoch 40 / 500...\n",
      "train loss:  -3.7686872301389394\n",
      "test loss:  -3.591386564722016\n",
      "train MSE:  110.88307517401469\n",
      "test MSE:  109.37799577569538\n",
      "Epoch 41 / 500...\n",
      "train loss:  -3.8295799565062745\n",
      "test loss:  -3.653428035939905\n",
      "train MSE:  110.7873161960949\n",
      "test MSE:  109.29224842384214\n",
      "Epoch 42 / 500...\n",
      "train loss:  -3.8974230733441604\n",
      "test loss:  -3.7145354089942137\n",
      "train MSE:  110.69435118626495\n",
      "test MSE:  109.20912903201685\n",
      "Epoch 43 / 500...\n",
      "train loss:  -3.969163106171453\n",
      "test loss:  -3.7749282808771323\n",
      "train MSE:  110.60325189840134\n",
      "test MSE:  109.12772381566654\n",
      "Epoch 44 / 500...\n",
      "train loss:  -4.027749796695551\n",
      "test loss:  -3.834392829700768\n",
      "train MSE:  110.51490881640572\n",
      "test MSE:  109.04890146651933\n",
      "Epoch 45 / 500...\n",
      "train loss:  -4.089059509933636\n",
      "test loss:  -3.8933376307146514\n",
      "train MSE:  110.42756565639155\n",
      "test MSE:  108.97077916422408\n",
      "Epoch 46 / 500...\n",
      "train loss:  -4.154326916706217\n",
      "test loss:  -3.9514741099031463\n",
      "train MSE:  110.34301761170104\n",
      "test MSE:  108.89536663470021\n",
      "Epoch 47 / 500...\n",
      "train loss:  -4.215930998217883\n",
      "test loss:  -4.009012658388159\n",
      "train MSE:  110.25999933969905\n",
      "test MSE:  108.82129608195706\n",
      "Epoch 48 / 500...\n",
      "train loss:  -4.276422924916193\n",
      "test loss:  -4.065724181849416\n",
      "train MSE:  110.17969254197693\n",
      "test MSE:  108.74984294610425\n",
      "Epoch 49 / 500...\n",
      "train loss:  -4.338890773903715\n",
      "test loss:  -4.121910880311183\n",
      "train MSE:  110.10076299544504\n",
      "test MSE:  108.67955766278254\n",
      "Epoch 50 / 500...\n",
      "train loss:  -4.397364884180649\n",
      "test loss:  -4.17755661908578\n",
      "train MSE:  110.02345777240784\n",
      "test MSE:  108.6107726501017\n",
      "Epoch 51 / 500...\n",
      "train loss:  -4.455461886765464\n",
      "test loss:  -4.232412396789004\n",
      "train MSE:  109.94895037535677\n",
      "test MSE:  108.54463853641558\n",
      "Epoch 52 / 500...\n",
      "train loss:  -4.514815081330975\n",
      "test loss:  -4.287035074935727\n",
      "train MSE:  109.87435131422984\n",
      "test MSE:  108.4782071032279\n",
      "Epoch 53 / 500...\n",
      "train loss:  -4.573720514292594\n",
      "test loss:  -4.341046424443383\n",
      "train MSE:  109.80172354744745\n",
      "test MSE:  108.41365887589309\n",
      "Epoch 54 / 500...\n",
      "train loss:  -4.633589225620804\n",
      "test loss:  -4.394484893989024\n",
      "train MSE:  109.73100621895573\n",
      "test MSE:  108.35093583286577\n",
      "Epoch 55 / 500...\n",
      "train loss:  -4.6891238564416415\n",
      "test loss:  -4.447367506777298\n",
      "train MSE:  109.66203405529625\n",
      "test MSE:  108.28983410388942\n",
      "Epoch 56 / 500...\n",
      "train loss:  -4.748891545165406\n",
      "test loss:  -4.499975409629587\n",
      "train MSE:  109.59374032794898\n",
      "test MSE:  108.22924179934816\n",
      "Epoch 57 / 500...\n",
      "train loss:  -4.810044869718809\n",
      "test loss:  -4.55217226970418\n",
      "train MSE:  109.52667585689787\n",
      "test MSE:  108.16979709757847\n",
      "Epoch 58 / 500...\n",
      "train loss:  -4.856756835759268\n",
      "test loss:  -4.603833556768572\n",
      "train MSE:  109.46117557472259\n",
      "test MSE:  108.11176179915988\n",
      "Epoch 59 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -4.911365048098699\n",
      "test loss:  -4.655172192445904\n",
      "train MSE:  109.39636062935386\n",
      "test MSE:  108.05420661419336\n",
      "Epoch 60 / 500...\n",
      "train loss:  -4.967561158362382\n",
      "test loss:  -4.705972745355408\n",
      "train MSE:  109.33356647394956\n",
      "test MSE:  107.99873024733894\n",
      "Epoch 61 / 500...\n",
      "train loss:  -5.0209055431655445\n",
      "test loss:  -4.756447040529365\n",
      "train MSE:  109.2717273140358\n",
      "test MSE:  107.94403562189903\n",
      "Epoch 62 / 500...\n",
      "train loss:  -5.07468057485499\n",
      "test loss:  -4.806313582856561\n",
      "train MSE:  109.21225518838854\n",
      "test MSE:  107.891747444345\n",
      "Epoch 63 / 500...\n",
      "train loss:  -5.128228483399719\n",
      "test loss:  -4.855901358137532\n",
      "train MSE:  109.1535280623313\n",
      "test MSE:  107.8400647307682\n",
      "Epoch 64 / 500...\n",
      "train loss:  -5.180556519164454\n",
      "test loss:  -4.905208075456542\n",
      "train MSE:  109.09546790981418\n",
      "test MSE:  107.78892971579332\n",
      "Epoch 65 / 500...\n",
      "train loss:  -5.231911713993515\n",
      "test loss:  -4.954200764592095\n",
      "train MSE:  109.03831952740498\n",
      "test MSE:  107.73857840504022\n",
      "Epoch 66 / 500...\n",
      "train loss:  -5.285443883473493\n",
      "test loss:  -5.0027997831389905\n",
      "train MSE:  108.98277698599749\n",
      "test MSE:  107.6898024140942\n",
      "Epoch 67 / 500...\n",
      "train loss:  -5.337766905643213\n",
      "test loss:  -5.051059758229452\n",
      "train MSE:  108.92844351698535\n",
      "test MSE:  107.64215570027481\n",
      "Epoch 68 / 500...\n",
      "train loss:  -5.388977747163237\n",
      "test loss:  -5.099118118482058\n",
      "train MSE:  108.87452074666109\n",
      "test MSE:  107.59480756409927\n",
      "Epoch 69 / 500...\n",
      "train loss:  -5.439090548018921\n",
      "test loss:  -5.1469861103736365\n",
      "train MSE:  108.82125870960279\n",
      "test MSE:  107.54802709760736\n",
      "Epoch 70 / 500...\n",
      "train loss:  -5.49009370633799\n",
      "test loss:  -5.194570887976915\n",
      "train MSE:  108.76922837496689\n",
      "test MSE:  107.50240104221864\n",
      "Epoch 71 / 500...\n",
      "train loss:  -5.542512781705767\n",
      "test loss:  -5.241755217055398\n",
      "train MSE:  108.71874931220565\n",
      "test MSE:  107.4583536070562\n",
      "Epoch 72 / 500...\n",
      "train loss:  -5.591213565818846\n",
      "test loss:  -5.288708229784099\n",
      "train MSE:  108.66903712058559\n",
      "test MSE:  107.41494603437961\n",
      "Epoch 73 / 500...\n",
      "train loss:  -5.643043798144652\n",
      "test loss:  -5.3353779373928205\n",
      "train MSE:  108.62039566872573\n",
      "test MSE:  107.37256326467919\n",
      "Epoch 74 / 500...\n",
      "train loss:  -5.6911875292313985\n",
      "test loss:  -5.381761193351978\n",
      "train MSE:  108.57277242315212\n",
      "test MSE:  107.33117389435061\n",
      "Epoch 75 / 500...\n",
      "train loss:  -5.743095913201803\n",
      "test loss:  -5.428003388135716\n",
      "train MSE:  108.52572535464842\n",
      "test MSE:  107.29029891329742\n",
      "Epoch 76 / 500...\n",
      "train loss:  -5.789887269965311\n",
      "test loss:  -5.474050027923922\n",
      "train MSE:  108.4792677488268\n",
      "test MSE:  107.24987221140654\n",
      "Epoch 77 / 500...\n",
      "train loss:  -5.839201943232115\n",
      "test loss:  -5.519787106755598\n",
      "train MSE:  108.4340083109269\n",
      "test MSE:  107.21069106271257\n",
      "Epoch 78 / 500...\n",
      "train loss:  -5.8899487962914625\n",
      "test loss:  -5.565402766479194\n",
      "train MSE:  108.38937068471415\n",
      "test MSE:  107.17200836312216\n",
      "Epoch 79 / 500...\n",
      "train loss:  -5.935669267104181\n",
      "test loss:  -5.610836917396894\n",
      "train MSE:  108.34529480297215\n",
      "test MSE:  107.13379352800554\n",
      "Epoch 80 / 500...\n",
      "train loss:  -5.986840682670718\n",
      "test loss:  -5.656045506987717\n",
      "train MSE:  108.30209073703544\n",
      "test MSE:  107.09643122245909\n",
      "Epoch 81 / 500...\n",
      "train loss:  -6.034211959195539\n",
      "test loss:  -5.700981793847857\n",
      "train MSE:  108.26004902943754\n",
      "test MSE:  107.06030546658366\n",
      "Epoch 82 / 500...\n",
      "train loss:  -6.080749164453245\n",
      "test loss:  -5.745954086659174\n",
      "train MSE:  108.21778417549488\n",
      "test MSE:  107.02370136688582\n",
      "Epoch 83 / 500...\n",
      "train loss:  -6.130843632736738\n",
      "test loss:  -5.79054616004982\n",
      "train MSE:  108.17704743468825\n",
      "test MSE:  106.98874147087342\n",
      "Epoch 84 / 500...\n",
      "train loss:  -6.176819910922614\n",
      "test loss:  -5.834954395511597\n",
      "train MSE:  108.13703710327366\n",
      "test MSE:  106.95442626020619\n",
      "Epoch 85 / 500...\n",
      "train loss:  -6.223656354018897\n",
      "test loss:  -5.8790752561439295\n",
      "train MSE:  108.09811800589618\n",
      "test MSE:  106.92118603890874\n",
      "Epoch 86 / 500...\n",
      "train loss:  -6.271657252147487\n",
      "test loss:  -5.9231890064879975\n",
      "train MSE:  108.05914885131371\n",
      "test MSE:  106.88774882489965\n",
      "Epoch 87 / 500...\n",
      "train loss:  -6.3179688193777075\n",
      "test loss:  -5.966879155129865\n",
      "train MSE:  108.02200120415101\n",
      "test MSE:  106.85624872188001\n",
      "Epoch 88 / 500...\n",
      "train loss:  -6.362999180083091\n",
      "test loss:  -6.010603456576148\n",
      "train MSE:  107.98453009154754\n",
      "test MSE:  106.82421897277723\n",
      "Epoch 89 / 500...\n",
      "train loss:  -6.4101077400714805\n",
      "test loss:  -6.054068579085567\n",
      "train MSE:  107.94808409807295\n",
      "test MSE:  106.79325473282415\n",
      "Epoch 90 / 500...\n",
      "train loss:  -6.455135868524613\n",
      "test loss:  -6.097426598913207\n",
      "train MSE:  107.9119772343297\n",
      "test MSE:  106.76254256450157\n",
      "Epoch 91 / 500...\n",
      "train loss:  -6.502371024729579\n",
      "test loss:  -6.140811641301485\n",
      "train MSE:  107.87574594324052\n",
      "test MSE:  106.73154627230467\n",
      "Epoch 92 / 500...\n",
      "train loss:  -6.548616011819312\n",
      "test loss:  -6.184095281566599\n",
      "train MSE:  107.83987148236417\n",
      "test MSE:  106.70083679903189\n",
      "Epoch 93 / 500...\n",
      "train loss:  -6.594605213513271\n",
      "test loss:  -6.226903862078417\n",
      "train MSE:  107.80599409416313\n",
      "test MSE:  106.67231193403931\n",
      "Epoch 94 / 500...\n",
      "train loss:  -6.63937806987625\n",
      "test loss:  -6.269561390635773\n",
      "train MSE:  107.77278030922571\n",
      "test MSE:  106.64438469180355\n",
      "Epoch 95 / 500...\n",
      "train loss:  -6.68557204606836\n",
      "test loss:  -6.3120748848818975\n",
      "train MSE:  107.74024443808754\n",
      "test MSE:  106.61711507681007\n",
      "Epoch 96 / 500...\n",
      "train loss:  -6.728935548128958\n",
      "test loss:  -6.354793573974891\n",
      "train MSE:  107.70681380205082\n",
      "test MSE:  106.58873546312435\n",
      "Epoch 97 / 500...\n",
      "train loss:  -6.777271568304118\n",
      "test loss:  -6.396972215001107\n",
      "train MSE:  107.67571861071984\n",
      "test MSE:  106.56289296020826\n",
      "Epoch 98 / 500...\n",
      "train loss:  -6.822382465619934\n",
      "test loss:  -6.439087823109478\n",
      "train MSE:  107.64487435516006\n",
      "test MSE:  106.53722130083517\n",
      "Epoch 99 / 500...\n",
      "train loss:  -6.865463762478247\n",
      "test loss:  -6.481151308057392\n",
      "train MSE:  107.61417726039696\n",
      "test MSE:  106.51159851247688\n",
      "Epoch 100 / 500...\n",
      "train loss:  -6.910399582061697\n",
      "test loss:  -6.523317879093185\n",
      "train MSE:  107.58303097060497\n",
      "test MSE:  106.48533609020431\n",
      "Epoch 101 / 500...\n",
      "train loss:  -6.95351173766875\n",
      "test loss:  -6.564892116565765\n",
      "train MSE:  107.55439787703529\n",
      "test MSE:  106.46189208973998\n",
      "Epoch 102 / 500...\n",
      "train loss:  -7.001244741210695\n",
      "test loss:  -6.606502054555031\n",
      "train MSE:  107.52576717948685\n",
      "test MSE:  106.43831062707943\n",
      "Epoch 103 / 500...\n",
      "train loss:  -7.047095343553467\n",
      "test loss:  -6.648010707717647\n",
      "train MSE:  107.49754965650425\n",
      "test MSE:  106.41506962239464\n",
      "Epoch 104 / 500...\n",
      "train loss:  -7.085812506874973\n",
      "test loss:  -6.689571827987802\n",
      "train MSE:  107.46902424489113\n",
      "test MSE:  106.39138818494408\n",
      "Epoch 105 / 500...\n",
      "train loss:  -7.1312243679060225\n",
      "test loss:  -6.731135062602355\n",
      "train MSE:  107.44065652191372\n",
      "test MSE:  106.36778815520707\n",
      "Epoch 106 / 500...\n",
      "train loss:  -7.175085145325703\n",
      "test loss:  -6.772212342721809\n",
      "train MSE:  107.41429142488701\n",
      "test MSE:  106.34636701822625\n",
      "Epoch 107 / 500...\n",
      "train loss:  -7.218021282058151\n",
      "test loss:  -6.8131172769358415\n",
      "train MSE:  107.38862502080954\n",
      "test MSE:  106.3256860863491\n",
      "Epoch 108 / 500...\n",
      "train loss:  -7.263761176516794\n",
      "test loss:  -6.854089504586583\n",
      "train MSE:  107.36266368798138\n",
      "test MSE:  106.30456347087956\n",
      "Epoch 109 / 500...\n",
      "train loss:  -7.306294946961182\n",
      "test loss:  -6.894763438154927\n",
      "train MSE:  107.3379162762994\n",
      "test MSE:  106.28472560042839\n",
      "Epoch 110 / 500...\n",
      "train loss:  -7.347754790129386\n",
      "test loss:  -6.935684318225567\n",
      "train MSE:  107.31236839888359\n",
      "test MSE:  106.26385247694284\n",
      "Epoch 111 / 500...\n",
      "train loss:  -7.393519247478618\n",
      "test loss:  -6.976268899165891\n",
      "train MSE:  107.28828027808318\n",
      "test MSE:  106.2445497519686\n",
      "Epoch 112 / 500...\n",
      "train loss:  -7.434864612797018\n",
      "test loss:  -7.016663113967163\n",
      "train MSE:  107.26491354966085\n",
      "test MSE:  106.22597657459389\n",
      "Epoch 113 / 500...\n",
      "train loss:  -7.477172304834815\n",
      "test loss:  -7.057035779152157\n",
      "train MSE:  107.24171831386934\n",
      "test MSE:  106.20750663642113\n",
      "Epoch 114 / 500...\n",
      "train loss:  -7.52261760483293\n",
      "test loss:  -7.0973202458985005\n",
      "train MSE:  107.21902461421696\n",
      "test MSE:  106.18951274832361\n",
      "Epoch 115 / 500...\n",
      "train loss:  -7.565463544476131\n",
      "test loss:  -7.137514440465692\n",
      "train MSE:  107.19689678460298\n",
      "test MSE:  106.17207642029541\n",
      "Epoch 116 / 500...\n",
      "train loss:  -7.605305595823015\n",
      "test loss:  -7.177574702080743\n",
      "train MSE:  107.17522325479148\n",
      "test MSE:  106.15506776379843\n",
      "Epoch 117 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -7.649932109963921\n",
      "test loss:  -7.217603505660587\n",
      "train MSE:  107.15381801082427\n",
      "test MSE:  106.13827789396191\n",
      "Epoch 118 / 500...\n",
      "train loss:  -7.69045726560203\n",
      "test loss:  -7.257473207065319\n",
      "train MSE:  107.13317923132192\n",
      "test MSE:  106.12230344490483\n",
      "Epoch 119 / 500...\n",
      "train loss:  -7.7336838517773545\n",
      "test loss:  -7.297030152358147\n",
      "train MSE:  107.11370671345799\n",
      "test MSE:  106.10756513749486\n",
      "Epoch 120 / 500...\n",
      "train loss:  -7.776245757861539\n",
      "test loss:  -7.3365039837587265\n",
      "train MSE:  107.09464145099801\n",
      "test MSE:  106.09317616601538\n",
      "Epoch 121 / 500...\n",
      "train loss:  -7.817444028557911\n",
      "test loss:  -7.3763232070361004\n",
      "train MSE:  107.07420832436978\n",
      "test MSE:  106.07711302395452\n",
      "Epoch 122 / 500...\n",
      "train loss:  -7.857394721940592\n",
      "test loss:  -7.415905087121069\n",
      "train MSE:  107.0547839979991\n",
      "test MSE:  106.06211479820475\n",
      "Epoch 123 / 500...\n",
      "train loss:  -7.90006243269593\n",
      "test loss:  -7.455286364940039\n",
      "train MSE:  107.03618849981815\n",
      "test MSE:  106.04801299308762\n",
      "Epoch 124 / 500...\n",
      "train loss:  -7.944057991596902\n",
      "test loss:  -7.494565499893788\n",
      "train MSE:  107.01815553364672\n",
      "test MSE:  106.03447098767685\n",
      "Epoch 125 / 500...\n",
      "train loss:  -7.989941828041666\n",
      "test loss:  -7.5337080221701225\n",
      "train MSE:  107.00089200221547\n",
      "test MSE:  106.02175970867133\n",
      "Epoch 126 / 500...\n",
      "train loss:  -8.025395266758172\n",
      "test loss:  -7.572710063264131\n",
      "train MSE:  106.98413451149555\n",
      "test MSE:  106.00952330635998\n",
      "Epoch 127 / 500...\n",
      "train loss:  -8.065903334011205\n",
      "test loss:  -7.611645058195482\n",
      "train MSE:  106.96763993873073\n",
      "test MSE:  105.99752378082657\n",
      "Epoch 128 / 500...\n",
      "train loss:  -8.107301994108857\n",
      "test loss:  -7.650267554729887\n",
      "train MSE:  106.95243808782553\n",
      "test MSE:  105.98684911568209\n",
      "Epoch 129 / 500...\n",
      "train loss:  -8.147237137964755\n",
      "test loss:  -7.689078977919395\n",
      "train MSE:  106.93662107783031\n",
      "test MSE:  105.97542895481347\n",
      "Epoch 130 / 500...\n",
      "train loss:  -8.191975031544455\n",
      "test loss:  -7.727686431225452\n",
      "train MSE:  106.92195374786537\n",
      "test MSE:  105.96529912653364\n",
      "Epoch 131 / 500...\n",
      "train loss:  -8.232068196429298\n",
      "test loss:  -7.766059831255368\n",
      "train MSE:  106.90822101968716\n",
      "test MSE:  105.95610409778973\n",
      "Epoch 132 / 500...\n",
      "train loss:  -8.271471852305432\n",
      "test loss:  -7.804484452888487\n",
      "train MSE:  106.89439531762909\n",
      "test MSE:  105.94671167455762\n",
      "Epoch 133 / 500...\n",
      "train loss:  -8.31214972468486\n",
      "test loss:  -7.843006198032217\n",
      "train MSE:  106.88035795182468\n",
      "test MSE:  105.9369783550914\n",
      "Epoch 134 / 500...\n",
      "train loss:  -8.349261314470388\n",
      "test loss:  -7.881377523621034\n",
      "train MSE:  106.86699097430618\n",
      "test MSE:  105.92790178357792\n",
      "Epoch 135 / 500...\n",
      "train loss:  -8.393967272920147\n",
      "test loss:  -7.919534369124685\n",
      "train MSE:  106.8546510007558\n",
      "test MSE:  105.91998003404026\n",
      "Epoch 136 / 500...\n",
      "train loss:  -8.436336103708554\n",
      "test loss:  -7.9576250072992964\n",
      "train MSE:  106.84272277816994\n",
      "test MSE:  105.91248958011903\n",
      "Epoch 137 / 500...\n",
      "train loss:  -8.474162359768993\n",
      "test loss:  -7.995897157898934\n",
      "train MSE:  106.83007534703782\n",
      "test MSE:  105.90406511733225\n",
      "Epoch 138 / 500...\n",
      "train loss:  -8.512404333130583\n",
      "test loss:  -8.03387607987976\n",
      "train MSE:  106.81863766918445\n",
      "test MSE:  105.89695056925285\n",
      "Epoch 139 / 500...\n",
      "train loss:  -8.556931475782072\n",
      "test loss:  -8.072150070457\n",
      "train MSE:  106.80642604843338\n",
      "test MSE:  105.88893494053976\n",
      "Epoch 140 / 500...\n",
      "train loss:  -8.596736484560717\n",
      "test loss:  -8.109862585329562\n",
      "train MSE:  106.79630956163736\n",
      "test MSE:  105.88314062644024\n",
      "Epoch 141 / 500...\n",
      "train loss:  -8.63881954209117\n",
      "test loss:  -8.147787861673752\n",
      "train MSE:  106.78551643114321\n",
      "test MSE:  105.87656859098591\n",
      "Epoch 142 / 500...\n",
      "train loss:  -8.673592055890417\n",
      "test loss:  -8.185537846347106\n",
      "train MSE:  106.77542497680459\n",
      "test MSE:  105.87072652186723\n",
      "Epoch 143 / 500...\n",
      "train loss:  -8.718555084562494\n",
      "test loss:  -8.223288280822796\n",
      "train MSE:  106.76560633986111\n",
      "test MSE:  105.86508306486078\n",
      "Epoch 144 / 500...\n",
      "train loss:  -8.763128185378422\n",
      "test loss:  -8.260712635943522\n",
      "train MSE:  106.75729942483974\n",
      "test MSE:  105.86113296764894\n",
      "Epoch 145 / 500...\n",
      "train loss:  -8.798388634806297\n",
      "test loss:  -8.297881960031903\n",
      "train MSE:  106.74986201779919\n",
      "test MSE:  105.85806659389273\n",
      "Epoch 146 / 500...\n",
      "train loss:  -8.832268214832228\n",
      "test loss:  -8.335395288949538\n",
      "train MSE:  106.74106113177122\n",
      "test MSE:  105.85335971643632\n",
      "Epoch 147 / 500...\n",
      "train loss:  -8.870680199397789\n",
      "test loss:  -8.372617220694098\n",
      "train MSE:  106.73338767309788\n",
      "test MSE:  105.84983682483394\n",
      "Epoch 148 / 500...\n",
      "train loss:  -8.913817451051232\n",
      "test loss:  -8.40987568774682\n",
      "train MSE:  106.72589397999002\n",
      "test MSE:  105.84649970856913\n",
      "Epoch 149 / 500...\n",
      "train loss:  -8.952319126804053\n",
      "test loss:  -8.447119023412782\n",
      "train MSE:  106.71860332659782\n",
      "test MSE:  105.84328749949734\n",
      "Epoch 150 / 500...\n",
      "train loss:  -8.993820279376747\n",
      "test loss:  -8.484151685305676\n",
      "train MSE:  106.71228103114619\n",
      "test MSE:  105.84116128815222\n",
      "Epoch 151 / 500...\n",
      "train loss:  -9.028311024692862\n",
      "test loss:  -8.52139292615737\n",
      "train MSE:  106.7050556234333\n",
      "test MSE:  105.83790205976547\n",
      "Epoch 152 / 500...\n",
      "train loss:  -9.068287583102366\n",
      "test loss:  -8.55863267052711\n",
      "train MSE:  106.69814033345675\n",
      "test MSE:  105.83493400788366\n",
      "Epoch 153 / 500...\n",
      "train loss:  -9.1096675583628\n",
      "test loss:  -8.595552691149345\n",
      "train MSE:  106.69263925364375\n",
      "test MSE:  105.83354118589115\n",
      "Epoch 154 / 500...\n",
      "train loss:  -9.147159589359418\n",
      "test loss:  -8.632420750230834\n",
      "train MSE:  106.6873533818352\n",
      "test MSE:  105.83232744356108\n",
      "Epoch 155 / 500...\n",
      "train loss:  -9.187205564372743\n",
      "test loss:  -8.66938215342449\n",
      "train MSE:  106.68193713704673\n",
      "test MSE:  105.83089937199186\n",
      "Epoch 156 / 500...\n",
      "train loss:  -9.225500530643805\n",
      "test loss:  -8.706112319839884\n",
      "train MSE:  106.67752209805231\n",
      "test MSE:  105.83050715232021\n",
      "Epoch 157 / 500...\n",
      "train loss:  -9.264862023883227\n",
      "test loss:  -8.74264092825783\n",
      "train MSE:  106.67382308209969\n",
      "test MSE:  105.8308734397376\n",
      "Epoch 158 / 500...\n",
      "train loss:  -9.30282104977637\n",
      "test loss:  -8.779253904237999\n",
      "train MSE:  106.66996459878413\n",
      "test MSE:  105.83101652398526\n",
      "Epoch 159 / 500...\n",
      "train loss:  -9.341709193526091\n",
      "test loss:  -8.815466161180028\n",
      "train MSE:  106.66779853968136\n",
      "test MSE:  105.8329918240249\n",
      "Epoch 160 / 500...\n",
      "train loss:  -9.384413498832838\n",
      "test loss:  -8.85222562644806\n",
      "train MSE:  106.66386441069268\n",
      "test MSE:  105.83296238420274\n",
      "Epoch 161 / 500...\n",
      "train loss:  -9.420706837625712\n",
      "test loss:  -8.8886888828377\n",
      "train MSE:  106.66115765302074\n",
      "test MSE:  105.83423274955413\n",
      "Epoch 162 / 500...\n",
      "train loss:  -9.455933863197782\n",
      "test loss:  -8.924942783317283\n",
      "train MSE:  106.65931391496046\n",
      "test MSE:  105.83641411591476\n",
      "Epoch 163 / 500...\n",
      "train loss:  -9.493345818790019\n",
      "test loss:  -8.961080593150095\n",
      "train MSE:  106.65801664755286\n",
      "test MSE:  105.83914344941\n",
      "Epoch 164 / 500...\n",
      "train loss:  -9.535371687856612\n",
      "test loss:  -8.997040364068264\n",
      "train MSE:  106.65759482065548\n",
      "test MSE:  105.84285397418499\n",
      "Epoch 165 / 500...\n",
      "train loss:  -9.569830474196083\n",
      "test loss:  -9.033345739686103\n",
      "train MSE:  106.6558167716647\n",
      "test MSE:  105.84488279880574\n",
      "Epoch 166 / 500...\n",
      "train loss:  -9.604550741937729\n",
      "test loss:  -9.069648569426262\n",
      "train MSE:  106.65412679588702\n",
      "test MSE:  105.84694885147722\n",
      "Epoch 167 / 500...\n",
      "train loss:  -9.647772047319247\n",
      "test loss:  -9.105639636197834\n",
      "train MSE:  106.6541329285632\n",
      "test MSE:  105.85093280116129\n",
      "Epoch 168 / 500...\n",
      "train loss:  -9.685080813027911\n",
      "test loss:  -9.141570901417674\n",
      "train MSE:  106.65423714143903\n",
      "test MSE:  105.85491179719745\n",
      "Epoch 169 / 500...\n",
      "train loss:  -9.723449099335953\n",
      "test loss:  -9.177424727791347\n",
      "train MSE:  106.65475157967143\n",
      "test MSE:  105.85932288190897\n",
      "Epoch 170 / 500...\n",
      "train loss:  -9.761799785955624\n",
      "test loss:  -9.21344970473087\n",
      "train MSE:  106.65478523790773\n",
      "test MSE:  105.86314577486884\n",
      "Epoch 171 / 500...\n",
      "train loss:  -9.800283089610112\n",
      "test loss:  -9.249191597064922\n",
      "train MSE:  106.65618420612476\n",
      "test MSE:  105.86847982444777\n",
      "Epoch 172 / 500...\n",
      "train loss:  -9.836247327723012\n",
      "test loss:  -9.284888228469583\n",
      "train MSE:  106.65774260340415\n",
      "test MSE:  105.87390407347259\n",
      "Epoch 173 / 500...\n",
      "train loss:  -9.870450663992319\n",
      "test loss:  -9.320930535721857\n",
      "train MSE:  106.6581054520903\n",
      "test MSE:  105.87790817608357\n",
      "Epoch 174 / 500...\n",
      "train loss:  -9.914229134240259\n",
      "test loss:  -9.35629450926895\n",
      "train MSE:  106.66126460310282\n",
      "test MSE:  105.88505527697342\n",
      "Epoch 175 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -9.949382549645668\n",
      "test loss:  -9.391815579240275\n",
      "train MSE:  106.66388860365\n",
      "test MSE:  105.89153380894417\n",
      "Epoch 176 / 500...\n",
      "train loss:  -9.987833542727662\n",
      "test loss:  -9.427340018248255\n",
      "train MSE:  106.66655196567694\n",
      "test MSE:  105.89797254090662\n",
      "Epoch 177 / 500...\n",
      "train loss:  -10.02328109598414\n",
      "test loss:  -9.462696906145178\n",
      "train MSE:  106.66996922531933\n",
      "test MSE:  105.90520455684174\n",
      "Epoch 178 / 500...\n",
      "train loss:  -10.062701440238818\n",
      "test loss:  -9.498014104639438\n",
      "train MSE:  106.67380858505044\n",
      "test MSE:  105.91286000343156\n",
      "Epoch 179 / 500...\n",
      "train loss:  -10.09943858391797\n",
      "test loss:  -9.533496073931817\n",
      "train MSE:  106.67714701455512\n",
      "test MSE:  105.9198957391041\n",
      "Epoch 180 / 500...\n",
      "train loss:  -10.139008284459559\n",
      "test loss:  -9.568831971028029\n",
      "train MSE:  106.68130525594543\n",
      "test MSE:  105.92780267099323\n",
      "Epoch 181 / 500...\n",
      "train loss:  -10.17756644919314\n",
      "test loss:  -9.604048031513054\n",
      "train MSE:  106.68603619971975\n",
      "test MSE:  105.93628691619516\n",
      "Epoch 182 / 500...\n",
      "train loss:  -10.212949877877262\n",
      "test loss:  -9.639092259829367\n",
      "train MSE:  106.69128693739263\n",
      "test MSE:  105.94528906588039\n",
      "Epoch 183 / 500...\n",
      "train loss:  -10.247583599651378\n",
      "test loss:  -9.674058545925234\n",
      "train MSE:  106.69714134401926\n",
      "test MSE:  105.95495278135054\n",
      "Epoch 184 / 500...\n",
      "train loss:  -10.282169936689693\n",
      "test loss:  -9.709337754467622\n",
      "train MSE:  106.70180937766793\n",
      "test MSE:  105.96317100220723\n",
      "Epoch 185 / 500...\n",
      "train loss:  -10.320021748443022\n",
      "test loss:  -9.744459293562441\n",
      "train MSE:  106.70720469013541\n",
      "test MSE:  105.97216911988616\n",
      "Epoch 186 / 500...\n",
      "train loss:  -10.356121384880954\n",
      "test loss:  -9.779316052587465\n",
      "train MSE:  106.71394649476053\n",
      "test MSE:  105.982688051808\n",
      "Epoch 187 / 500...\n",
      "train loss:  -10.396471043045034\n",
      "test loss:  -9.814152268411481\n",
      "train MSE:  106.72090305744098\n",
      "test MSE:  105.9933883444138\n",
      "Epoch 188 / 500...\n",
      "train loss:  -10.433181453806743\n",
      "test loss:  -9.848906527074487\n",
      "train MSE:  106.72837400054968\n",
      "test MSE:  106.00464448587212\n",
      "Epoch 189 / 500...\n",
      "train loss:  -10.467949046226169\n",
      "test loss:  -9.883627761180472\n",
      "train MSE:  106.73602133389957\n",
      "test MSE:  106.01603361361832\n",
      "Epoch 190 / 500...\n",
      "train loss:  -10.506462947327469\n",
      "test loss:  -9.91853575744181\n",
      "train MSE:  106.74287325612138\n",
      "test MSE:  106.02639087835554\n",
      "Epoch 191 / 500...\n",
      "train loss:  -10.538730282519191\n",
      "test loss:  -9.953285581716528\n",
      "train MSE:  106.75066676997795\n",
      "test MSE:  106.0378327730851\n",
      "Epoch 192 / 500...\n",
      "train loss:  -10.580776800283745\n",
      "test loss:  -9.98772260310138\n",
      "train MSE:  106.75989080783738\n",
      "test MSE:  106.05084923173975\n",
      "Epoch 193 / 500...\n",
      "train loss:  -10.612868956212221\n",
      "test loss:  -10.022186668561881\n",
      "train MSE:  106.76899871796887\n",
      "test MSE:  106.0636579410152\n",
      "Epoch 194 / 500...\n",
      "train loss:  -10.652832945741066\n",
      "test loss:  -10.056408126361148\n",
      "train MSE:  106.77891964739264\n",
      "test MSE:  106.0772879288699\n",
      "Epoch 195 / 500...\n",
      "train loss:  -10.687343392315961\n",
      "test loss:  -10.090908839004072\n",
      "train MSE:  106.78812872454493\n",
      "test MSE:  106.09008221203655\n",
      "Epoch 196 / 500...\n",
      "train loss:  -10.72187379761117\n",
      "test loss:  -10.125165865077864\n",
      "train MSE:  106.79823607835439\n",
      "test MSE:  106.10382854264684\n",
      "Epoch 197 / 500...\n",
      "train loss:  -10.758824666660265\n",
      "test loss:  -10.159700399158853\n",
      "train MSE:  106.80763617540856\n",
      "test MSE:  106.11674008099696\n",
      "Epoch 198 / 500...\n",
      "train loss:  -10.808801951170596\n",
      "test loss:  -10.194059004363078\n",
      "train MSE:  106.81810867505274\n",
      "test MSE:  106.13083974057068\n",
      "Epoch 199 / 500...\n",
      "train loss:  -10.834171021332285\n",
      "test loss:  -10.228107339943733\n",
      "train MSE:  106.82964709691657\n",
      "test MSE:  106.1461055305311\n",
      "Epoch 200 / 500...\n",
      "train loss:  -10.867598229533447\n",
      "test loss:  -10.262033457742774\n",
      "train MSE:  106.84167303487641\n",
      "test MSE:  106.16181845359127\n",
      "Epoch 201 / 500...\n",
      "train loss:  -10.901958704117837\n",
      "test loss:  -10.29617537958499\n",
      "train MSE:  106.85306533458493\n",
      "test MSE:  106.17680377196533\n",
      "Epoch 202 / 500...\n",
      "train loss:  -10.940902210974361\n",
      "test loss:  -10.330128083635094\n",
      "train MSE:  106.86529946371677\n",
      "test MSE:  106.19268816435512\n",
      "Epoch 203 / 500...\n",
      "train loss:  -10.979628898653617\n",
      "test loss:  -10.364191811642202\n",
      "train MSE:  106.87706865757555\n",
      "test MSE:  106.20793796374\n",
      "Epoch 204 / 500...\n",
      "train loss:  -11.012168372416264\n",
      "test loss:  -10.398061805904467\n",
      "train MSE:  106.89015526576092\n",
      "test MSE:  106.22468481596842\n",
      "Epoch 205 / 500...\n",
      "train loss:  -11.047538173710338\n",
      "test loss:  -10.43210896779811\n",
      "train MSE:  106.90261898433309\n",
      "test MSE:  106.2406683762516\n",
      "Epoch 206 / 500...\n",
      "train loss:  -11.088371916678094\n",
      "test loss:  -10.46613587349416\n",
      "train MSE:  106.91520922995177\n",
      "test MSE:  106.25671195928557\n",
      "Epoch 207 / 500...\n",
      "train loss:  -11.119460743232347\n",
      "test loss:  -10.50025214014919\n",
      "train MSE:  106.92754428543974\n",
      "test MSE:  106.27240837390329\n",
      "Epoch 208 / 500...\n",
      "train loss:  -11.156821051751715\n",
      "test loss:  -10.533979826796928\n",
      "train MSE:  106.9414075160512\n",
      "test MSE:  106.2897958820621\n",
      "Epoch 209 / 500...\n",
      "train loss:  -11.187641279575837\n",
      "test loss:  -10.56797035837344\n",
      "train MSE:  106.95473033980055\n",
      "test MSE:  106.30654569461853\n",
      "Epoch 210 / 500...\n",
      "train loss:  -11.22841983492648\n",
      "test loss:  -10.601644829389196\n",
      "train MSE:  106.96929905431135\n",
      "test MSE:  106.32465425863487\n",
      "Epoch 211 / 500...\n",
      "train loss:  -11.262263387361882\n",
      "test loss:  -10.635446931719756\n",
      "train MSE:  106.9833982324525\n",
      "test MSE:  106.34214550016117\n",
      "Epoch 212 / 500...\n",
      "train loss:  -11.29854749176212\n",
      "test loss:  -10.669071391563238\n",
      "train MSE:  106.9985011490234\n",
      "test MSE:  106.36077579876608\n",
      "Epoch 213 / 500...\n",
      "train loss:  -11.333432595994582\n",
      "test loss:  -10.702793683219168\n",
      "train MSE:  107.01345716431321\n",
      "test MSE:  106.37916071214067\n",
      "Epoch 214 / 500...\n",
      "train loss:  -11.368126393634862\n",
      "test loss:  -10.73653749745677\n",
      "train MSE:  107.02860274444933\n",
      "test MSE:  106.39771745871955\n",
      "Epoch 215 / 500...\n",
      "train loss:  -11.409804890115527\n",
      "test loss:  -10.769689124890977\n",
      "train MSE:  107.04569198434838\n",
      "test MSE:  106.41839624323941\n",
      "Epoch 216 / 500...\n",
      "train loss:  -11.441481852577569\n",
      "test loss:  -10.803003559189433\n",
      "train MSE:  107.06232524112174\n",
      "test MSE:  106.43853452906036\n",
      "Epoch 217 / 500...\n",
      "train loss:  -11.477641973308925\n",
      "test loss:  -10.836248919918013\n",
      "train MSE:  107.07965977317077\n",
      "test MSE:  106.45946772658567\n",
      "Epoch 218 / 500...\n",
      "train loss:  -11.510676353931496\n",
      "test loss:  -10.869874649659577\n",
      "train MSE:  107.09550622377341\n",
      "test MSE:  106.47858835200562\n",
      "Epoch 219 / 500...\n",
      "train loss:  -11.546411025245591\n",
      "test loss:  -10.903236706170503\n",
      "train MSE:  107.11265909573844\n",
      "test MSE:  106.49918382112418\n",
      "Epoch 220 / 500...\n",
      "train loss:  -11.581296040358144\n",
      "test loss:  -10.936519082184473\n",
      "train MSE:  107.13022997382278\n",
      "test MSE:  106.52019362036738\n",
      "Epoch 221 / 500...\n",
      "train loss:  -11.615283725558445\n",
      "test loss:  -10.969515761549777\n",
      "train MSE:  107.14897280143133\n",
      "test MSE:  106.54251960988341\n",
      "Epoch 222 / 500...\n",
      "train loss:  -11.65155844108198\n",
      "test loss:  -11.002665128827452\n",
      "train MSE:  107.16750782728754\n",
      "test MSE:  106.56455532705277\n",
      "Epoch 223 / 500...\n",
      "train loss:  -11.686150943443696\n",
      "test loss:  -11.036122663441205\n",
      "train MSE:  107.18498456697127\n",
      "test MSE:  106.5853134317604\n",
      "Epoch 224 / 500...\n",
      "train loss:  -11.719192112100172\n",
      "test loss:  -11.069224183661584\n",
      "train MSE:  107.20346363867066\n",
      "test MSE:  106.60711113559644\n",
      "Epoch 225 / 500...\n",
      "train loss:  -11.754924490719224\n",
      "test loss:  -11.102252128208612\n",
      "train MSE:  107.22265647381177\n",
      "test MSE:  106.62971559297242\n",
      "Epoch 226 / 500...\n",
      "train loss:  -11.790826850267123\n",
      "test loss:  -11.135311587086612\n",
      "train MSE:  107.24208593443242\n",
      "test MSE:  106.65258290190178\n",
      "Epoch 227 / 500...\n",
      "train loss:  -11.825885426393109\n",
      "test loss:  -11.168002927014498\n",
      "train MSE:  107.26289969844667\n",
      "test MSE:  106.67694487397527\n",
      "Epoch 228 / 500...\n",
      "train loss:  -11.856289777449923\n",
      "test loss:  -11.201367508029616\n",
      "train MSE:  107.28098905267575\n",
      "test MSE:  106.69809969696361\n",
      "Epoch 229 / 500...\n",
      "train loss:  -11.897975482777271\n",
      "test loss:  -11.23429360536184\n",
      "train MSE:  107.30107216279426\n",
      "test MSE:  106.7215002954518\n",
      "Epoch 230 / 500...\n",
      "train loss:  -11.927973415767688\n",
      "test loss:  -11.26729325481279\n",
      "train MSE:  107.32116322179016\n",
      "test MSE:  106.74486836163696\n",
      "Epoch 231 / 500...\n",
      "train loss:  -11.963769518809098\n",
      "test loss:  -11.300127322489548\n",
      "train MSE:  107.34184772195236\n",
      "test MSE:  106.76886383435154\n",
      "Epoch 232 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -11.99740472812678\n",
      "test loss:  -11.33292810604139\n",
      "train MSE:  107.36276677421694\n",
      "test MSE:  106.79305676202371\n",
      "Epoch 233 / 500...\n",
      "train loss:  -12.03253725398351\n",
      "test loss:  -11.365482836107423\n",
      "train MSE:  107.38496939503246\n",
      "test MSE:  106.81871358261914\n",
      "Epoch 234 / 500...\n",
      "train loss:  -12.067152305290433\n",
      "test loss:  -11.398218596466513\n",
      "train MSE:  107.40646513505565\n",
      "test MSE:  106.84347336049659\n",
      "Epoch 235 / 500...\n",
      "train loss:  -12.10445390005668\n",
      "test loss:  -11.430726721356773\n",
      "train MSE:  107.4288771375259\n",
      "test MSE:  106.86920976057226\n",
      "Epoch 236 / 500...\n",
      "train loss:  -12.134278270970187\n",
      "test loss:  -11.463324816549765\n",
      "train MSE:  107.45141077758547\n",
      "test MSE:  106.89511038107999\n",
      "Epoch 237 / 500...\n",
      "train loss:  -12.170036283420067\n",
      "test loss:  -11.495719925885027\n",
      "train MSE:  107.47493776112582\n",
      "test MSE:  106.92208621225691\n",
      "Epoch 238 / 500...\n",
      "train loss:  -12.205530865786434\n",
      "test loss:  -11.528154603310735\n",
      "train MSE:  107.49821391699567\n",
      "test MSE:  106.94872547674805\n",
      "Epoch 239 / 500...\n",
      "train loss:  -12.236806281595053\n",
      "test loss:  -11.56085303208507\n",
      "train MSE:  107.52056555243188\n",
      "test MSE:  106.97421616064422\n",
      "Epoch 240 / 500...\n",
      "train loss:  -12.273037501789238\n",
      "test loss:  -11.593237598380712\n",
      "train MSE:  107.54445271700196\n",
      "test MSE:  107.00144390470848\n",
      "Epoch 241 / 500...\n",
      "train loss:  -12.305621043287605\n",
      "test loss:  -11.625621017784637\n",
      "train MSE:  107.56811534597749\n",
      "test MSE:  107.02832458915692\n",
      "Epoch 242 / 500...\n",
      "train loss:  -12.347681856101145\n",
      "test loss:  -11.657777817620103\n",
      "train MSE:  107.5931419768758\n",
      "test MSE:  107.0567531472036\n",
      "Epoch 243 / 500...\n",
      "train loss:  -12.375762257664556\n",
      "test loss:  -11.68973727692709\n",
      "train MSE:  107.61868149385302\n",
      "test MSE:  107.0856939252008\n",
      "Epoch 244 / 500...\n",
      "train loss:  -12.408040156180627\n",
      "test loss:  -11.7220002917322\n",
      "train MSE:  107.64344358190729\n",
      "test MSE:  107.11372433982628\n",
      "Epoch 245 / 500...\n",
      "train loss:  -12.446391285626687\n",
      "test loss:  -11.753963203989626\n",
      "train MSE:  107.66936458744938\n",
      "test MSE:  107.14297429063271\n",
      "Epoch 246 / 500...\n",
      "train loss:  -12.47998526643564\n",
      "test loss:  -11.786418960934085\n",
      "train MSE:  107.69426054763493\n",
      "test MSE:  107.17105597191282\n",
      "Epoch 247 / 500...\n",
      "train loss:  -12.513463169875632\n",
      "test loss:  -11.818025152615865\n",
      "train MSE:  107.72155272700581\n",
      "test MSE:  107.20180111493771\n",
      "Epoch 248 / 500...\n",
      "train loss:  -12.549108245485904\n",
      "test loss:  -11.85046003201037\n",
      "train MSE:  107.74674973996413\n",
      "test MSE:  107.23017376274989\n",
      "Epoch 249 / 500...\n",
      "train loss:  -12.576551452552106\n",
      "test loss:  -11.882966224426104\n",
      "train MSE:  107.77157353392803\n",
      "test MSE:  107.25803598456311\n",
      "Epoch 250 / 500...\n",
      "train loss:  -12.614415938295254\n",
      "test loss:  -11.914648076570774\n",
      "train MSE:  107.79915943633848\n",
      "test MSE:  107.28898016889285\n",
      "Epoch 251 / 500...\n",
      "train loss:  -12.645494598803594\n",
      "test loss:  -11.946727363063752\n",
      "train MSE:  107.82574859104595\n",
      "test MSE:  107.3187417439858\n",
      "Epoch 252 / 500...\n",
      "train loss:  -12.677286491366308\n",
      "test loss:  -11.979020552909098\n",
      "train MSE:  107.8518497272939\n",
      "test MSE:  107.34791630275076\n",
      "Epoch 253 / 500...\n",
      "train loss:  -12.712476249862503\n",
      "test loss:  -12.010931365717786\n",
      "train MSE:  107.87934817025304\n",
      "test MSE:  107.37863738479061\n",
      "Epoch 254 / 500...\n",
      "train loss:  -12.749495152494447\n",
      "test loss:  -12.04291735286339\n",
      "train MSE:  107.90691286927394\n",
      "test MSE:  107.40937728077724\n",
      "Epoch 255 / 500...\n",
      "train loss:  -12.782890580331419\n",
      "test loss:  -12.074875849766311\n",
      "train MSE:  107.93453615971568\n",
      "test MSE:  107.44012452327848\n",
      "Epoch 256 / 500...\n",
      "train loss:  -12.82037499751112\n",
      "test loss:  -12.106846036823574\n",
      "train MSE:  107.96234551356433\n",
      "test MSE:  107.47102206384409\n",
      "Epoch 257 / 500...\n",
      "train loss:  -12.847283443300439\n",
      "test loss:  -12.138770750694034\n",
      "train MSE:  107.99057058363223\n",
      "test MSE:  107.50238486981169\n",
      "Epoch 258 / 500...\n",
      "train loss:  -12.881925166459878\n",
      "test loss:  -12.170523085352018\n",
      "train MSE:  108.01939516692273\n",
      "test MSE:  107.53437576784826\n",
      "Epoch 259 / 500...\n",
      "train loss:  -12.919239897047945\n",
      "test loss:  -12.202482540042887\n",
      "train MSE:  108.04781609155002\n",
      "test MSE:  107.56587589473257\n",
      "Epoch 260 / 500...\n",
      "train loss:  -12.949236377595437\n",
      "test loss:  -12.234062012799281\n",
      "train MSE:  108.0775764510056\n",
      "test MSE:  107.59880636286303\n",
      "Epoch 261 / 500...\n",
      "train loss:  -12.98486861944291\n",
      "test loss:  -12.265678471769087\n",
      "train MSE:  108.10699048081848\n",
      "test MSE:  107.63126487802606\n",
      "Epoch 262 / 500...\n",
      "train loss:  -13.016971554615862\n",
      "test loss:  -12.296813265592915\n",
      "train MSE:  108.13871425188319\n",
      "test MSE:  107.66635598273268\n",
      "Epoch 263 / 500...\n",
      "train loss:  -13.047793475758558\n",
      "test loss:  -12.328543801292078\n",
      "train MSE:  108.16854037631931\n",
      "test MSE:  107.69925463517325\n",
      "Epoch 264 / 500...\n",
      "train loss:  -13.085244840771312\n",
      "test loss:  -12.359907274133064\n",
      "train MSE:  108.19970527171833\n",
      "test MSE:  107.73365573269093\n",
      "Epoch 265 / 500...\n",
      "train loss:  -13.113637739245586\n",
      "test loss:  -12.391523819853512\n",
      "train MSE:  108.2303492293132\n",
      "test MSE:  107.76737678715759\n",
      "Epoch 266 / 500...\n",
      "train loss:  -13.148100532826176\n",
      "test loss:  -12.422811142265925\n",
      "train MSE:  108.26208580663003\n",
      "test MSE:  107.80230576733732\n",
      "Epoch 267 / 500...\n",
      "train loss:  -13.183627300186105\n",
      "test loss:  -12.454237026502424\n",
      "train MSE:  108.29392725307373\n",
      "test MSE:  107.83734219158794\n",
      "Epoch 268 / 500...\n",
      "train loss:  -13.223526048009495\n",
      "test loss:  -12.485507557838899\n",
      "train MSE:  108.32615861018577\n",
      "test MSE:  107.8727474378695\n",
      "Epoch 269 / 500...\n",
      "train loss:  -13.24640250751822\n",
      "test loss:  -12.516863365537668\n",
      "train MSE:  108.35800484429085\n",
      "test MSE:  107.90766785943076\n",
      "Epoch 270 / 500...\n",
      "train loss:  -13.282920581455626\n",
      "test loss:  -12.548353893458012\n",
      "train MSE:  108.38966770240162\n",
      "test MSE:  107.94232025095481\n",
      "Epoch 271 / 500...\n",
      "train loss:  -13.313066443747593\n",
      "test loss:  -12.580088312278773\n",
      "train MSE:  108.4208779769762\n",
      "test MSE:  107.97645655217418\n",
      "Epoch 272 / 500...\n",
      "train loss:  -13.349567186810127\n",
      "test loss:  -12.611327907218461\n",
      "train MSE:  108.45397998292151\n",
      "test MSE:  108.01272799089226\n",
      "Epoch 273 / 500...\n",
      "train loss:  -13.381002638629031\n",
      "test loss:  -12.642561826900478\n",
      "train MSE:  108.48684187795259\n",
      "test MSE:  108.0486068899394\n",
      "Epoch 274 / 500...\n",
      "train loss:  -13.417736539329876\n",
      "test loss:  -12.673485235627535\n",
      "train MSE:  108.52104609480119\n",
      "test MSE:  108.08597325366458\n",
      "Epoch 275 / 500...\n",
      "train loss:  -13.451855206284224\n",
      "test loss:  -12.704597709868656\n",
      "train MSE:  108.5549101293739\n",
      "test MSE:  108.12290843128758\n",
      "Epoch 276 / 500...\n",
      "train loss:  -13.480217480237588\n",
      "test loss:  -12.73549130679984\n",
      "train MSE:  108.58934175522683\n",
      "test MSE:  108.16044204252087\n",
      "Epoch 277 / 500...\n",
      "train loss:  -13.513554657716549\n",
      "test loss:  -12.766475975768222\n",
      "train MSE:  108.62404719359984\n",
      "test MSE:  108.19829312290403\n",
      "Epoch 278 / 500...\n",
      "train loss:  -13.543555750751212\n",
      "test loss:  -12.797756628365532\n",
      "train MSE:  108.65782584109743\n",
      "test MSE:  108.23498731840697\n",
      "Epoch 279 / 500...\n",
      "train loss:  -13.584417363347521\n",
      "test loss:  -12.828820349616773\n",
      "train MSE:  108.69255873491173\n",
      "test MSE:  108.27274555692632\n",
      "Epoch 280 / 500...\n",
      "train loss:  -13.615484851816987\n",
      "test loss:  -12.860148004795587\n",
      "train MSE:  108.72681430598817\n",
      "test MSE:  108.30993538077146\n",
      "Epoch 281 / 500...\n",
      "train loss:  -13.650106837219685\n",
      "test loss:  -12.891007806235551\n",
      "train MSE:  108.76229557992538\n",
      "test MSE:  108.34843923124194\n",
      "Epoch 282 / 500...\n",
      "train loss:  -13.681712079934199\n",
      "test loss:  -12.921789528637584\n",
      "train MSE:  108.79860697065861\n",
      "test MSE:  108.38789159263204\n",
      "Epoch 283 / 500...\n",
      "train loss:  -13.706878159214687\n",
      "test loss:  -12.952834530392021\n",
      "train MSE:  108.83361796837053\n",
      "test MSE:  108.4257505380055\n",
      "Epoch 284 / 500...\n",
      "train loss:  -13.74171419356331\n",
      "test loss:  -12.98357909978875\n",
      "train MSE:  108.870374410903\n",
      "test MSE:  108.46564421304264\n",
      "Epoch 285 / 500...\n",
      "train loss:  -13.773108744990731\n",
      "test loss:  -13.014276849038287\n",
      "train MSE:  108.90747978763702\n",
      "test MSE:  108.50590112825232\n",
      "Epoch 286 / 500...\n",
      "train loss:  -13.807835808508376\n",
      "test loss:  -13.045278397030835\n",
      "train MSE:  108.9436693757198\n",
      "test MSE:  108.5450489148216\n",
      "Epoch 287 / 500...\n",
      "train loss:  -13.839651420624428\n",
      "test loss:  -13.07582797566751\n",
      "train MSE:  108.9814418979887\n",
      "test MSE:  108.585910948541\n",
      "Epoch 288 / 500...\n",
      "train loss:  -13.86663513732594\n",
      "test loss:  -13.106888813751832\n",
      "train MSE:  109.01753329657592\n",
      "test MSE:  108.62481411993772\n",
      "Epoch 289 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -13.905486760633659\n",
      "test loss:  -13.137681112691975\n",
      "train MSE:  109.05479723112809\n",
      "test MSE:  108.66503515331705\n",
      "Epoch 290 / 500...\n",
      "train loss:  -13.94025199816248\n",
      "test loss:  -13.168364854743395\n",
      "train MSE:  109.0927742242959\n",
      "test MSE:  108.70602636993154\n",
      "Epoch 291 / 500...\n",
      "train loss:  -13.96922871022747\n",
      "test loss:  -13.199159898333487\n",
      "train MSE:  109.13035910400164\n",
      "test MSE:  108.74649692738198\n",
      "Epoch 292 / 500...\n",
      "train loss:  -13.999543291107612\n",
      "test loss:  -13.229940068066947\n",
      "train MSE:  109.16837079985547\n",
      "test MSE:  108.78746190777491\n",
      "Epoch 293 / 500...\n",
      "train loss:  -14.032135364386036\n",
      "test loss:  -13.260645976071698\n",
      "train MSE:  109.20668013883335\n",
      "test MSE:  108.82868475556454\n",
      "Epoch 294 / 500...\n",
      "train loss:  -14.063835967105682\n",
      "test loss:  -13.290913947544905\n",
      "train MSE:  109.24642938775779\n",
      "test MSE:  108.87154842729198\n",
      "Epoch 295 / 500...\n",
      "train loss:  -14.09714969552805\n",
      "test loss:  -13.321519162201643\n",
      "train MSE:  109.28548054384396\n",
      "test MSE:  108.9135698290383\n",
      "Epoch 296 / 500...\n",
      "train loss:  -14.13022019280498\n",
      "test loss:  -13.352246717547015\n",
      "train MSE:  109.32400923274245\n",
      "test MSE:  108.95491623822993\n",
      "Epoch 297 / 500...\n",
      "train loss:  -14.165127494744254\n",
      "test loss:  -13.382497321547186\n",
      "train MSE:  109.36488313047703\n",
      "test MSE:  108.99890148712016\n",
      "Epoch 298 / 500...\n",
      "train loss:  -14.20010930689729\n",
      "test loss:  -13.412857323458853\n",
      "train MSE:  109.40513382522069\n",
      "test MSE:  109.04214846421077\n",
      "Epoch 299 / 500...\n",
      "train loss:  -14.233126219788419\n",
      "test loss:  -13.442967716479025\n",
      "train MSE:  109.44651294766277\n",
      "test MSE:  109.08663952930767\n",
      "Epoch 300 / 500...\n",
      "train loss:  -14.260102923846071\n",
      "test loss:  -13.473610886842359\n",
      "train MSE:  109.48643819335632\n",
      "test MSE:  109.12941337412775\n",
      "Epoch 301 / 500...\n",
      "train loss:  -14.296888733931675\n",
      "test loss:  -13.503978822452558\n",
      "train MSE:  109.52735863074028\n",
      "test MSE:  109.17330278084674\n",
      "Epoch 302 / 500...\n",
      "train loss:  -14.325945448982619\n",
      "test loss:  -13.534232077577107\n",
      "train MSE:  109.56825078361803\n",
      "test MSE:  109.2170999260664\n",
      "Epoch 303 / 500...\n",
      "train loss:  -14.35389316309893\n",
      "test loss:  -13.564814487074969\n",
      "train MSE:  109.60915401235275\n",
      "test MSE:  109.26089694607319\n",
      "Epoch 304 / 500...\n",
      "train loss:  -14.382461298700134\n",
      "test loss:  -13.595688286836706\n",
      "train MSE:  109.64897563333106\n",
      "test MSE:  109.30339861418703\n",
      "Epoch 305 / 500...\n",
      "train loss:  -14.4225949692059\n",
      "test loss:  -13.625757240558872\n",
      "train MSE:  109.69113300658428\n",
      "test MSE:  109.34850121017904\n",
      "Epoch 306 / 500...\n",
      "train loss:  -14.448846746347256\n",
      "test loss:  -13.65610654335187\n",
      "train MSE:  109.73274943671436\n",
      "test MSE:  109.39293890480549\n",
      "Epoch 307 / 500...\n",
      "train loss:  -14.478435446655661\n",
      "test loss:  -13.686652319999627\n",
      "train MSE:  109.77431945816222\n",
      "test MSE:  109.43729914994367\n",
      "Epoch 308 / 500...\n",
      "train loss:  -14.514879636190193\n",
      "test loss:  -13.716806097241056\n",
      "train MSE:  109.81718566135132\n",
      "test MSE:  109.48314175015753\n",
      "Epoch 309 / 500...\n",
      "train loss:  -14.550450853738953\n",
      "test loss:  -13.746973358776998\n",
      "train MSE:  109.86013224096666\n",
      "test MSE:  109.5290058443217\n",
      "Epoch 310 / 500...\n",
      "train loss:  -14.584083108067187\n",
      "test loss:  -13.776923806126991\n",
      "train MSE:  109.9037940161947\n",
      "test MSE:  109.57563652014683\n",
      "Epoch 311 / 500...\n",
      "train loss:  -14.607108260719697\n",
      "test loss:  -13.806989350719931\n",
      "train MSE:  109.9471851351408\n",
      "test MSE:  109.62190871984919\n",
      "Epoch 312 / 500...\n",
      "train loss:  -14.64736341720249\n",
      "test loss:  -13.837092573947645\n",
      "train MSE:  109.99091226854334\n",
      "test MSE:  109.66852038821726\n",
      "Epoch 313 / 500...\n",
      "train loss:  -14.67305632480123\n",
      "test loss:  -13.867487534064933\n",
      "train MSE:  110.0337953234129\n",
      "test MSE:  109.71413257636539\n",
      "Epoch 314 / 500...\n",
      "train loss:  -14.70451008321437\n",
      "test loss:  -13.897705704461746\n",
      "train MSE:  110.07714316996476\n",
      "test MSE:  109.76023581114308\n",
      "Epoch 315 / 500...\n",
      "train loss:  -14.737954290117974\n",
      "test loss:  -13.927150108909556\n",
      "train MSE:  110.12314852462448\n",
      "test MSE:  109.80932112293848\n",
      "Epoch 316 / 500...\n",
      "train loss:  -14.77129211114155\n",
      "test loss:  -13.95700540398783\n",
      "train MSE:  110.16779406964164\n",
      "test MSE:  109.85678886394372\n",
      "Epoch 317 / 500...\n",
      "train loss:  -14.808940954428072\n",
      "test loss:  -13.986865388857849\n",
      "train MSE:  110.21313649887527\n",
      "test MSE:  109.90503319095934\n",
      "Epoch 318 / 500...\n",
      "train loss:  -14.832420477249785\n",
      "test loss:  -14.017069121845743\n",
      "train MSE:  110.257414361282\n",
      "test MSE:  109.95203462171486\n",
      "Epoch 319 / 500...\n",
      "train loss:  -14.867444964546346\n",
      "test loss:  -14.047010920855147\n",
      "train MSE:  110.30301561793517\n",
      "test MSE:  110.00056644194021\n",
      "Epoch 320 / 500...\n",
      "train loss:  -14.897717684503034\n",
      "test loss:  -14.077245624007656\n",
      "train MSE:  110.34790948777928\n",
      "test MSE:  110.04820925528544\n",
      "Epoch 321 / 500...\n",
      "train loss:  -14.925965661420047\n",
      "test loss:  -14.106851190173053\n",
      "train MSE:  110.39450887843415\n",
      "test MSE:  110.09776165302887\n",
      "Epoch 322 / 500...\n",
      "train loss:  -14.961306904870174\n",
      "test loss:  -14.136646029793537\n",
      "train MSE:  110.44104426724549\n",
      "test MSE:  110.14718190072703\n",
      "Epoch 323 / 500...\n",
      "train loss:  -14.988652947879666\n",
      "test loss:  -14.166817863063798\n",
      "train MSE:  110.48654363600532\n",
      "test MSE:  110.1953667283064\n",
      "Epoch 324 / 500...\n",
      "train loss:  -15.016125196287199\n",
      "test loss:  -14.196713096469106\n",
      "train MSE:  110.53257660486608\n",
      "test MSE:  110.24410840848945\n",
      "Epoch 325 / 500...\n",
      "train loss:  -15.059712308861034\n",
      "test loss:  -14.226437459603135\n",
      "train MSE:  110.58000022966573\n",
      "test MSE:  110.2944649483235\n",
      "Epoch 326 / 500...\n",
      "train loss:  -15.081620716175332\n",
      "test loss:  -14.255888719023483\n",
      "train MSE:  110.62793680871395\n",
      "test MSE:  110.3453264349429\n",
      "Epoch 327 / 500...\n",
      "train loss:  -15.114634751301354\n",
      "test loss:  -14.2861703543005\n",
      "train MSE:  110.6734073273848\n",
      "test MSE:  110.39334499502132\n",
      "Epoch 328 / 500...\n",
      "train loss:  -15.141225324123633\n",
      "test loss:  -14.315963351534908\n",
      "train MSE:  110.7204364008533\n",
      "test MSE:  110.44308251691923\n",
      "Epoch 329 / 500...\n",
      "train loss:  -15.175952833503501\n",
      "test loss:  -14.34538398119723\n",
      "train MSE:  110.76894131521809\n",
      "test MSE:  110.49448384728035\n",
      "Epoch 330 / 500...\n",
      "train loss:  -15.212122780098705\n",
      "test loss:  -14.37538770149417\n",
      "train MSE:  110.8162221485838\n",
      "test MSE:  110.54447856556327\n",
      "Epoch 331 / 500...\n",
      "train loss:  -15.237007966795334\n",
      "test loss:  -14.405005006710846\n",
      "train MSE:  110.86494634552537\n",
      "test MSE:  110.5960631810445\n",
      "Epoch 332 / 500...\n",
      "train loss:  -15.270469030100614\n",
      "test loss:  -14.43457279982177\n",
      "train MSE:  110.9133641369028\n",
      "test MSE:  110.64724222604943\n",
      "Epoch 333 / 500...\n",
      "train loss:  -15.299089292736044\n",
      "test loss:  -14.464201979965857\n",
      "train MSE:  110.96236688738912\n",
      "test MSE:  110.69908713003977\n",
      "Epoch 334 / 500...\n",
      "train loss:  -15.334741552451536\n",
      "test loss:  -14.493951108054164\n",
      "train MSE:  111.01098443455942\n",
      "test MSE:  110.75045131605751\n",
      "Epoch 335 / 500...\n",
      "train loss:  -15.362418758327278\n",
      "test loss:  -14.523989349910664\n",
      "train MSE:  111.05902247049306\n",
      "test MSE:  110.80105321829218\n",
      "Epoch 336 / 500...\n",
      "train loss:  -15.397907997087817\n",
      "test loss:  -14.552889182078484\n",
      "train MSE:  111.11043993109283\n",
      "test MSE:  110.8554931263758\n",
      "Epoch 337 / 500...\n",
      "train loss:  -15.4267739718161\n",
      "test loss:  -14.582726980305221\n",
      "train MSE:  111.15915020406027\n",
      "test MSE:  110.90679731715471\n",
      "Epoch 338 / 500...\n",
      "train loss:  -15.454422954432422\n",
      "test loss:  -14.612344255707637\n",
      "train MSE:  111.20844892517766\n",
      "test MSE:  110.95873437314096\n",
      "Epoch 339 / 500...\n",
      "train loss:  -15.492387907554999\n",
      "test loss:  -14.641356084815357\n",
      "train MSE:  111.25982782417593\n",
      "test MSE:  111.0130161746286\n",
      "Epoch 340 / 500...\n",
      "train loss:  -15.521522202834452\n",
      "test loss:  -14.67069331439414\n",
      "train MSE:  111.31058026302321\n",
      "test MSE:  111.06654825588313\n",
      "Epoch 341 / 500...\n",
      "train loss:  -15.554558893222223\n",
      "test loss:  -14.699908432622138\n",
      "train MSE:  111.3613483095913\n",
      "test MSE:  111.1200215825682\n",
      "Epoch 342 / 500...\n",
      "train loss:  -15.581489888868965\n",
      "test loss:  -14.729474373245392\n",
      "train MSE:  111.41144755747611\n",
      "test MSE:  111.17271396832251\n",
      "Epoch 343 / 500...\n",
      "train loss:  -15.611619940556785\n",
      "test loss:  -14.758821894185024\n",
      "train MSE:  111.46275603073744\n",
      "test MSE:  111.2267797041938\n",
      "Epoch 344 / 500...\n",
      "train loss:  -15.64265426214764\n",
      "test loss:  -14.788113774838152\n",
      "train MSE:  111.51446251188207\n",
      "test MSE:  111.28124486895346\n",
      "Epoch 345 / 500...\n",
      "train loss:  -15.670948097157796\n",
      "test loss:  -14.81752666259277\n",
      "train MSE:  111.5656227974059\n",
      "test MSE:  111.33504143315568\n",
      "Epoch 346 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -15.715182259485042\n",
      "test loss:  -14.846711621810941\n",
      "train MSE:  111.61809890476223\n",
      "test MSE:  111.39032502126372\n",
      "Epoch 347 / 500...\n",
      "train loss:  -15.745544744465906\n",
      "test loss:  -14.875916252006414\n",
      "train MSE:  111.67098412790256\n",
      "test MSE:  111.44604566244676\n",
      "Epoch 348 / 500...\n",
      "train loss:  -15.766857906563489\n",
      "test loss:  -14.90517237266773\n",
      "train MSE:  111.72322332148191\n",
      "test MSE:  111.50092935956378\n",
      "Epoch 349 / 500...\n",
      "train loss:  -15.799599405752632\n",
      "test loss:  -14.934565080785434\n",
      "train MSE:  111.77527683885624\n",
      "test MSE:  111.55556780574551\n",
      "Epoch 350 / 500...\n",
      "train loss:  -15.829419600473413\n",
      "test loss:  -14.963399776731936\n",
      "train MSE:  111.82897619235713\n",
      "test MSE:  111.6120311896182\n",
      "Epoch 351 / 500...\n",
      "train loss:  -15.85869471948887\n",
      "test loss:  -14.99238685890443\n",
      "train MSE:  111.88220062621197\n",
      "test MSE:  111.66792047031926\n",
      "Epoch 352 / 500...\n",
      "train loss:  -15.886737236266956\n",
      "test loss:  -15.022007657828397\n",
      "train MSE:  111.93441054137314\n",
      "test MSE:  111.72262120954045\n",
      "Epoch 353 / 500...\n",
      "train loss:  -15.91781055763369\n",
      "test loss:  -15.051268914413807\n",
      "train MSE:  111.9875277892113\n",
      "test MSE:  111.77834297350947\n",
      "Epoch 354 / 500...\n",
      "train loss:  -15.94798606607027\n",
      "test loss:  -15.08011806349538\n",
      "train MSE:  112.0420812946602\n",
      "test MSE:  111.8356646468415\n",
      "Epoch 355 / 500...\n",
      "train loss:  -15.9890678116778\n",
      "test loss:  -15.10909478348769\n",
      "train MSE:  112.09671088754784\n",
      "test MSE:  111.89304894423704\n",
      "Epoch 356 / 500...\n",
      "train loss:  -16.00723800464942\n",
      "test loss:  -15.13823761143817\n",
      "train MSE:  112.15073231472364\n",
      "test MSE:  111.94968636543328\n",
      "Epoch 357 / 500...\n",
      "train loss:  -16.039528493362887\n",
      "test loss:  -15.16691785852096\n",
      "train MSE:  112.20601513427494\n",
      "test MSE:  112.0076874141819\n",
      "Epoch 358 / 500...\n",
      "train loss:  -16.08073909218887\n",
      "test loss:  -15.195685644536212\n",
      "train MSE:  112.26154989562956\n",
      "test MSE:  112.06598789957512\n",
      "Epoch 359 / 500...\n",
      "train loss:  -16.10208136211731\n",
      "test loss:  -15.224870287530969\n",
      "train MSE:  112.31613830636299\n",
      "test MSE:  112.12316137878379\n",
      "Epoch 360 / 500...\n",
      "train loss:  -16.135762739454993\n",
      "test loss:  -15.254010132163515\n",
      "train MSE:  112.37081627275317\n",
      "test MSE:  112.18038944621328\n",
      "Epoch 361 / 500...\n",
      "train loss:  -16.161219285568396\n",
      "test loss:  -15.283177214513419\n",
      "train MSE:  112.42559675455135\n",
      "test MSE:  112.2377267150252\n",
      "Epoch 362 / 500...\n",
      "train loss:  -16.187375813382374\n",
      "test loss:  -15.312232901537811\n",
      "train MSE:  112.48077799443169\n",
      "test MSE:  112.29544293682149\n",
      "Epoch 363 / 500...\n",
      "train loss:  -16.22639382565876\n",
      "test loss:  -15.341376670612583\n",
      "train MSE:  112.53620414378629\n",
      "test MSE:  112.35343159802888\n",
      "Epoch 364 / 500...\n",
      "train loss:  -16.262087553548483\n",
      "test loss:  -15.370031815914235\n",
      "train MSE:  112.59299384234478\n",
      "test MSE:  112.41291979722631\n",
      "Epoch 365 / 500...\n",
      "train loss:  -16.288719002906458\n",
      "test loss:  -15.399016473645641\n",
      "train MSE:  112.64893310131261\n",
      "test MSE:  112.47142177084342\n",
      "Epoch 366 / 500...\n",
      "train loss:  -16.316037063643748\n",
      "test loss:  -15.427525422053003\n",
      "train MSE:  112.705954685383\n",
      "test MSE:  112.53108776536467\n",
      "Epoch 367 / 500...\n",
      "train loss:  -16.343896671825142\n",
      "test loss:  -15.455648772497758\n",
      "train MSE:  112.76489996215729\n",
      "test MSE:  112.59293099854985\n",
      "Epoch 368 / 500...\n",
      "train loss:  -16.377849311287004\n",
      "test loss:  -15.48379210415984\n",
      "train MSE:  112.8233231217794\n",
      "test MSE:  112.65408311898906\n",
      "Epoch 369 / 500...\n",
      "train loss:  -16.417529775733296\n",
      "test loss:  -15.512809490707566\n",
      "train MSE:  112.8801588853119\n",
      "test MSE:  112.71339844304421\n",
      "Epoch 370 / 500...\n",
      "train loss:  -16.449221996371545\n",
      "test loss:  -15.541207405632107\n",
      "train MSE:  112.93909717660651\n",
      "test MSE:  112.77510763563868\n",
      "Epoch 371 / 500...\n",
      "train loss:  -16.471465245454354\n",
      "test loss:  -15.569673943500035\n",
      "train MSE:  112.99779626750916\n",
      "test MSE:  112.83651856763824\n",
      "Epoch 372 / 500...\n",
      "train loss:  -16.49732834221465\n",
      "test loss:  -15.59831985879801\n",
      "train MSE:  113.05633599788217\n",
      "test MSE:  112.89772455347182\n",
      "Epoch 373 / 500...\n",
      "train loss:  -16.531959840720756\n",
      "test loss:  -15.62671386611348\n",
      "train MSE:  113.11509570345576\n",
      "test MSE:  112.95910834700061\n",
      "Epoch 374 / 500...\n",
      "train loss:  -16.55908126742715\n",
      "test loss:  -15.655606050772152\n",
      "train MSE:  113.17254480071361\n",
      "test MSE:  113.01896976106028\n",
      "Epoch 375 / 500...\n",
      "train loss:  -16.589572569139257\n",
      "test loss:  -15.684046958567569\n",
      "train MSE:  113.2320802948745\n",
      "test MSE:  113.0811797906286\n",
      "Epoch 376 / 500...\n",
      "train loss:  -16.619167535701926\n",
      "test loss:  -15.713118544191014\n",
      "train MSE:  113.2897515692397\n",
      "test MSE:  113.14121348323886\n",
      "Epoch 377 / 500...\n",
      "train loss:  -16.649166336500564\n",
      "test loss:  -15.741642216088595\n",
      "train MSE:  113.34916004064473\n",
      "test MSE:  113.20320190493122\n",
      "Epoch 378 / 500...\n",
      "train loss:  -16.679150827842236\n",
      "test loss:  -15.770123194813559\n",
      "train MSE:  113.4088553301548\n",
      "test MSE:  113.2654714236719\n",
      "Epoch 379 / 500...\n",
      "train loss:  -16.711980646267747\n",
      "test loss:  -15.798793389056463\n",
      "train MSE:  113.4680451543619\n",
      "test MSE:  113.32708135394233\n",
      "Epoch 380 / 500...\n",
      "train loss:  -16.735888121614483\n",
      "test loss:  -15.827401659571555\n",
      "train MSE:  113.52777264969791\n",
      "test MSE:  113.38931907364527\n",
      "Epoch 381 / 500...\n",
      "train loss:  -16.76406129767081\n",
      "test loss:  -15.855852936914454\n",
      "train MSE:  113.58786394210092\n",
      "test MSE:  113.45194033684086\n",
      "Epoch 382 / 500...\n",
      "train loss:  -16.804558610246477\n",
      "test loss:  -15.884338674358062\n",
      "train MSE:  113.64852719898077\n",
      "test MSE:  113.51518167119416\n",
      "Epoch 383 / 500...\n",
      "train loss:  -16.83136879465832\n",
      "test loss:  -15.912519354414274\n",
      "train MSE:  113.71009969615184\n",
      "test MSE:  113.57941297752474\n",
      "Epoch 384 / 500...\n",
      "train loss:  -16.860301861173188\n",
      "test loss:  -15.940528903855164\n",
      "train MSE:  113.77196736501315\n",
      "test MSE:  113.6439407687209\n",
      "Epoch 385 / 500...\n",
      "train loss:  -16.882929707066765\n",
      "test loss:  -15.968934532221295\n",
      "train MSE:  113.83346842649111\n",
      "test MSE:  113.70806061118405\n",
      "Epoch 386 / 500...\n",
      "train loss:  -16.912677860183877\n",
      "test loss:  -15.997585616119094\n",
      "train MSE:  113.89401199295708\n",
      "test MSE:  113.77100464580921\n",
      "Epoch 387 / 500...\n",
      "train loss:  -16.959139291666755\n",
      "test loss:  -16.02598229991959\n",
      "train MSE:  113.95637829896941\n",
      "test MSE:  113.8360570233956\n",
      "Epoch 388 / 500...\n",
      "train loss:  -16.9730220542859\n",
      "test loss:  -16.054496697575463\n",
      "train MSE:  114.01759244532434\n",
      "test MSE:  113.89965759931995\n",
      "Epoch 389 / 500...\n",
      "train loss:  -17.009783813815346\n",
      "test loss:  -16.082226807006194\n",
      "train MSE:  114.08073920051783\n",
      "test MSE:  113.96547106569038\n",
      "Epoch 390 / 500...\n",
      "train loss:  -17.037018420366913\n",
      "test loss:  -16.110377279226338\n",
      "train MSE:  114.14297144859177\n",
      "test MSE:  114.0302098506976\n",
      "Epoch 391 / 500...\n",
      "train loss:  -17.066611764799234\n",
      "test loss:  -16.13855911474057\n",
      "train MSE:  114.20582096105666\n",
      "test MSE:  114.09560352679422\n",
      "Epoch 392 / 500...\n",
      "train loss:  -17.099728444674604\n",
      "test loss:  -16.166694288948165\n",
      "train MSE:  114.26895843644719\n",
      "test MSE:  114.16127594701788\n",
      "Epoch 393 / 500...\n",
      "train loss:  -17.134868859524147\n",
      "test loss:  -16.19450795804389\n",
      "train MSE:  114.33356054033712\n",
      "test MSE:  114.22863117958453\n",
      "Epoch 394 / 500...\n",
      "train loss:  -17.151999473289525\n",
      "test loss:  -16.22316984570189\n",
      "train MSE:  114.39515925917627\n",
      "test MSE:  114.29246117674577\n",
      "Epoch 395 / 500...\n",
      "train loss:  -17.185594579922782\n",
      "test loss:  -16.251207923079537\n",
      "train MSE:  114.4590093216989\n",
      "test MSE:  114.35886192520836\n",
      "Epoch 396 / 500...\n",
      "train loss:  -17.228408671733277\n",
      "test loss:  -16.279357603476548\n",
      "train MSE:  114.52349187797064\n",
      "test MSE:  114.42597271096122\n",
      "Epoch 397 / 500...\n",
      "train loss:  -17.248289868064653\n",
      "test loss:  -16.30751133992748\n",
      "train MSE:  114.58751836934401\n",
      "test MSE:  114.49249873110881\n",
      "Epoch 398 / 500...\n",
      "train loss:  -17.281866534498295\n",
      "test loss:  -16.335703173254316\n",
      "train MSE:  114.65155208236847\n",
      "test MSE:  114.55900246992559\n",
      "Epoch 399 / 500...\n",
      "train loss:  -17.29886324548297\n",
      "test loss:  -16.36389996632303\n",
      "train MSE:  114.71586330086751\n",
      "test MSE:  114.62579927734718\n",
      "Epoch 400 / 500...\n",
      "train loss:  -17.336130509900567\n",
      "test loss:  -16.392038227888822\n",
      "train MSE:  114.78025693953111\n",
      "test MSE:  114.6925829486871\n",
      "Epoch 401 / 500...\n",
      "train loss:  -17.365147891592226\n",
      "test loss:  -16.419700336223453\n",
      "train MSE:  114.846235647718\n",
      "test MSE:  114.76123053715078\n",
      "Epoch 402 / 500...\n",
      "train loss:  -17.394911935696296\n",
      "test loss:  -16.447653181506045\n",
      "train MSE:  114.91163277771177\n",
      "test MSE:  114.82916430171855\n",
      "Epoch 403 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -17.425519672830514\n",
      "test loss:  -16.475736437118616\n",
      "train MSE:  114.97692263227552\n",
      "test MSE:  114.89691538468763\n",
      "Epoch 404 / 500...\n",
      "train loss:  -17.45168664424905\n",
      "test loss:  -16.503773930591368\n",
      "train MSE:  115.04264299748561\n",
      "test MSE:  114.96510572583905\n",
      "Epoch 405 / 500...\n",
      "train loss:  -17.482800156225224\n",
      "test loss:  -16.532194817519315\n",
      "train MSE:  115.10721808987098\n",
      "test MSE:  115.0319061618853\n",
      "Epoch 406 / 500...\n",
      "train loss:  -17.510635285413148\n",
      "test loss:  -16.56047234147056\n",
      "train MSE:  115.1725437029148\n",
      "test MSE:  115.09956919496035\n",
      "Epoch 407 / 500...\n",
      "train loss:  -17.55318320735902\n",
      "test loss:  -16.58837856101111\n",
      "train MSE:  115.23926306539279\n",
      "test MSE:  115.16878160667774\n",
      "Epoch 408 / 500...\n",
      "train loss:  -17.57810960850874\n",
      "test loss:  -16.616070259002374\n",
      "train MSE:  115.3061891576469\n",
      "test MSE:  115.23820441448812\n",
      "Epoch 409 / 500...\n",
      "train loss:  -17.600339643678385\n",
      "test loss:  -16.643650966641303\n",
      "train MSE:  115.37392124072991\n",
      "test MSE:  115.30856199348827\n",
      "Epoch 410 / 500...\n",
      "train loss:  -17.629775539520658\n",
      "test loss:  -16.671827029824314\n",
      "train MSE:  115.43990775335499\n",
      "test MSE:  115.37683404976917\n",
      "Epoch 411 / 500...\n",
      "train loss:  -17.65940690625045\n",
      "test loss:  -16.699786615227936\n",
      "train MSE:  115.506634893143\n",
      "test MSE:  115.44593707473287\n",
      "Epoch 412 / 500...\n",
      "train loss:  -17.68458517305128\n",
      "test loss:  -16.72794789781028\n",
      "train MSE:  115.57289818395107\n",
      "test MSE:  115.51444129243491\n",
      "Epoch 413 / 500...\n",
      "train loss:  -17.71885935160487\n",
      "test loss:  -16.755633279431382\n",
      "train MSE:  115.64067598248876\n",
      "test MSE:  115.5846682401802\n",
      "Epoch 414 / 500...\n",
      "train loss:  -17.743051633533174\n",
      "test loss:  -16.783683565606655\n",
      "train MSE:  115.70825330701926\n",
      "test MSE:  115.65461535716825\n",
      "Epoch 415 / 500...\n",
      "train loss:  -17.773012111763457\n",
      "test loss:  -16.811472941383588\n",
      "train MSE:  115.7761192594139\n",
      "test MSE:  115.72484559761577\n",
      "Epoch 416 / 500...\n",
      "train loss:  -17.80322047323576\n",
      "test loss:  -16.839054360706026\n",
      "train MSE:  115.8444717038565\n",
      "test MSE:  115.79559192148858\n",
      "Epoch 417 / 500...\n",
      "train loss:  -17.835113471078007\n",
      "test loss:  -16.86696286709755\n",
      "train MSE:  115.91283916625844\n",
      "test MSE:  115.86632025594703\n",
      "Epoch 418 / 500...\n",
      "train loss:  -17.868626419042606\n",
      "test loss:  -16.894346062556828\n",
      "train MSE:  115.98217856459132\n",
      "test MSE:  115.93813492467895\n",
      "Epoch 419 / 500...\n",
      "train loss:  -17.88514492454285\n",
      "test loss:  -16.922449863308874\n",
      "train MSE:  116.04967229564762\n",
      "test MSE:  116.00776860564049\n",
      "Epoch 420 / 500...\n",
      "train loss:  -17.92009132765936\n",
      "test loss:  -16.950124825036664\n",
      "train MSE:  116.11928243806972\n",
      "test MSE:  116.07988634321077\n",
      "Epoch 421 / 500...\n",
      "train loss:  -17.96356492507498\n",
      "test loss:  -16.97729506572407\n",
      "train MSE:  116.19021262035655\n",
      "test MSE:  116.15343316563497\n",
      "Epoch 422 / 500...\n",
      "train loss:  -17.991116831678106\n",
      "test loss:  -17.005277327766613\n",
      "train MSE:  116.25900704327944\n",
      "test MSE:  116.22447844528953\n",
      "Epoch 423 / 500...\n",
      "train loss:  -18.00405424787937\n",
      "test loss:  -17.03305926889882\n",
      "train MSE:  116.32854940999218\n",
      "test MSE:  116.296378902007\n",
      "Epoch 424 / 500...\n",
      "train loss:  -18.039209878820504\n",
      "test loss:  -17.061197076207524\n",
      "train MSE:  116.39740466247295\n",
      "test MSE:  116.36744408378361\n",
      "Epoch 425 / 500...\n",
      "train loss:  -18.06908385733815\n",
      "test loss:  -17.0889398019225\n",
      "train MSE:  116.46709876043641\n",
      "test MSE:  116.43939826518636\n",
      "Epoch 426 / 500...\n",
      "train loss:  -18.106018093721367\n",
      "test loss:  -17.116281564177914\n",
      "train MSE:  116.53800538604257\n",
      "test MSE:  116.51269658749224\n",
      "Epoch 427 / 500...\n",
      "train loss:  -18.117839512013706\n",
      "test loss:  -17.14418409003709\n",
      "train MSE:  116.60772450975956\n",
      "test MSE:  116.58464354876186\n",
      "Epoch 428 / 500...\n",
      "train loss:  -18.16425780641423\n",
      "test loss:  -17.17165261885407\n",
      "train MSE:  116.6789916845145\n",
      "test MSE:  116.6583125384987\n",
      "Epoch 429 / 500...\n",
      "train loss:  -18.17782922396438\n",
      "test loss:  -17.199556390259435\n",
      "train MSE:  116.74924121916962\n",
      "test MSE:  116.73080642184598\n",
      "Epoch 430 / 500...\n",
      "train loss:  -18.217020449866205\n",
      "test loss:  -17.226994062919054\n",
      "train MSE:  116.82110939700134\n",
      "test MSE:  116.80514414319653\n",
      "Epoch 431 / 500...\n",
      "train loss:  -18.24796987701376\n",
      "test loss:  -17.254200943216674\n",
      "train MSE:  116.89295586939242\n",
      "test MSE:  116.87935710000056\n",
      "Epoch 432 / 500...\n",
      "train loss:  -18.276429883317558\n",
      "test loss:  -17.281358177115393\n",
      "train MSE:  116.96531782003431\n",
      "test MSE:  116.9541380403523\n",
      "Epoch 433 / 500...\n",
      "train loss:  -18.300723145341564\n",
      "test loss:  -17.308519496195558\n",
      "train MSE:  117.03792346014974\n",
      "test MSE:  117.02914759685709\n",
      "Epoch 434 / 500...\n",
      "train loss:  -18.32563230130898\n",
      "test loss:  -17.33629657523964\n",
      "train MSE:  117.10945400997065\n",
      "test MSE:  117.10292677250087\n",
      "Epoch 435 / 500...\n",
      "train loss:  -18.356324147353916\n",
      "test loss:  -17.363705582887782\n",
      "train MSE:  117.18172101201331\n",
      "test MSE:  117.17751759729893\n",
      "Epoch 436 / 500...\n",
      "train loss:  -18.38989717596773\n",
      "test loss:  -17.3909626953609\n",
      "train MSE:  117.25477484851085\n",
      "test MSE:  117.25295067446616\n",
      "Epoch 437 / 500...\n",
      "train loss:  -18.407987301043022\n",
      "test loss:  -17.419149528291417\n",
      "train MSE:  117.32566230663227\n",
      "test MSE:  117.32587705220357\n",
      "Epoch 438 / 500...\n",
      "train loss:  -18.44874355347477\n",
      "test loss:  -17.446800648642405\n",
      "train MSE:  117.39817569319105\n",
      "test MSE:  117.40058804334947\n",
      "Epoch 439 / 500...\n",
      "train loss:  -18.482961104751244\n",
      "test loss:  -17.473703122712145\n",
      "train MSE:  117.47288433728151\n",
      "test MSE:  117.47779968593171\n",
      "Epoch 440 / 500...\n",
      "train loss:  -18.508166971490077\n",
      "test loss:  -17.50066756629388\n",
      "train MSE:  117.5471270569059\n",
      "test MSE:  117.5544712166478\n",
      "Epoch 441 / 500...\n",
      "train loss:  -18.528778515795437\n",
      "test loss:  -17.52751127809696\n",
      "train MSE:  117.62196905370526\n",
      "test MSE:  117.63175422609923\n",
      "Epoch 442 / 500...\n",
      "train loss:  -18.559656862808204\n",
      "test loss:  -17.55498674845155\n",
      "train MSE:  117.69506792708202\n",
      "test MSE:  117.70702581956604\n",
      "Epoch 443 / 500...\n",
      "train loss:  -18.58023838123819\n",
      "test loss:  -17.58275019470535\n",
      "train MSE:  117.76830780134581\n",
      "test MSE:  117.78245658705845\n",
      "Epoch 444 / 500...\n",
      "train loss:  -18.61332222796246\n",
      "test loss:  -17.61011611469016\n",
      "train MSE:  117.84264376039428\n",
      "test MSE:  117.85910920400059\n",
      "Epoch 445 / 500...\n",
      "train loss:  -18.644098229847508\n",
      "test loss:  -17.637343877658523\n",
      "train MSE:  117.91734293338256\n",
      "test MSE:  117.93612123585557\n",
      "Epoch 446 / 500...\n",
      "train loss:  -18.670772832856386\n",
      "test loss:  -17.664455808085762\n",
      "train MSE:  117.99240453173273\n",
      "test MSE:  118.01353930794468\n",
      "Epoch 447 / 500...\n",
      "train loss:  -18.696830545654095\n",
      "test loss:  -17.691951296032492\n",
      "train MSE:  118.06679248439286\n",
      "test MSE:  118.0901266413242\n",
      "Epoch 448 / 500...\n",
      "train loss:  -18.72915448646435\n",
      "test loss:  -17.7189284494197\n",
      "train MSE:  118.14204270488564\n",
      "test MSE:  118.16763723541214\n",
      "Epoch 449 / 500...\n",
      "train loss:  -18.759751448993644\n",
      "test loss:  -17.74562346242093\n",
      "train MSE:  118.21844103276813\n",
      "test MSE:  118.24648272964086\n",
      "Epoch 450 / 500...\n",
      "train loss:  -18.788356853597264\n",
      "test loss:  -17.77304485297617\n",
      "train MSE:  118.2934744917907\n",
      "test MSE:  118.32370338841521\n",
      "Epoch 451 / 500...\n",
      "train loss:  -18.81884039883607\n",
      "test loss:  -17.80004741573066\n",
      "train MSE:  118.36947959475039\n",
      "test MSE:  118.40199738850767\n",
      "Epoch 452 / 500...\n",
      "train loss:  -18.842728657142853\n",
      "test loss:  -17.827401712182496\n",
      "train MSE:  118.44500813691296\n",
      "test MSE:  118.4796987676786\n",
      "Epoch 453 / 500...\n",
      "train loss:  -18.881447581320955\n",
      "test loss:  -17.85437338565726\n",
      "train MSE:  118.52168687809863\n",
      "test MSE:  118.55867606286218\n",
      "Epoch 454 / 500...\n",
      "train loss:  -18.90999339987113\n",
      "test loss:  -17.881803180585838\n",
      "train MSE:  118.59728420749066\n",
      "test MSE:  118.63639030077337\n",
      "Epoch 455 / 500...\n",
      "train loss:  -18.932458900004637\n",
      "test loss:  -17.909017886690535\n",
      "train MSE:  118.67439969591575\n",
      "test MSE:  118.71580949119402\n",
      "Epoch 456 / 500...\n",
      "train loss:  -18.957229688596122\n",
      "test loss:  -17.93631366301136\n",
      "train MSE:  118.75037076992643\n",
      "test MSE:  118.79386836769764\n",
      "Epoch 457 / 500...\n",
      "train loss:  -18.990012299548628\n",
      "test loss:  -17.96333002102002\n",
      "train MSE:  118.82764725196678\n",
      "test MSE:  118.87338187231929\n",
      "Epoch 458 / 500...\n",
      "train loss:  -19.028397585699587\n",
      "test loss:  -17.99013927199446\n",
      "train MSE:  118.90584301212688\n",
      "test MSE:  118.9539537784574\n",
      "Epoch 459 / 500...\n",
      "train loss:  -19.044415402528763\n",
      "test loss:  -18.0171276745819\n",
      "train MSE:  118.98368410871761\n",
      "test MSE:  119.03410359109684\n",
      "Epoch 460 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -19.07464363433507\n",
      "test loss:  -18.043876798136584\n",
      "train MSE:  119.06173435601441\n",
      "test MSE:  119.11441241038159\n",
      "Epoch 461 / 500...\n",
      "train loss:  -19.103782389708776\n",
      "test loss:  -18.070723998052646\n",
      "train MSE:  119.13994117872956\n",
      "test MSE:  119.19488120914639\n",
      "Epoch 462 / 500...\n",
      "train loss:  -19.13824771705784\n",
      "test loss:  -18.098045726083825\n",
      "train MSE:  119.2173096597623\n",
      "test MSE:  119.27432397655282\n",
      "Epoch 463 / 500...\n",
      "train loss:  -19.16123166738393\n",
      "test loss:  -18.124959567995724\n",
      "train MSE:  119.29594162910087\n",
      "test MSE:  119.35523126498131\n",
      "Epoch 464 / 500...\n",
      "train loss:  -19.193696266525226\n",
      "test loss:  -18.15169388580381\n",
      "train MSE:  119.37565832171913\n",
      "test MSE:  119.43731395133952\n",
      "Epoch 465 / 500...\n",
      "train loss:  -19.214341130389712\n",
      "test loss:  -18.178492315749267\n",
      "train MSE:  119.4550438123442\n",
      "test MSE:  119.5190037843211\n",
      "Epoch 466 / 500...\n",
      "train loss:  -19.24871490422544\n",
      "test loss:  -18.20509936195787\n",
      "train MSE:  119.53430687462404\n",
      "test MSE:  119.60050373894599\n",
      "Epoch 467 / 500...\n",
      "train loss:  -19.273751016500956\n",
      "test loss:  -18.231873140708448\n",
      "train MSE:  119.61397820357674\n",
      "test MSE:  119.68244789563862\n",
      "Epoch 468 / 500...\n",
      "train loss:  -19.302680495180365\n",
      "test loss:  -18.259146091989205\n",
      "train MSE:  119.6924913371639\n",
      "test MSE:  119.7629960985537\n",
      "Epoch 469 / 500...\n",
      "train loss:  -19.326955290224685\n",
      "test loss:  -18.28646566430977\n",
      "train MSE:  119.77132292306496\n",
      "test MSE:  119.84392558205398\n",
      "Epoch 470 / 500...\n",
      "train loss:  -19.356610983665195\n",
      "test loss:  -18.313386305684347\n",
      "train MSE:  119.8514618262255\n",
      "test MSE:  119.92634101832857\n",
      "Epoch 471 / 500...\n",
      "train loss:  -19.38588576947271\n",
      "test loss:  -18.34034300512467\n",
      "train MSE:  119.93137566215766\n",
      "test MSE:  120.00844155791603\n",
      "Epoch 472 / 500...\n",
      "train loss:  -19.412392663598848\n",
      "test loss:  -18.36727605499777\n",
      "train MSE:  120.01171761396122\n",
      "test MSE:  120.09096367143154\n",
      "Epoch 473 / 500...\n",
      "train loss:  -19.44235287407632\n",
      "test loss:  -18.393869857939418\n",
      "train MSE:  120.09228849589657\n",
      "test MSE:  120.17369526122219\n",
      "Epoch 474 / 500...\n",
      "train loss:  -19.468823782312533\n",
      "test loss:  -18.420869084006696\n",
      "train MSE:  120.17227815728454\n",
      "test MSE:  120.25577266764958\n",
      "Epoch 475 / 500...\n",
      "train loss:  -19.498432500828446\n",
      "test loss:  -18.44772915117798\n",
      "train MSE:  120.2537248639695\n",
      "test MSE:  120.33949689871872\n",
      "Epoch 476 / 500...\n",
      "train loss:  -19.52990816759209\n",
      "test loss:  -18.473931410821088\n",
      "train MSE:  120.3363696494479\n",
      "test MSE:  120.42458192149203\n",
      "Epoch 477 / 500...\n",
      "train loss:  -19.561222102617705\n",
      "test loss:  -18.500117929253655\n",
      "train MSE:  120.41977925227\n",
      "test MSE:  120.51049896699449\n",
      "Epoch 478 / 500...\n",
      "train loss:  -19.58575653578959\n",
      "test loss:  -18.52704048575013\n",
      "train MSE:  120.50105455590338\n",
      "test MSE:  120.5939166439984\n",
      "Epoch 479 / 500...\n",
      "train loss:  -19.623634632597305\n",
      "test loss:  -18.553602938729387\n",
      "train MSE:  120.58347160843613\n",
      "test MSE:  120.6786177484673\n",
      "Epoch 480 / 500...\n",
      "train loss:  -19.650202234020345\n",
      "test loss:  -18.580612514402862\n",
      "train MSE:  120.66488868003877\n",
      "test MSE:  120.7620520437142\n",
      "Epoch 481 / 500...\n",
      "train loss:  -19.670674990746527\n",
      "test loss:  -18.606868250891075\n",
      "train MSE:  120.74863271833756\n",
      "test MSE:  120.84818763495919\n",
      "Epoch 482 / 500...\n",
      "train loss:  -19.690902510477272\n",
      "test loss:  -18.633809178806242\n",
      "train MSE:  120.83064514775245\n",
      "test MSE:  120.9323405318964\n",
      "Epoch 483 / 500...\n",
      "train loss:  -19.722539079136887\n",
      "test loss:  -18.660891300859504\n",
      "train MSE:  120.9117794310346\n",
      "test MSE:  121.0153520071\n",
      "Epoch 484 / 500...\n",
      "train loss:  -19.74919734671598\n",
      "test loss:  -18.687842010783974\n",
      "train MSE:  120.99374898237036\n",
      "test MSE:  121.09932614665658\n",
      "Epoch 485 / 500...\n",
      "train loss:  -19.787926551627123\n",
      "test loss:  -18.714133374258523\n",
      "train MSE:  121.07793243880688\n",
      "test MSE:  121.18586360233628\n",
      "Epoch 486 / 500...\n",
      "train loss:  -19.80855883035362\n",
      "test loss:  -18.74079760369897\n",
      "train MSE:  121.16160822495166\n",
      "test MSE:  121.27175046126541\n",
      "Epoch 487 / 500...\n",
      "train loss:  -19.834848705433238\n",
      "test loss:  -18.76713889545526\n",
      "train MSE:  121.2459450409917\n",
      "test MSE:  121.35838047350222\n",
      "Epoch 488 / 500...\n",
      "train loss:  -19.8598341956568\n",
      "test loss:  -18.793945672758785\n",
      "train MSE:  121.32924981528409\n",
      "test MSE:  121.4437540063694\n",
      "Epoch 489 / 500...\n",
      "train loss:  -19.899021444102612\n",
      "test loss:  -18.819838524884318\n",
      "train MSE:  121.41512284455416\n",
      "test MSE:  121.53210078903548\n",
      "Epoch 490 / 500...\n",
      "train loss:  -19.922714773961726\n",
      "test loss:  -18.84625408244561\n",
      "train MSE:  121.49968089305888\n",
      "test MSE:  121.61884029498128\n",
      "Epoch 491 / 500...\n",
      "train loss:  -19.947137721481024\n",
      "test loss:  -18.872952177529214\n",
      "train MSE:  121.5836446274953\n",
      "test MSE:  121.70482483051667\n",
      "Epoch 492 / 500...\n",
      "train loss:  -19.978662828031034\n",
      "test loss:  -18.899218047575058\n",
      "train MSE:  121.66890925093999\n",
      "test MSE:  121.7923135384326\n",
      "Epoch 493 / 500...\n",
      "train loss:  -20.003942654260836\n",
      "test loss:  -18.92619751465654\n",
      "train MSE:  121.75191359686163\n",
      "test MSE:  121.8771479036507\n",
      "Epoch 494 / 500...\n",
      "train loss:  -20.0428618658065\n",
      "test loss:  -18.952368239076645\n",
      "train MSE:  121.83839974026596\n",
      "test MSE:  121.96603817690456\n",
      "Epoch 495 / 500...\n",
      "train loss:  -20.066341860961277\n",
      "test loss:  -18.97847839160558\n",
      "train MSE:  121.92433046895171\n",
      "test MSE:  122.05422398187574\n",
      "Epoch 496 / 500...\n",
      "train loss:  -20.094443140224822\n",
      "test loss:  -19.005120667082135\n",
      "train MSE:  122.0097578478507\n",
      "test MSE:  122.14179795594904\n",
      "Epoch 497 / 500...\n",
      "train loss:  -20.11439266427544\n",
      "test loss:  -19.03180663352455\n",
      "train MSE:  122.0947343614964\n",
      "test MSE:  122.22877176937837\n",
      "Epoch 498 / 500...\n",
      "train loss:  -20.14163042499004\n",
      "test loss:  -19.05838592618918\n",
      "train MSE:  122.18093151220576\n",
      "test MSE:  122.3172025260207\n",
      "Epoch 499 / 500...\n",
      "train loss:  -20.178529786634858\n",
      "test loss:  -19.084561753547803\n",
      "train MSE:  122.26754476617332\n",
      "test MSE:  122.40599463268107\n",
      "Epoch 500 / 500...\n",
      "train loss:  -20.19985911127485\n",
      "test loss:  -19.11104928918165\n",
      "train MSE:  122.35365245524878\n",
      "test MSE:  122.49418448621257\n"
     ]
    }
   ],
   "source": [
    "# ==========  Poisson Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_mse_arr, test_mse_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the weights\n",
    "regr.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.72606185],\n",
       "       [ -6.14030572],\n",
       "       [ -5.82944917],\n",
       "       [ -2.95568921],\n",
       "       [ -6.89330173],\n",
       "       [ -6.8706404 ],\n",
       "       [ -6.28919821],\n",
       "       [ -1.70994999],\n",
       "       [ -2.85672681],\n",
       "       [-22.44216783],\n",
       "       [  0.50697497],\n",
       "       [ -2.66193985],\n",
       "       [ -0.62060526],\n",
       "       [ -8.91248448],\n",
       "       [ -2.79876902],\n",
       "       [ -2.52727487],\n",
       "       [ -3.76870775],\n",
       "       [ -3.48042063],\n",
       "       [ -4.21772827],\n",
       "       [ -2.09322842],\n",
       "       [ -3.0315747 ],\n",
       "       [ -3.21220148],\n",
       "       [ -9.45713574],\n",
       "       [ -3.24707102],\n",
       "       [ -1.87185348],\n",
       "       [ -3.03414263],\n",
       "       [ -4.05499774],\n",
       "       [ -1.03467248],\n",
       "       [ -0.37951301],\n",
       "       [ -1.45694014],\n",
       "       [ -4.41609655],\n",
       "       [-20.18059186],\n",
       "       [ -5.13050564],\n",
       "       [ -3.72264187],\n",
       "       [-15.8542955 ],\n",
       "       [ -6.09727862],\n",
       "       [ -2.56863545],\n",
       "       [ -5.49204598],\n",
       "       [ -2.86193008],\n",
       "       [ -4.50307792],\n",
       "       [ -3.36787259],\n",
       "       [ -7.12586482],\n",
       "       [ -1.18037291],\n",
       "       [ -7.21178707],\n",
       "       [ -3.79780335],\n",
       "       [-13.05836263],\n",
       "       [-16.05754669],\n",
       "       [ -7.82840175],\n",
       "       [-13.58673811],\n",
       "       [ -8.9465514 ],\n",
       "       [ -6.5651948 ],\n",
       "       [ -6.88680759],\n",
       "       [ -3.5075289 ],\n",
       "       [ -5.80565116],\n",
       "       [ -8.0703252 ],\n",
       "       [ -3.34177777],\n",
       "       [ -8.87618743],\n",
       "       [-12.97352967],\n",
       "       [ -4.19988498],\n",
       "       [ -7.0373665 ]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[2050:2110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [ -0.49592299],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [  0.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [ 10.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  9.50407701],\n",
       "       [  6.50407701],\n",
       "       [  6.50407701],\n",
       "       [-15.49592299],\n",
       "       [ -5.49592299],\n",
       "       [-21.49592299],\n",
       "       [-17.49592299],\n",
       "       [-17.49592299],\n",
       "       [ -5.49592299],\n",
       "       [-21.49592299],\n",
       "       [ -5.49592299],\n",
       "       [  3.50407701],\n",
       "       [-16.49592299],\n",
       "       [ -5.49592299],\n",
       "       [-13.49592299],\n",
       "       [-16.49592299],\n",
       "       [ -5.49592299],\n",
       "       [-21.49592299]])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_sm[2050:2110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
