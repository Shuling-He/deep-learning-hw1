{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate =  0.00000001\n",
    "        self.epochs = 250\n",
    "        self.batch_size = 100\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.5  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "        \n",
    "        # calc feature-wise mean\n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        # calc feature-wise std\n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x_norm = x - mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x_norm = x_norm / std\n",
    "\n",
    "        return x_norm, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        \n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        temp = label - self.y_mean\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def train_loss(self, x, yt_sm):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.exp(np.dot(x, self.weights))\n",
    "        \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt_sm\n",
    "    \n",
    "        # calc the diff, and divide\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def test_loss(self, x, yt_sm):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        # this function adds the y mean back\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "\n",
    "        # predict\n",
    "        pred_y = np.exp(np.dot(x, self.weights))\n",
    "    \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        yt = yt_sm\n",
    "    \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt\n",
    "        \n",
    "    \n",
    "        # calc the diff, divide, and add the mean\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "        y_pred = np.exp(np.dot(x, self.weights))\n",
    "\n",
    "        dW = np.dot(x.T, (y_pred - yt_sm).reshape(-1)).reshape(-1, 1)\n",
    "        \n",
    "        # return the avg dW\n",
    "        return dW \n",
    "\n",
    "    def calc_mse(self, x, y_sm):\n",
    "        # preprocesses (adds the y_mean back to both x and y, and calls musicMSE)\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # add the y mean to the pred and convert to int to round\n",
    "#         pred_y += self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        pred_y = pred_y\n",
    "        \n",
    "        # convert to int to round\n",
    "        y_labels = y_sm\n",
    "        \n",
    "        # calc the MSE\n",
    "        mse = self.musicMSE(pred_y, y_labels)\n",
    "        \n",
    "        return mse, pred_y\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            \n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Epoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            test_loss = self.test_phase(x_test, y_test_sm)\n",
    "            \n",
    "            train_mse, train_preds = self.calc_mse(x_train, y_train_sm)\n",
    "            test_mse, test_preds = self.calc_mse(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "        \n",
    "#         return train_losses, test_losses\n",
    "\n",
    "        # return all the vals\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds\n",
    "    \n",
    "    def plot_graph(self, train_losses, test_losses, train_mse, test_mse):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Poisson: Loss vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse, label=\"Train MSE\")\n",
    "        plt.plot(test_mse, label=\"Test MSE\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Poisson: MSE vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_weights(self):\n",
    "        \n",
    "        plt.hist(self.weights, bins=12)\n",
    "        plt.xlabel('bins')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Lasso Regression Weights Histogram')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "# x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 250...\n",
      "train loss:  0.8919680467929182\n",
      "test loss:  0.7885788332697007\n",
      "train MSE:  119.25695790700757\n",
      "test MSE:  117.34768365072888\n",
      "Epoch 2 / 250...\n",
      "train loss:  0.6800086558031794\n",
      "test loss:  0.5841211885639227\n",
      "train MSE:  118.84973802335065\n",
      "test MSE:  116.9466871872668\n",
      "Epoch 3 / 250...\n",
      "train loss:  0.47505868731726447\n",
      "test loss:  0.3864746972991066\n",
      "train MSE:  118.45650056099974\n",
      "test MSE:  116.55981161496548\n",
      "Epoch 4 / 250...\n",
      "train loss:  0.2766286915617388\n",
      "test loss:  0.19534714453261123\n",
      "train MSE:  118.07662989049592\n",
      "test MSE:  116.18648771721459\n",
      "Epoch 5 / 250...\n",
      "train loss:  0.08449900513335215\n",
      "test loss:  0.010571674803389471\n",
      "train MSE:  117.70966153771472\n",
      "test MSE:  115.82630879469545\n",
      "Epoch 6 / 250...\n",
      "train loss:  -0.10144225562653703\n",
      "test loss:  -0.1679446871661665\n",
      "train MSE:  117.35524004397142\n",
      "test MSE:  115.47895383094453\n",
      "Epoch 7 / 250...\n",
      "train loss:  -0.2813248390894366\n",
      "test loss:  -0.3402494489465784\n",
      "train MSE:  117.01307998561298\n",
      "test MSE:  115.14416716040644\n",
      "Epoch 8 / 250...\n",
      "train loss:  -0.45481954295379623\n",
      "test loss:  -0.5063518300849145\n",
      "train MSE:  116.68297625267547\n",
      "test MSE:  114.82177623749548\n",
      "Epoch 9 / 250...\n",
      "train loss:  -0.6222865212381319\n",
      "test loss:  -0.6662499779383741\n",
      "train MSE:  116.36477245023708\n",
      "test MSE:  114.51165817256805\n",
      "Epoch 10 / 250...\n",
      "train loss:  -0.7844646933125701\n",
      "test loss:  -0.8199926906704298\n",
      "train MSE:  116.05826252017341\n",
      "test MSE:  114.21362137831629\n",
      "Epoch 11 / 250...\n",
      "train loss:  -0.939651842464276\n",
      "test loss:  -0.9675897282751843\n",
      "train MSE:  115.76338679070636\n",
      "test MSE:  113.92762380776499\n",
      "Epoch 12 / 250...\n",
      "train loss:  -1.089634646051846\n",
      "test loss:  -1.1091456940354774\n",
      "train MSE:  115.47997094843991\n",
      "test MSE:  113.65349367141489\n",
      "Epoch 13 / 250...\n",
      "train loss:  -1.2332958098533175\n",
      "test loss:  -1.2447978486484856\n",
      "train MSE:  115.20782603944475\n",
      "test MSE:  113.39104174279906\n",
      "Epoch 14 / 250...\n",
      "train loss:  -1.371730929370446\n",
      "test loss:  -1.374744756319894\n",
      "train MSE:  114.94672555963184\n",
      "test MSE:  113.14005380135114\n",
      "Epoch 15 / 250...\n",
      "train loss:  -1.5030805994472907\n",
      "test loss:  -1.4991332019283554\n",
      "train MSE:  114.69663613153448\n",
      "test MSE:  112.9004721909749\n",
      "Epoch 16 / 250...\n",
      "train loss:  -1.6305598374922623\n",
      "test loss:  -1.6183380855484095\n",
      "train MSE:  114.45695436344839\n",
      "test MSE:  112.67165110643435\n",
      "Epoch 17 / 250...\n",
      "train loss:  -1.7524945950080426\n",
      "test loss:  -1.7324598604226034\n",
      "train MSE:  114.22791027384619\n",
      "test MSE:  112.4538308376559\n",
      "Epoch 18 / 250...\n",
      "train loss:  -1.869474160936887\n",
      "test loss:  -1.8419041678064783\n",
      "train MSE:  114.00885743761033\n",
      "test MSE:  112.24630518615763\n",
      "Epoch 19 / 250...\n",
      "train loss:  -1.9832205462919457\n",
      "test loss:  -1.9469650016824303\n",
      "train MSE:  113.79941562009901\n",
      "test MSE:  112.04866380692317\n",
      "Epoch 20 / 250...\n",
      "train loss:  -2.0912320562342\n",
      "test loss:  -2.04795237711865\n",
      "train MSE:  113.59916194225801\n",
      "test MSE:  111.86043993598041\n",
      "Epoch 21 / 250...\n",
      "train loss:  -2.1950045008709695\n",
      "test loss:  -2.1451252238470895\n",
      "train MSE:  113.40775009144255\n",
      "test MSE:  111.68121715141586\n",
      "Epoch 22 / 250...\n",
      "train loss:  -2.296128111147448\n",
      "test loss:  -2.238859715124925\n",
      "train MSE:  113.22426600639172\n",
      "test MSE:  111.51001003433424\n",
      "Epoch 23 / 250...\n",
      "train loss:  -2.394475459996167\n",
      "test loss:  -2.3292911461985097\n",
      "train MSE:  113.04880473402513\n",
      "test MSE:  111.34695294377579\n",
      "Epoch 24 / 250...\n",
      "train loss:  -2.4888242421110203\n",
      "test loss:  -2.4167232510880385\n",
      "train MSE:  112.88068948054843\n",
      "test MSE:  111.19125989261772\n",
      "Epoch 25 / 250...\n",
      "train loss:  -2.5802612156533615\n",
      "test loss:  -2.501355134486305\n",
      "train MSE:  112.7193688586348\n",
      "test MSE:  111.04235432464422\n",
      "Epoch 26 / 250...\n",
      "train loss:  -2.6696750087543406\n",
      "test loss:  -2.5833941669928056\n",
      "train MSE:  112.56479016894333\n",
      "test MSE:  110.90018623626239\n",
      "Epoch 27 / 250...\n",
      "train loss:  -2.756900402792732\n",
      "test loss:  -2.6630131904264407\n",
      "train MSE:  112.41645874024243\n",
      "test MSE:  110.76422947726137\n",
      "Epoch 28 / 250...\n",
      "train loss:  -2.8411280364080223\n",
      "test loss:  -2.7405540535628767\n",
      "train MSE:  112.27299380293839\n",
      "test MSE:  110.63297227200569\n",
      "Epoch 29 / 250...\n",
      "train loss:  -2.922741796580635\n",
      "test loss:  -2.8159449290521557\n",
      "train MSE:  112.13579852968535\n",
      "test MSE:  110.50797647151238\n",
      "Epoch 30 / 250...\n",
      "train loss:  -3.00451305850804\n",
      "test loss:  -2.889555830761859\n",
      "train MSE:  112.002827129239\n",
      "test MSE:  110.38698062083314\n",
      "Epoch 31 / 250...\n",
      "train loss:  -3.0846336734250515\n",
      "test loss:  -2.9614586286687765\n",
      "train MSE:  111.87428837090373\n",
      "test MSE:  110.27028192395831\n",
      "Epoch 32 / 250...\n",
      "train loss:  -3.160958620796898\n",
      "test loss:  -3.0317388216620067\n",
      "train MSE:  111.75026756731826\n",
      "test MSE:  110.15797305685838\n",
      "Epoch 33 / 250...\n",
      "train loss:  -3.2360260583398843\n",
      "test loss:  -3.10056319004033\n",
      "train MSE:  111.63019505690404\n",
      "test MSE:  110.04943382773752\n",
      "Epoch 34 / 250...\n",
      "train loss:  -3.3102501042853\n",
      "test loss:  -3.1680185088218247\n",
      "train MSE:  111.51391727430708\n",
      "test MSE:  109.94451680441969\n",
      "Epoch 35 / 250...\n",
      "train loss:  -3.382883884314158\n",
      "test loss:  -3.2342292221300997\n",
      "train MSE:  111.40106630242617\n",
      "test MSE:  109.84280636096696\n",
      "Epoch 36 / 250...\n",
      "train loss:  -3.455008830080916\n",
      "test loss:  -3.299220145327128\n",
      "train MSE:  111.2920625577967\n",
      "test MSE:  109.74483413316321\n",
      "Epoch 37 / 250...\n",
      "train loss:  -3.5257414055766465\n",
      "test loss:  -3.3631571722256073\n",
      "train MSE:  111.18565442415567\n",
      "test MSE:  109.64920992774107\n",
      "Epoch 38 / 250...\n",
      "train loss:  -3.59505947509371\n",
      "test loss:  -3.4260659598567846\n",
      "train MSE:  111.08255851872464\n",
      "test MSE:  109.55674747892687\n",
      "Epoch 39 / 250...\n",
      "train loss:  -3.662261368047868\n",
      "test loss:  -3.4880313018969393\n",
      "train MSE:  110.98182866651584\n",
      "test MSE:  109.4664089539954\n",
      "Epoch 40 / 250...\n",
      "train loss:  -3.728977413438774\n",
      "test loss:  -3.549115626202557\n",
      "train MSE:  110.88367683618459\n",
      "test MSE:  109.37845010184553\n",
      "Epoch 41 / 250...\n",
      "train loss:  -3.7956892386942984\n",
      "test loss:  -3.609342695038989\n",
      "train MSE:  110.7883603326975\n",
      "test MSE:  109.29317669478517\n",
      "Epoch 42 / 250...\n",
      "train loss:  -3.86142634150763\n",
      "test loss:  -3.668762254363074\n",
      "train MSE:  110.69524156139133\n",
      "test MSE:  109.20990987925376\n",
      "Epoch 43 / 250...\n",
      "train loss:  -3.924825321693124\n",
      "test loss:  -3.727409594211299\n",
      "train MSE:  110.60502141849612\n",
      "test MSE:  109.12941985086164\n",
      "Epoch 44 / 250...\n",
      "train loss:  -3.9885163617505133\n",
      "test loss:  -3.785330732882912\n",
      "train MSE:  110.51657263047554\n",
      "test MSE:  109.05048003910704\n",
      "Epoch 45 / 250...\n",
      "train loss:  -4.051035361840108\n",
      "test loss:  -3.8426027385290755\n",
      "train MSE:  110.4293850127604\n",
      "test MSE:  108.97255422248396\n",
      "Epoch 46 / 250...\n",
      "train loss:  -4.112925286002416\n",
      "test loss:  -3.899176479747662\n",
      "train MSE:  110.34493647260238\n",
      "test MSE:  108.89728381767783\n",
      "Epoch 47 / 250...\n",
      "train loss:  -4.172860162522451\n",
      "test loss:  -3.955198406056258\n",
      "train MSE:  110.26178699325165\n",
      "test MSE:  108.82306459615023\n",
      "Epoch 48 / 250...\n",
      "train loss:  -4.235605188265895\n",
      "test loss:  -4.010585940591163\n",
      "train MSE:  110.18075369815703\n",
      "test MSE:  108.75083374759816\n",
      "Epoch 49 / 250...\n",
      "train loss:  -4.29289434761571\n",
      "test loss:  -4.065384089101611\n",
      "train MSE:  110.1011580450045\n",
      "test MSE:  108.67986129384177\n",
      "Epoch 50 / 250...\n",
      "train loss:  -4.351423221654017\n",
      "test loss:  -4.11967545866868\n",
      "train MSE:  110.02374161324563\n",
      "test MSE:  108.61093589910436\n",
      "Epoch 51 / 250...\n",
      "train loss:  -4.410669540456678\n",
      "test loss:  -4.173408446046025\n",
      "train MSE:  109.94801177291232\n",
      "test MSE:  108.54356975406031\n",
      "Epoch 52 / 250...\n",
      "train loss:  -4.468194746804489\n",
      "test loss:  -4.2265820175919995\n",
      "train MSE:  109.87352922429753\n",
      "test MSE:  108.47729357339033\n",
      "Epoch 53 / 250...\n",
      "train loss:  -4.526399410741586\n",
      "test loss:  -4.279351610685129\n",
      "train MSE:  109.8007687223096\n",
      "test MSE:  108.41259558269886\n",
      "Epoch 54 / 250...\n",
      "train loss:  -4.582303576298409\n",
      "test loss:  -4.331552079653727\n",
      "train MSE:  109.72982391052516\n",
      "test MSE:  108.3496547254306\n",
      "Epoch 55 / 250...\n",
      "train loss:  -4.637464562109231\n",
      "test loss:  -4.383349006349628\n",
      "train MSE:  109.660537847941\n",
      "test MSE:  108.28823073822484\n",
      "Epoch 56 / 250...\n",
      "train loss:  -4.691801345707135\n",
      "test loss:  -4.4346807302188305\n",
      "train MSE:  109.59288456675591\n",
      "test MSE:  108.22835119574823\n",
      "Epoch 57 / 250...\n",
      "train loss:  -4.748156402900102\n",
      "test loss:  -4.485601451797644\n",
      "train MSE:  109.52694337933043\n",
      "test MSE:  108.17007942324828\n",
      "Epoch 58 / 250...\n",
      "train loss:  -4.801858766088164\n",
      "test loss:  -4.536142796260681\n",
      "train MSE:  109.46147257789721\n",
      "test MSE:  108.11210373577352\n",
      "Epoch 59 / 250...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -4.856803826691194\n",
      "test loss:  -4.586326515081904\n",
      "train MSE:  109.3974722472609\n",
      "test MSE:  108.05546494184075\n",
      "Epoch 60 / 250...\n",
      "train loss:  -4.909248548361138\n",
      "test loss:  -4.635998147349997\n",
      "train MSE:  109.33496971344606\n",
      "test MSE:  108.00028719385114\n",
      "Epoch 61 / 250...\n",
      "train loss:  -4.9631778402492595\n",
      "test loss:  -4.68544080732812\n",
      "train MSE:  109.27342972112035\n",
      "test MSE:  107.94591355239605\n",
      "Epoch 62 / 250...\n",
      "train loss:  -5.014674186988866\n",
      "test loss:  -4.734479997353287\n",
      "train MSE:  109.21228778831961\n",
      "test MSE:  107.89175240489253\n",
      "Epoch 63 / 250...\n",
      "train loss:  -5.064631668973413\n",
      "test loss:  -4.7831196444536825\n",
      "train MSE:  109.15363473523247\n",
      "test MSE:  107.8401462935015\n",
      "Epoch 64 / 250...\n",
      "train loss:  -5.117128998091533\n",
      "test loss:  -4.831339776573294\n",
      "train MSE:  109.0963104605727\n",
      "test MSE:  107.7898030107343\n",
      "Epoch 65 / 250...\n",
      "train loss:  -5.168230303934679\n",
      "test loss:  -4.879264698489105\n",
      "train MSE:  109.04020789147907\n",
      "test MSE:  107.74061424513482\n",
      "Epoch 66 / 250...\n",
      "train loss:  -5.219755071177518\n",
      "test loss:  -4.92689134449026\n",
      "train MSE:  108.9849873030253\n",
      "test MSE:  107.69219204727203\n",
      "Epoch 67 / 250...\n",
      "train loss:  -5.268725208839181\n",
      "test loss:  -4.974171249335506\n",
      "train MSE:  108.93158119740453\n",
      "test MSE:  107.64559738380451\n",
      "Epoch 68 / 250...\n",
      "train loss:  -5.319491484819714\n",
      "test loss:  -5.021309394874936\n",
      "train MSE:  108.87819029047512\n",
      "test MSE:  107.59879967465463\n",
      "Epoch 69 / 250...\n",
      "train loss:  -5.369703375250235\n",
      "test loss:  -5.068093803566071\n",
      "train MSE:  108.8261566033243\n",
      "test MSE:  107.5533177580319\n",
      "Epoch 70 / 250...\n",
      "train loss:  -5.418384474930015\n",
      "test loss:  -5.114528986399448\n",
      "train MSE:  108.77559897993052\n",
      "test MSE:  107.50928267064515\n",
      "Epoch 71 / 250...\n",
      "train loss:  -5.46821379896797\n",
      "test loss:  -5.160690929949403\n",
      "train MSE:  108.72541951679753\n",
      "test MSE:  107.46549885436197\n",
      "Epoch 72 / 250...\n",
      "train loss:  -5.517407303287823\n",
      "test loss:  -5.2066727665825985\n",
      "train MSE:  108.67735099260065\n",
      "test MSE:  107.42387524889054\n",
      "Epoch 73 / 250...\n",
      "train loss:  -5.56517192397372\n",
      "test loss:  -5.252497638940515\n",
      "train MSE:  108.62861940322092\n",
      "test MSE:  107.38132627997673\n",
      "Epoch 74 / 250...\n",
      "train loss:  -5.615450644755504\n",
      "test loss:  -5.298060456788161\n",
      "train MSE:  108.58142584763428\n",
      "test MSE:  107.34031754740515\n",
      "Epoch 75 / 250...\n",
      "train loss:  -5.659673994936223\n",
      "test loss:  -5.343350203939171\n",
      "train MSE:  108.53523880555225\n",
      "test MSE:  107.30028416719446\n",
      "Epoch 76 / 250...\n",
      "train loss:  -5.706835161880844\n",
      "test loss:  -5.388356254605364\n",
      "train MSE:  108.49035574190138\n",
      "test MSE:  107.26149936799744\n",
      "Epoch 77 / 250...\n",
      "train loss:  -5.756508201429503\n",
      "test loss:  -5.433172340948011\n",
      "train MSE:  108.44623608718366\n",
      "test MSE:  107.22343052433818\n",
      "Epoch 78 / 250...\n",
      "train loss:  -5.8038693085772675\n",
      "test loss:  -5.477805027261965\n",
      "train MSE:  108.40258824707077\n",
      "test MSE:  107.18575955882623\n",
      "Epoch 79 / 250...\n",
      "train loss:  -5.84852884068136\n",
      "test loss:  -5.52212913434476\n",
      "train MSE:  108.36049714060452\n",
      "test MSE:  107.1496641111121\n",
      "Epoch 80 / 250...\n",
      "train loss:  -5.894209220686688\n",
      "test loss:  -5.566348411675656\n",
      "train MSE:  108.31833763256024\n",
      "test MSE:  107.11331149995746\n",
      "Epoch 81 / 250...\n",
      "train loss:  -5.9412987959803525\n",
      "test loss:  -5.610523922056986\n",
      "train MSE:  108.2756378721717\n",
      "test MSE:  107.07621180230247\n",
      "Epoch 82 / 250...\n",
      "train loss:  -5.98836116103955\n",
      "test loss:  -5.654445990764835\n",
      "train MSE:  108.2352616298374\n",
      "test MSE:  107.04155636204801\n",
      "Epoch 83 / 250...\n",
      "train loss:  -6.035893607694249\n",
      "test loss:  -5.698149188820692\n",
      "train MSE:  108.19489583929062\n",
      "test MSE:  107.00678271127335\n",
      "Epoch 84 / 250...\n",
      "train loss:  -6.079803519764806\n",
      "test loss:  -5.741690885318752\n",
      "train MSE:  108.15527132485144\n",
      "test MSE:  106.97268153955771\n",
      "Epoch 85 / 250...\n",
      "train loss:  -6.126492048088751\n",
      "test loss:  -5.785086815271383\n",
      "train MSE:  108.11696687325833\n",
      "test MSE:  106.93990723071461\n",
      "Epoch 86 / 250...\n",
      "train loss:  -6.170888249323527\n",
      "test loss:  -5.82829732834881\n",
      "train MSE:  108.07910871803864\n",
      "test MSE:  106.90752841065363\n",
      "Epoch 87 / 250...\n",
      "train loss:  -6.217558591202444\n",
      "test loss:  -5.871366118655204\n",
      "train MSE:  108.04160245117286\n",
      "test MSE:  106.87540556106578\n",
      "Epoch 88 / 250...\n",
      "train loss:  -6.262495139306917\n",
      "test loss:  -5.91417217946413\n",
      "train MSE:  108.00561170192843\n",
      "test MSE:  106.84486858186038\n",
      "Epoch 89 / 250...\n",
      "train loss:  -6.307591132996742\n",
      "test loss:  -5.956941534212254\n",
      "train MSE:  107.96947232132325\n",
      "test MSE:  106.81400690739163\n",
      "Epoch 90 / 250...\n",
      "train loss:  -6.353421643077608\n",
      "test loss:  -5.999549576473949\n",
      "train MSE:  107.9329948475586\n",
      "test MSE:  106.78267292228536\n",
      "Epoch 91 / 250...\n",
      "train loss:  -6.39615816757511\n",
      "test loss:  -6.041919662883786\n",
      "train MSE:  107.89858548281741\n",
      "test MSE:  106.75354482622201\n",
      "Epoch 92 / 250...\n",
      "train loss:  -6.44059155379262\n",
      "test loss:  -6.084220300055944\n",
      "train MSE:  107.8642117495456\n",
      "test MSE:  106.72435663980224\n",
      "Epoch 93 / 250...\n",
      "train loss:  -6.484243442860193\n",
      "test loss:  -6.126360408240562\n",
      "train MSE:  107.83122766979965\n",
      "test MSE:  106.69656412907136\n",
      "Epoch 94 / 250...\n",
      "train loss:  -6.53159871206397\n",
      "test loss:  -6.1683289040916165\n",
      "train MSE:  107.7986451443916\n",
      "test MSE:  106.6691236601656\n",
      "Epoch 95 / 250...\n",
      "train loss:  -6.571645492689227\n",
      "test loss:  -6.210086325508104\n",
      "train MSE:  107.76721044612415\n",
      "test MSE:  106.64287955081069\n",
      "Epoch 96 / 250...\n",
      "train loss:  -6.617472199280176\n",
      "test loss:  -6.251782775418463\n",
      "train MSE:  107.73582673795293\n",
      "test MSE:  106.61657746139562\n",
      "Epoch 97 / 250...\n",
      "train loss:  -6.661263521158177\n",
      "test loss:  -6.293299512185002\n",
      "train MSE:  107.70573902305544\n",
      "test MSE:  106.59167437684414\n",
      "Epoch 98 / 250...\n",
      "train loss:  -6.703568016025571\n",
      "test loss:  -6.334598345496524\n",
      "train MSE:  107.67678417934422\n",
      "test MSE:  106.56790593465713\n",
      "Epoch 99 / 250...\n",
      "train loss:  -6.748468171952255\n",
      "test loss:  -6.37594593380195\n",
      "train MSE:  107.64587930817889\n",
      "test MSE:  106.5418260358527\n",
      "Epoch 100 / 250...\n",
      "train loss:  -6.789613442150767\n",
      "test loss:  -6.417092476088974\n",
      "train MSE:  107.61642731480319\n",
      "test MSE:  106.51732729416129\n",
      "Epoch 101 / 250...\n",
      "train loss:  -6.834375688715277\n",
      "test loss:  -6.458280359765482\n",
      "train MSE:  107.58631455244276\n",
      "test MSE:  106.49197148403267\n",
      "Epoch 102 / 250...\n",
      "train loss:  -6.877221546115289\n",
      "test loss:  -6.49919780514356\n",
      "train MSE:  107.55869131229906\n",
      "test MSE:  106.46929371713577\n",
      "Epoch 103 / 250...\n",
      "train loss:  -6.918406589206011\n",
      "test loss:  -6.5400627331656205\n",
      "train MSE:  107.5308709941632\n",
      "test MSE:  106.44629062475532\n",
      "Epoch 104 / 250...\n",
      "train loss:  -6.9599335802403965\n",
      "test loss:  -6.580787356621936\n",
      "train MSE:  107.50351316493976\n",
      "test MSE:  106.42372568029144\n",
      "Epoch 105 / 250...\n",
      "train loss:  -7.0055384769063584\n",
      "test loss:  -6.6212524636429215\n",
      "train MSE:  107.47760503428817\n",
      "test MSE:  106.40270220922646\n",
      "Epoch 106 / 250...\n",
      "train loss:  -7.047775480699425\n",
      "test loss:  -6.661702958986077\n",
      "train MSE:  107.45122851790151\n",
      "test MSE:  106.38108456381094\n",
      "Epoch 107 / 250...\n",
      "train loss:  -7.087153921259138\n",
      "test loss:  -6.701912148488112\n",
      "train MSE:  107.4264476515809\n",
      "test MSE:  106.36118352669446\n",
      "Epoch 108 / 250...\n",
      "train loss:  -7.129353254478246\n",
      "test loss:  -6.742076502254348\n",
      "train MSE:  107.40185648501158\n",
      "test MSE:  106.34137965358396\n",
      "Epoch 109 / 250...\n",
      "train loss:  -7.17342187834722\n",
      "test loss:  -6.78217000111479\n",
      "train MSE:  107.37676972047082\n",
      "test MSE:  106.32095326640211\n",
      "Epoch 110 / 250...\n",
      "train loss:  -7.216253047157448\n",
      "test loss:  -6.822194860828681\n",
      "train MSE:  107.35206816242788\n",
      "test MSE:  106.3008394229357\n",
      "Epoch 111 / 250...\n",
      "train loss:  -7.257236060208317\n",
      "test loss:  -6.862153236904392\n",
      "train MSE:  107.32801981826219\n",
      "test MSE:  106.28136581871605\n",
      "Epoch 112 / 250...\n",
      "train loss:  -7.298664174341145\n",
      "test loss:  -6.901834279102659\n",
      "train MSE:  107.30538297102281\n",
      "test MSE:  106.26344721681622\n",
      "Epoch 113 / 250...\n",
      "train loss:  -7.341537612835835\n",
      "test loss:  -6.941502307423245\n",
      "train MSE:  107.28245682246664\n",
      "test MSE:  106.24510497951702\n",
      "Epoch 114 / 250...\n",
      "train loss:  -7.381192701394615\n",
      "test loss:  -6.981067464950571\n",
      "train MSE:  107.26007186707102\n",
      "test MSE:  106.22729885465468\n",
      "Epoch 115 / 250...\n",
      "train loss:  -7.423932047494656\n",
      "test loss:  -7.02052016773372\n",
      "train MSE:  107.2384451314338\n",
      "test MSE:  106.2102471376921\n",
      "Epoch 116 / 250...\n",
      "train loss:  -7.462627549348125\n",
      "test loss:  -7.05987680046305\n",
      "train MSE:  107.21732952064663\n",
      "test MSE:  106.19367992277358\n",
      "Epoch 117 / 250...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -7.509489516484919\n",
      "test loss:  -7.099074474207232\n",
      "train MSE:  107.19741643384512\n",
      "test MSE:  106.17838419370763\n",
      "Epoch 118 / 250...\n",
      "train loss:  -7.548223689565908\n",
      "test loss:  -7.13814393579772\n",
      "train MSE:  107.17734925699234\n",
      "test MSE:  106.16292736861553\n",
      "Epoch 119 / 250...\n",
      "train loss:  -7.586838097681849\n",
      "test loss:  -7.1771569530439745\n",
      "train MSE:  107.15748514320204\n",
      "test MSE:  106.14757368972269\n",
      "Epoch 120 / 250...\n",
      "train loss:  -7.626447976618238\n",
      "test loss:  -7.21611727990875\n",
      "train MSE:  107.13737324400115\n",
      "test MSE:  106.13182875626532\n",
      "Epoch 121 / 250...\n",
      "train loss:  -7.671423250047159\n",
      "test loss:  -7.255012558104677\n",
      "train MSE:  107.11755862548873\n",
      "test MSE:  106.11641545057168\n",
      "Epoch 122 / 250...\n",
      "train loss:  -7.711208217162784\n",
      "test loss:  -7.293689403173662\n",
      "train MSE:  107.09971903128582\n",
      "test MSE:  106.10307200575797\n",
      "Epoch 123 / 250...\n",
      "train loss:  -7.752513570642096\n",
      "test loss:  -7.3323252408229775\n",
      "train MSE:  107.0819867267596\n",
      "test MSE:  106.0898024824574\n",
      "Epoch 124 / 250...\n",
      "train loss:  -7.793300640747248\n",
      "test loss:  -7.370719727138873\n",
      "train MSE:  107.06617109905264\n",
      "test MSE:  106.078617337189\n",
      "Epoch 125 / 250...\n",
      "train loss:  -7.834732864280445\n",
      "test loss:  -7.409174178055162\n",
      "train MSE:  107.0492871322789\n",
      "test MSE:  106.0661103029368\n",
      "Epoch 126 / 250...\n",
      "train loss:  -7.870562655627055\n",
      "test loss:  -7.447519556906937\n",
      "train MSE:  107.03264674499613\n",
      "test MSE:  106.05383144610258\n",
      "Epoch 127 / 250...\n",
      "train loss:  -7.911047756418736\n",
      "test loss:  -7.485738615457457\n",
      "train MSE:  107.01624239952073\n",
      "test MSE:  106.04177089591298\n",
      "Epoch 128 / 250...\n",
      "train loss:  -7.952103486656572\n",
      "test loss:  -7.523916825042259\n",
      "train MSE:  107.00005599006647\n",
      "test MSE:  106.02988665345451\n",
      "Epoch 129 / 250...\n",
      "train loss:  -7.992335353894323\n",
      "test loss:  -7.561964411072898\n",
      "train MSE:  106.98462768060952\n",
      "test MSE:  106.01875667276315\n",
      "Epoch 130 / 250...\n",
      "train loss:  -8.02948554525057\n",
      "test loss:  -7.5998934990337865\n",
      "train MSE:  106.96988321817526\n",
      "test MSE:  106.00836780639455\n",
      "Epoch 131 / 250...\n",
      "train loss:  -8.075589433717829\n",
      "test loss:  -7.637728577057509\n",
      "train MSE:  106.95592209710362\n",
      "test MSE:  105.99876870927412\n",
      "Epoch 132 / 250...\n",
      "train loss:  -8.109798162877572\n",
      "test loss:  -7.675512467973834\n",
      "train MSE:  106.9420635033694\n",
      "test MSE:  105.98919543866648\n",
      "Epoch 133 / 250...\n",
      "train loss:  -8.149603092443314\n",
      "test loss:  -7.713148678682119\n",
      "train MSE:  106.92886823718302\n",
      "test MSE:  105.98036174157797\n",
      "Epoch 134 / 250...\n",
      "train loss:  -8.191843327108906\n",
      "test loss:  -7.750764079973255\n",
      "train MSE:  106.91573750203\n",
      "test MSE:  105.97146978584286\n",
      "Epoch 135 / 250...\n",
      "train loss:  -8.227520719109702\n",
      "test loss:  -7.7883316322518805\n",
      "train MSE:  106.90273966716553\n",
      "test MSE:  105.9627045048606\n",
      "Epoch 136 / 250...\n",
      "train loss:  -8.269146118652213\n",
      "test loss:  -7.825761632948215\n",
      "train MSE:  106.88995341649918\n",
      "test MSE:  105.9541284036191\n",
      "Epoch 137 / 250...\n",
      "train loss:  -8.309695645172756\n",
      "test loss:  -7.86313740796748\n",
      "train MSE:  106.87790974188113\n",
      "test MSE:  105.94626123125542\n",
      "Epoch 138 / 250...\n",
      "train loss:  -8.34749362959088\n",
      "test loss:  -7.900349998180336\n",
      "train MSE:  106.8672592427929\n",
      "test MSE:  105.93992833260907\n",
      "Epoch 139 / 250...\n",
      "train loss:  -8.386727337743888\n",
      "test loss:  -7.937568687741931\n",
      "train MSE:  106.8557349292179\n",
      "test MSE:  105.93255183160834\n",
      "Epoch 140 / 250...\n",
      "train loss:  -8.427244142580927\n",
      "test loss:  -7.9746082899209645\n",
      "train MSE:  106.84542665603777\n",
      "test MSE:  105.92647640868441\n",
      "Epoch 141 / 250...\n",
      "train loss:  -8.464087763593069\n",
      "test loss:  -8.011611010074441\n",
      "train MSE:  106.83502166135787\n",
      "test MSE:  105.92022668125004\n",
      "Epoch 142 / 250...\n",
      "train loss:  -8.503904776454261\n",
      "test loss:  -8.048456293043632\n",
      "train MSE:  106.8254506408548\n",
      "test MSE:  105.9148641746724\n",
      "Epoch 143 / 250...\n",
      "train loss:  -8.541451759093446\n",
      "test loss:  -8.085326985733754\n",
      "train MSE:  106.81594098529783\n",
      "test MSE:  105.90948811233055\n",
      "Epoch 144 / 250...\n",
      "train loss:  -8.581305200443072\n",
      "test loss:  -8.122274317882054\n",
      "train MSE:  106.8047384912021\n",
      "test MSE:  105.90215393625243\n",
      "Epoch 145 / 250...\n",
      "train loss:  -8.618285165933678\n",
      "test loss:  -8.158996911949691\n",
      "train MSE:  106.79582088037476\n",
      "test MSE:  105.89731117443941\n",
      "Epoch 146 / 250...\n",
      "train loss:  -8.656340883037682\n",
      "test loss:  -8.195633789438595\n",
      "train MSE:  106.78679313820813\n",
      "test MSE:  105.8923555674427\n",
      "Epoch 147 / 250...\n",
      "train loss:  -8.700080308932781\n",
      "test loss:  -8.232124890292436\n",
      "train MSE:  106.7793736305097\n",
      "test MSE:  105.88905657786134\n",
      "Epoch 148 / 250...\n",
      "train loss:  -8.7328226521007\n",
      "test loss:  -8.26854638750867\n",
      "train MSE:  106.77211966757125\n",
      "test MSE:  105.88591712395642\n",
      "Epoch 149 / 250...\n",
      "train loss:  -8.77562420360393\n",
      "test loss:  -8.304930842350853\n",
      "train MSE:  106.76491411660557\n",
      "test MSE:  105.88281120625345\n",
      "Epoch 150 / 250...\n",
      "train loss:  -8.813649246852163\n",
      "test loss:  -8.341280840454193\n",
      "train MSE:  106.75783911692572\n",
      "test MSE:  105.87974810084826\n",
      "Epoch 151 / 250...\n",
      "train loss:  -8.848633693822324\n",
      "test loss:  -8.377505629197014\n",
      "train MSE:  106.75151155973057\n",
      "test MSE:  105.8774970686027\n",
      "Epoch 152 / 250...\n",
      "train loss:  -8.889192550488824\n",
      "test loss:  -8.413678493924978\n",
      "train MSE:  106.74534786713461\n",
      "test MSE:  105.87536010476403\n",
      "Epoch 153 / 250...\n",
      "train loss:  -8.924178326595277\n",
      "test loss:  -8.449698470083492\n",
      "train MSE:  106.74146491200129\n",
      "test MSE:  105.87566768168527\n",
      "Epoch 154 / 250...\n",
      "train loss:  -8.964232086834896\n",
      "test loss:  -8.485533170159405\n",
      "train MSE:  106.73819413005613\n",
      "test MSE:  105.8766536426866\n",
      "Epoch 155 / 250...\n",
      "train loss:  -9.006982863655487\n",
      "test loss:  -8.521537703041675\n",
      "train MSE:  106.73317360182082\n",
      "test MSE:  105.87556840496386\n",
      "Epoch 156 / 250...\n",
      "train loss:  -9.040318296097404\n",
      "test loss:  -8.557381293778866\n",
      "train MSE:  106.72897900683158\n",
      "test MSE:  105.87541760540844\n",
      "Epoch 157 / 250...\n",
      "train loss:  -9.077497521254086\n",
      "test loss:  -8.593199091237414\n",
      "train MSE:  106.72474976157874\n",
      "test MSE:  105.87511696054479\n",
      "Epoch 158 / 250...\n",
      "train loss:  -9.113937065013351\n",
      "test loss:  -8.628922252193473\n",
      "train MSE:  106.72146769958842\n",
      "test MSE:  105.87584742353182\n",
      "Epoch 159 / 250...\n",
      "train loss:  -9.154181691940751\n",
      "test loss:  -8.664547266075083\n",
      "train MSE:  106.71804034150203\n",
      "test MSE:  105.87638446729146\n",
      "Epoch 160 / 250...\n",
      "train loss:  -9.193921438541448\n",
      "test loss:  -8.700063073097466\n",
      "train MSE:  106.71583009071566\n",
      "test MSE:  105.87820572072927\n",
      "Epoch 161 / 250...\n",
      "train loss:  -9.226827770487533\n",
      "test loss:  -8.735709104471985\n",
      "train MSE:  106.71228440257318\n",
      "test MSE:  105.87842691090444\n",
      "Epoch 162 / 250...\n",
      "train loss:  -9.266200994901865\n",
      "test loss:  -8.771141123558497\n",
      "train MSE:  106.71036435337186\n",
      "test MSE:  105.8804563072681\n",
      "Epoch 163 / 250...\n",
      "train loss:  -9.303103049441129\n",
      "test loss:  -8.806594344360368\n",
      "train MSE:  106.70785249609334\n",
      "test MSE:  105.88183490100415\n",
      "Epoch 164 / 250...\n",
      "train loss:  -9.338381073492878\n",
      "test loss:  -8.841976473690657\n",
      "train MSE:  106.70569541069469\n",
      "test MSE:  105.88348796495585\n",
      "Epoch 165 / 250...\n",
      "train loss:  -9.374742381866891\n",
      "test loss:  -8.877320485170415\n",
      "train MSE:  106.70406445681672\n",
      "test MSE:  105.88565920341341\n",
      "Epoch 166 / 250...\n",
      "train loss:  -9.418147677809666\n",
      "test loss:  -8.912438984826839\n",
      "train MSE:  106.7040975421118\n",
      "test MSE:  105.88965040316597\n",
      "Epoch 167 / 250...\n",
      "train loss:  -9.448974024865372\n",
      "test loss:  -8.947537133105405\n",
      "train MSE:  106.70389003874537\n",
      "test MSE:  105.89334517823838\n",
      "Epoch 168 / 250...\n",
      "train loss:  -9.487627764919885\n",
      "test loss:  -8.982530068183717\n",
      "train MSE:  106.70509769155184\n",
      "test MSE:  105.89850948296693\n",
      "Epoch 169 / 250...\n",
      "train loss:  -9.522971326391962\n",
      "test loss:  -9.017569678638717\n",
      "train MSE:  106.704946833448\n",
      "test MSE:  105.90219183685072\n",
      "Epoch 170 / 250...\n",
      "train loss:  -9.559427177996135\n",
      "test loss:  -9.052593864307578\n",
      "train MSE:  106.70398087770259\n",
      "test MSE:  105.90488641041806\n",
      "Epoch 171 / 250...\n",
      "train loss:  -9.600089857871119\n",
      "test loss:  -9.087443409946417\n",
      "train MSE:  106.70514146414762\n",
      "test MSE:  105.90983946699615\n",
      "Epoch 172 / 250...\n",
      "train loss:  -9.634952367001599\n",
      "test loss:  -9.122289440513208\n",
      "train MSE:  106.70594886254463\n",
      "test MSE:  105.91438976184887\n",
      "Epoch 173 / 250...\n",
      "train loss:  -9.674426847346954\n",
      "test loss:  -9.15712121423569\n",
      "train MSE:  106.70732741407359\n",
      "test MSE:  105.91952365700365\n",
      "Epoch 174 / 250...\n",
      "train loss:  -9.713823352960832\n",
      "test loss:  -9.191779321817066\n",
      "train MSE:  106.7094005402694\n",
      "test MSE:  105.9253722476598\n",
      "Epoch 175 / 250...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -9.744404909614062\n",
      "test loss:  -9.226315049982537\n",
      "train MSE:  106.71234077070152\n",
      "test MSE:  105.93217396194069\n",
      "Epoch 176 / 250...\n",
      "train loss:  -9.77934249401504\n",
      "test loss:  -9.260944910847003\n",
      "train MSE:  106.71500445166872\n",
      "test MSE:  105.93858587114315\n",
      "Epoch 177 / 250...\n",
      "train loss:  -9.816272803807323\n",
      "test loss:  -9.295535281825563\n",
      "train MSE:  106.71729323489093\n",
      "test MSE:  105.94450935683584\n",
      "Epoch 178 / 250...\n",
      "train loss:  -9.857274064523956\n",
      "test loss:  -9.329933262410751\n",
      "train MSE:  106.72169223557948\n",
      "test MSE:  105.95276731100579\n",
      "Epoch 179 / 250...\n",
      "train loss:  -9.89802508827268\n",
      "test loss:  -9.364386544671682\n",
      "train MSE:  106.72512272829941\n",
      "test MSE:  105.95986708339288\n",
      "Epoch 180 / 250...\n",
      "train loss:  -9.924135175883617\n",
      "test loss:  -9.398681973207543\n",
      "train MSE:  106.72940325606643\n",
      "test MSE:  105.96789419218399\n",
      "Epoch 181 / 250...\n",
      "train loss:  -9.966898377833969\n",
      "test loss:  -9.432874412446171\n",
      "train MSE:  106.73424937676228\n",
      "test MSE:  105.97652745180669\n",
      "Epoch 182 / 250...\n",
      "train loss:  -9.998352557592858\n",
      "test loss:  -9.466998566374341\n",
      "train MSE:  106.74012986006144\n",
      "test MSE:  105.98624116849017\n",
      "Epoch 183 / 250...\n",
      "train loss:  -10.033979075704396\n",
      "test loss:  -9.501110522005694\n",
      "train MSE:  106.74669459741214\n",
      "test MSE:  105.99665436694274\n",
      "Epoch 184 / 250...\n",
      "train loss:  -10.069055801629874\n",
      "test loss:  -9.535326982124515\n",
      "train MSE:  106.75140330635902\n",
      "test MSE:  106.00490648914442\n",
      "Epoch 185 / 250...\n",
      "train loss:  -10.112018966982768\n",
      "test loss:  -9.569307283134677\n",
      "train MSE:  106.75751429039269\n",
      "test MSE:  106.01477734334755\n",
      "Epoch 186 / 250...\n",
      "train loss:  -10.140211967230496\n",
      "test loss:  -9.603240064865899\n",
      "train MSE:  106.76386182718895\n",
      "test MSE:  106.02482763701985\n",
      "Epoch 187 / 250...\n",
      "train loss:  -10.178419214303974\n",
      "test loss:  -9.637042730398901\n",
      "train MSE:  106.77181010446456\n",
      "test MSE:  106.03657947655026\n",
      "Epoch 188 / 250...\n",
      "train loss:  -10.216772590424185\n",
      "test loss:  -9.670935185392874\n",
      "train MSE:  106.77906947429433\n",
      "test MSE:  106.04755610589973\n",
      "Epoch 189 / 250...\n",
      "train loss:  -10.249860102397504\n",
      "test loss:  -9.704809235397438\n",
      "train MSE:  106.78542075445165\n",
      "test MSE:  106.05738716289069\n",
      "Epoch 190 / 250...\n",
      "train loss:  -10.288547770870775\n",
      "test loss:  -9.738578802885247\n",
      "train MSE:  106.79329500005561\n",
      "test MSE:  106.06890467479683\n",
      "Epoch 191 / 250...\n",
      "train loss:  -10.317945661024988\n",
      "test loss:  -9.772283318327814\n",
      "train MSE:  106.80183855358285\n",
      "test MSE:  106.08114320543228\n",
      "Epoch 192 / 250...\n",
      "train loss:  -10.35527419158257\n",
      "test loss:  -9.80601292836978\n",
      "train MSE:  106.8090269994548\n",
      "test MSE:  106.09181682359005\n",
      "Epoch 193 / 250...\n",
      "train loss:  -10.391063030856694\n",
      "test loss:  -9.839693657546801\n",
      "train MSE:  106.81669890426376\n",
      "test MSE:  106.10295527535665\n",
      "Epoch 194 / 250...\n",
      "train loss:  -10.428622958152719\n",
      "test loss:  -9.873202690368034\n",
      "train MSE:  106.82618609429197\n",
      "test MSE:  106.11613955386782\n",
      "Epoch 195 / 250...\n",
      "train loss:  -10.463272201678638\n",
      "test loss:  -9.906704346296568\n",
      "train MSE:  106.83545882121945\n",
      "test MSE:  106.1290111142682\n",
      "Epoch 196 / 250...\n",
      "train loss:  -10.502488765631439\n",
      "test loss:  -9.940139914953791\n",
      "train MSE:  106.84473881415303\n",
      "test MSE:  106.14184774482199\n",
      "Epoch 197 / 250...\n",
      "train loss:  -10.533254113049821\n",
      "test loss:  -9.973539849898467\n",
      "train MSE:  106.85511581885821\n",
      "test MSE:  106.15584220551987\n",
      "Epoch 198 / 250...\n",
      "train loss:  -10.568795106069556\n",
      "test loss:  -10.006780738529251\n",
      "train MSE:  106.86622966035365\n",
      "test MSE:  106.1706550862539\n",
      "Epoch 199 / 250...\n",
      "train loss:  -10.604323754884396\n",
      "test loss:  -10.040085835554926\n",
      "train MSE:  106.87637126021257\n",
      "test MSE:  106.18430034442765\n",
      "Epoch 200 / 250...\n",
      "train loss:  -10.637769637053852\n",
      "test loss:  -10.07325071171529\n",
      "train MSE:  106.8879879603765\n",
      "test MSE:  106.199570328365\n",
      "Epoch 201 / 250...\n",
      "train loss:  -10.675321652329753\n",
      "test loss:  -10.106346631395883\n",
      "train MSE:  106.90009955912568\n",
      "test MSE:  106.21534228732622\n",
      "Epoch 202 / 250...\n",
      "train loss:  -10.706051111804507\n",
      "test loss:  -10.13963090382175\n",
      "train MSE:  106.90989865583369\n",
      "test MSE:  106.22845875614547\n",
      "Epoch 203 / 250...\n",
      "train loss:  -10.74393695897876\n",
      "test loss:  -10.172707450270439\n",
      "train MSE:  106.92143730646242\n",
      "test MSE:  106.24351321039256\n",
      "Epoch 204 / 250...\n",
      "train loss:  -10.777953420295946\n",
      "test loss:  -10.205689176133916\n",
      "train MSE:  106.93443357175273\n",
      "test MSE:  106.26014391312893\n",
      "Epoch 205 / 250...\n",
      "train loss:  -10.816010450643144\n",
      "test loss:  -10.23873937213713\n",
      "train MSE:  106.94672473547976\n",
      "test MSE:  106.27588182922095\n",
      "Epoch 206 / 250...\n",
      "train loss:  -10.846737246622162\n",
      "test loss:  -10.271562842354204\n",
      "train MSE:  106.96050039448384\n",
      "test MSE:  106.29331565143808\n",
      "Epoch 207 / 250...\n",
      "train loss:  -10.881160606765249\n",
      "test loss:  -10.304538445966537\n",
      "train MSE:  106.97296921214694\n",
      "test MSE:  106.30919829511404\n",
      "Epoch 208 / 250...\n",
      "train loss:  -10.91850789498869\n",
      "test loss:  -10.33735562130422\n",
      "train MSE:  106.98623013502267\n",
      "test MSE:  106.32589281382994\n",
      "Epoch 209 / 250...\n",
      "train loss:  -10.953000402141685\n",
      "test loss:  -10.37013582646235\n",
      "train MSE:  107.0001681171837\n",
      "test MSE:  106.34330932045432\n",
      "Epoch 210 / 250...\n",
      "train loss:  -10.987102674916773\n",
      "test loss:  -10.402818165951508\n",
      "train MSE:  107.0149701222158\n",
      "test MSE:  106.36167530850481\n",
      "Epoch 211 / 250...\n",
      "train loss:  -11.021955680899808\n",
      "test loss:  -10.435419064558573\n",
      "train MSE:  107.03005200483415\n",
      "test MSE:  106.38026932731944\n",
      "Epoch 212 / 250...\n",
      "train loss:  -11.063486760340792\n",
      "test loss:  -10.468084107763922\n",
      "train MSE:  107.04430287021832\n",
      "test MSE:  106.39791107198388\n",
      "Epoch 213 / 250...\n",
      "train loss:  -11.089522163935563\n",
      "test loss:  -10.500607495993467\n",
      "train MSE:  107.05974497412709\n",
      "test MSE:  106.41683396954414\n",
      "Epoch 214 / 250...\n",
      "train loss:  -11.124558812838458\n",
      "test loss:  -10.53321189507146\n",
      "train MSE:  107.0742725600419\n",
      "test MSE:  106.43466125993075\n",
      "Epoch 215 / 250...\n",
      "train loss:  -11.156557425358693\n",
      "test loss:  -10.565584544584361\n",
      "train MSE:  107.09128826825848\n",
      "test MSE:  106.45527872413054\n",
      "Epoch 216 / 250...\n",
      "train loss:  -11.192443080570918\n",
      "test loss:  -10.597986898097762\n",
      "train MSE:  107.1078278103845\n",
      "test MSE:  106.47528627129842\n",
      "Epoch 217 / 250...\n",
      "train loss:  -11.228982922319757\n",
      "test loss:  -10.630498555020317\n",
      "train MSE:  107.12339668717281\n",
      "test MSE:  106.49419046020888\n",
      "Epoch 218 / 250...\n",
      "train loss:  -11.263115081183118\n",
      "test loss:  -10.662881619067765\n",
      "train MSE:  107.13924513554473\n",
      "test MSE:  106.51333536398026\n",
      "Epoch 219 / 250...\n",
      "train loss:  -11.294343254954603\n",
      "test loss:  -10.695308224088214\n",
      "train MSE:  107.15509514233615\n",
      "test MSE:  106.53244676732298\n",
      "Epoch 220 / 250...\n",
      "train loss:  -11.334758115990738\n",
      "test loss:  -10.727602423772671\n",
      "train MSE:  107.1721320656023\n",
      "test MSE:  106.55283481701076\n",
      "Epoch 221 / 250...\n",
      "train loss:  -11.36494607032631\n",
      "test loss:  -10.759719233420265\n",
      "train MSE:  107.19039296956844\n",
      "test MSE:  106.5746300246443\n",
      "Epoch 222 / 250...\n",
      "train loss:  -11.39913195496021\n",
      "test loss:  -10.79189348901003\n",
      "train MSE:  107.20879895328645\n",
      "test MSE:  106.59647254657278\n",
      "Epoch 223 / 250...\n",
      "train loss:  -11.431364448505398\n",
      "test loss:  -10.823943611837196\n",
      "train MSE:  107.22800982782394\n",
      "test MSE:  106.61919006860023\n",
      "Epoch 224 / 250...\n",
      "train loss:  -11.464381838957348\n",
      "test loss:  -10.856049925356183\n",
      "train MSE:  107.24622687514582\n",
      "test MSE:  106.64077841171641\n",
      "Epoch 225 / 250...\n",
      "train loss:  -11.500613683352762\n",
      "test loss:  -10.888013659746505\n",
      "train MSE:  107.26613036697195\n",
      "test MSE:  106.6641367096568\n",
      "Epoch 226 / 250...\n",
      "train loss:  -11.532651071175419\n",
      "test loss:  -10.919994753768405\n",
      "train MSE:  107.28479879135926\n",
      "test MSE:  106.68606737789626\n",
      "Epoch 227 / 250...\n",
      "train loss:  -11.566673748275612\n",
      "test loss:  -10.95187212736272\n",
      "train MSE:  107.30440585395277\n",
      "test MSE:  106.70911468043889\n",
      "Epoch 228 / 250...\n",
      "train loss:  -11.600708516711222\n",
      "test loss:  -10.983740737071843\n",
      "train MSE:  107.32372680641906\n",
      "test MSE:  106.73170110692966\n",
      "Epoch 229 / 250...\n",
      "train loss:  -11.63916982630063\n",
      "test loss:  -11.01567612194736\n",
      "train MSE:  107.3433178031575\n",
      "test MSE:  106.75450367899865\n",
      "Epoch 230 / 250...\n",
      "train loss:  -11.670491398325689\n",
      "test loss:  -11.047465087547822\n",
      "train MSE:  107.36413190648203\n",
      "test MSE:  106.77869533990315\n",
      "Epoch 231 / 250...\n",
      "train loss:  -11.702897287212418\n",
      "test loss:  -11.079354638136309\n",
      "train MSE:  107.38390812830927\n",
      "test MSE:  106.80166824344501\n",
      "Epoch 232 / 250...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -11.737822418629948\n",
      "test loss:  -11.110997479165507\n",
      "train MSE:  107.40635917996994\n",
      "test MSE:  106.82760265801954\n",
      "Epoch 233 / 250...\n",
      "train loss:  -11.775048740788645\n",
      "test loss:  -11.14267848357873\n",
      "train MSE:  107.427462777768\n",
      "test MSE:  106.8520168862893\n",
      "Epoch 234 / 250...\n",
      "train loss:  -11.808918328757972\n",
      "test loss:  -11.174244666574593\n",
      "train MSE:  107.44965619567824\n",
      "test MSE:  106.87756905203521\n",
      "Epoch 235 / 250...\n",
      "train loss:  -11.838135564458446\n",
      "test loss:  -11.205915566770638\n",
      "train MSE:  107.47120101028324\n",
      "test MSE:  106.9023071359818\n",
      "Epoch 236 / 250...\n",
      "train loss:  -11.870184763220077\n",
      "test loss:  -11.23757115368243\n",
      "train MSE:  107.49203041994183\n",
      "test MSE:  106.9262528184145\n",
      "Epoch 237 / 250...\n",
      "train loss:  -11.902261910968456\n",
      "test loss:  -11.269015964284538\n",
      "train MSE:  107.51608215128346\n",
      "test MSE:  106.95378029724063\n",
      "Epoch 238 / 250...\n",
      "train loss:  -11.9422726968428\n",
      "test loss:  -11.300527855438235\n",
      "train MSE:  107.53780196218584\n",
      "test MSE:  106.97869479891992\n",
      "Epoch 239 / 250...\n",
      "train loss:  -11.967930459599662\n",
      "test loss:  -11.33193070668422\n",
      "train MSE:  107.56034028576515\n",
      "test MSE:  107.00441712672301\n",
      "Epoch 240 / 250...\n",
      "train loss:  -12.006278397863626\n",
      "test loss:  -11.363218788168345\n",
      "train MSE:  107.58502366294324\n",
      "test MSE:  107.03250847954271\n",
      "Epoch 241 / 250...\n",
      "train loss:  -12.039182599271694\n",
      "test loss:  -11.394595275318688\n",
      "train MSE:  107.60867953657262\n",
      "test MSE:  107.05937387724262\n",
      "Epoch 242 / 250...\n",
      "train loss:  -12.069236124582787\n",
      "test loss:  -11.425877189784295\n",
      "train MSE:  107.63275782561756\n",
      "test MSE:  107.0867573901003\n",
      "Epoch 243 / 250...\n",
      "train loss:  -12.10337150744061\n",
      "test loss:  -11.457172084954555\n",
      "train MSE:  107.6563335416996\n",
      "test MSE:  107.11345798163342\n",
      "Epoch 244 / 250...\n",
      "train loss:  -12.139237433402439\n",
      "test loss:  -11.48837814543962\n",
      "train MSE:  107.6811141285669\n",
      "test MSE:  107.14153075165343\n",
      "Epoch 245 / 250...\n",
      "train loss:  -12.170235259799279\n",
      "test loss:  -11.519571809613215\n",
      "train MSE:  107.70572055355586\n",
      "test MSE:  107.16929766050903\n",
      "Epoch 246 / 250...\n",
      "train loss:  -12.205623775097289\n",
      "test loss:  -11.550710165293314\n",
      "train MSE:  107.7309709850501\n",
      "test MSE:  107.19773742608652\n",
      "Epoch 247 / 250...\n",
      "train loss:  -12.238100393722968\n",
      "test loss:  -11.581830972522859\n",
      "train MSE:  107.75615117103395\n",
      "test MSE:  107.22604752368709\n",
      "Epoch 248 / 250...\n",
      "train loss:  -12.269645156285806\n",
      "test loss:  -11.612865961319986\n",
      "train MSE:  107.78189701655451\n",
      "test MSE:  107.25501942158547\n",
      "Epoch 249 / 250...\n",
      "train loss:  -12.302047299290303\n",
      "test loss:  -11.643886840868898\n",
      "train MSE:  107.80714630322441\n",
      "test MSE:  107.28341817056955\n",
      "Epoch 250 / 250...\n",
      "train loss:  -12.335386482953128\n",
      "test loss:  -11.675009772254969\n",
      "train MSE:  107.83221077683169\n",
      "test MSE:  107.31152224722717\n"
     ]
    }
   ],
   "source": [
    "# ==========  Poisson Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_mse_arr, test_mse_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the weights\n",
    "regr.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
