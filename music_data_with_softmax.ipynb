{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# part 5, 5 - softmax classifier on music data\n",
    "\n",
    "class SoftmaxClassifier:\n",
    "\n",
    "    def __init__(self, epochs, learning_rate, batch_size, regularization, momentum):\n",
    "\n",
    "        self.num_classes = 120  # for 1900 to 2020 range\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.regularization = regularization\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        self.weights = None\n",
    "\n",
    "    def one_hot(self, y):\n",
    "        # get a vector of labels, convert into 1 hot\n",
    "\n",
    "        self.num_classes = 120  # range is 1900-2020\n",
    "        y = np.asarray(y, dtype='int32')  # convert type to int\n",
    "        y = y.reshape(-1)  # convert into a list of numbers\n",
    "        y_one_hot = np.zeros((len(y), self.num_classes))  # init shape of len y, and out 3 (num of classes)\n",
    "        y_one_hot[np.arange(len(y)), y] = 1  # set the right indexes to 1, based on y (a list)\n",
    "        return y_one_hot  # shape N by num_classes (3)\n",
    "    \n",
    "    def calc_mse(self, pred, yt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred - yt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "#     def calc_accuracy(self, x, y):\n",
    "#         #  predict the class, then compare with the correct label.  return the average correct %\n",
    "#         pred = np.argmax(x.dot(self.weights), 1)  # predict\n",
    "#         pred = pred.reshape((-1, 1))  # convert to column vector\n",
    "#         return np.mean(np.equal(y, pred))  # return average over all the 1's (over the total)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # calc the softmax\n",
    "        exp_x = np.exp(x - np.max(x))  # make sure it doesn't blow up by sub max\n",
    "\n",
    "        # make sure sum along columns, and keep dims keeps the exact same dim when summing\n",
    "        # ie keep cols, instead of converting to rows\n",
    "        y = np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return exp_x / y\n",
    "\n",
    "    def loss_and_gradient(self, x, y):\n",
    "        # calc the loss and gradient.  forward prop, get softmax, calc the neg loss loss, and total loss.\n",
    "        # calc dW by taking the residual, then dot with X,  + regularization\n",
    "        # find average for both\n",
    "\n",
    "        n_samples = x.shape[0]  # num of examples\n",
    "\n",
    "        # forward prop\n",
    "        f = np.dot(x, self.weights)  # mult X by W\n",
    "        probs = self.softmax(f)  # pass f to softmax\n",
    "\n",
    "        # take neg log of the highest prob. for that row\n",
    "        neg_log_loss = -np.log(probs[np.arange(n_samples), np.argmax(probs, axis=1)])\n",
    "        loss = np.sum(neg_log_loss)  # sum to get total loss across all samples\n",
    "        # calc the regularization loss too\n",
    "        reg_loss = 0.5 * self.regularization * np.sum(self.weights * self.weights)\n",
    "        total_loss = (loss / n_samples) + reg_loss  # sum to get total, divide for avg\n",
    "\n",
    "        # calc dW\n",
    "        y_one_hot = self.one_hot(y)  # need one hot\n",
    "\n",
    "        # calc derivative of loss (including regularization derivative)\n",
    "        dW = x.T.dot( (probs - y_one_hot) ) + (self.regularization * self.weights) \n",
    "        dW /= n_samples  # compute average dW\n",
    "\n",
    "        return total_loss, dW\n",
    "\n",
    "    def train_phase(self, x_train, y_train):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch = y_train[i:i + self.batch_size]\n",
    "\n",
    "            # forward prop\n",
    "            loss, dW = self.loss_and_gradient(x_train_batch, y_train_batch)  # calc loss and dW\n",
    "            # calc velocity\n",
    "            self.velocity = (self.momentum * self.velocity) + (self.learning_rate * dW)\n",
    "            self.weights -= self.velocity  # update the weights\n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_test):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "\n",
    "        loss, _ = self.loss_and_gradient(x, y_test)  # calc loss and dW (don't need)\n",
    "        return loss\n",
    "\n",
    "    def run_epochs(self, x_train, y_train, x_test, y_test):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        num_dim = x_train.shape[1]  # num of dimensions\n",
    "\n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.001 * np.random.rand(num_dim, self.num_classes)\n",
    "        self.velocity = np.zeros(self.weights.shape)\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Ephoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train)\n",
    "            test_loss = self.test_phase(x_test, y_test)\n",
    "            train_mse = self.calc_mse(x_train, y_train)\n",
    "            test_mse = self.calc_mse(x_test, y_test)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr  # return all the vals\n",
    "\n",
    "    def plot_graph(self, train_losses, test_losses, train_mse_arr, test_mse_arr):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs. Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "        plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "        # plt.legend(loc='best')\n",
    "        plt.title(\"Softmax Class. Epochs vs Accuracy\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feat(x, mean=None, std=None):\n",
    "    # normalize the feature data.  test data must pass mean and std\n",
    "\n",
    "    # calc feature-wise mean\n",
    "    if mean is None:\n",
    "        mean = np.mean(x, axis=0)\n",
    "\n",
    "    # calc feature-wise std\n",
    "    if std is None:\n",
    "        std = np.std(x, axis=0)\n",
    "\n",
    "    # sub the mean per column\n",
    "    x_norm = x - mean\n",
    "\n",
    "    # div by the standard dev.\n",
    "    x_norm = x_norm / std\n",
    "\n",
    "    return x_norm, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loader\n",
    "\n",
    "def load_data(fname, bias=1):\n",
    "\n",
    "    data = loadtxt(fname, delimiter=',')\n",
    "\n",
    "    # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "    TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "    # process training data\n",
    "    x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "\n",
    "    x_train, train_mean, train_std = normalize_feat(x_train)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_train = np.hstack((x_train, col_bias))\n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "    y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # process test data\n",
    "    x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "    x_test, _, _ = normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "    y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_labels(y):\n",
    "    OFFSET = 1900 # starting the index 0 with year 1900\n",
    "    return y - OFFSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "\n",
    "# # note, features are normalized\n",
    "# x_train, y_train, x_test, y_test = load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset to make labels start at index 0\n",
    "y_train_off = offset_labels(y_train)\n",
    "y_test_off = offset_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ephoch 1 / 100...\n",
      "train loss:  4.780427104649142\n",
      "test loss:  4.778249878771481\n",
      "train MSE:  9798.508138769364\n",
      "test MSE:  9817.965857640336\n",
      "Ephoch 2 / 100...\n",
      "train loss:  4.775569207763842\n",
      "test loss:  4.77240604586792\n",
      "train MSE:  9798.508138769364\n",
      "test MSE:  9817.965857640336\n",
      "Ephoch 3 / 100...\n",
      "train loss:  4.769595838068432\n",
      "test loss:  4.766139416190811\n",
      "train MSE:  9798.508138769364\n",
      "test MSE:  9817.965857640336\n",
      "Ephoch 4 / 100...\n",
      "train loss:  4.763327944186613\n",
      "test loss:  4.759733931687148\n",
      "train MSE:  9798.508138769364\n",
      "test MSE:  9817.965857640336\n",
      "Ephoch 5 / 100...\n",
      "train loss:  4.757013153980332\n",
      "test loss:  4.753272240339001\n",
      "train MSE:  9798.508138769364\n",
      "test MSE:  9817.965857640336\n",
      "Ephoch 6 / 100...\n",
      "train loss:  4.750684989873597\n",
      "test loss:  4.7467951157502934\n",
      "train MSE:  9798.508138769364\n",
      "test MSE:  9817.965857640336\n",
      "Ephoch 7 / 100...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-bf65d830bab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmaxClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_off\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_off\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# smc.plot_decision_boundary(x_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-9a4c8f917f5e>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# calc loss and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-9a4c8f917f5e>\u001b[0m in \u001b[0;36mtrain_phase\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Randomize the data (using sklearn shuffle)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# get the next batch (loop through number of training samples, step by batch size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \"\"\"\n\u001b[1;32m    342\u001b[0m     \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set hyperparameters here\n",
    "epochs = 100\n",
    "learning_rate = 0.0001  # [0.1, 0.01, 0.001]\n",
    "batch_size = 1000  # try powers of 2\n",
    "regularization = 0.1  # L2 weight decay, range [1, 0.1, 0.01, 0.001]\n",
    "momentum = 0.30  # started with 0 to 1\n",
    "\n",
    "smc = SoftmaxClassifier(epochs, learning_rate, batch_size, regularization, momentum)\n",
    "train_losses, test_losses, train_mse, test_mse = smc.run_epochs(x_train, y_train_off, x_test, y_test_off)\n",
    "smc.plot_graph(train_losses, test_losses, train_mse, test_mse)\n",
    "# smc.plot_decision_boundary(x_train, y_train)\n",
    "# smc.plot_decision_boundary(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
