{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# part 5, 5 - softmax classifier on music data\n",
    "\n",
    "class SoftmaxClassifier:\n",
    "\n",
    "    def __init__(self, epochs, learning_rate, batch_size, regularization, momentum, num_classes):\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.regularization = regularization\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        self.weights = None\n",
    "    \n",
    "    def calc_mse(self, probs, yt_off):\n",
    "        \n",
    "        preds = np.argmax(probs, 1).reshape(-1, 1)\n",
    "        diff = preds - yt_off.reshape(-1, 1)\n",
    "        mse = (np.square(diff)).mean()\n",
    "        \n",
    "#         print('pred shape: ', preds.shape)\n",
    "#         print('yt_off shape: ', yt_off.shape)\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # calc the softmax\n",
    "        exp_x = np.exp(x - np.max(x))  # make sure it doesn't blow up by sub max\n",
    "\n",
    "        # make sure sum along columns, and keep dims keeps the exact same dim when summing\n",
    "        # ie keep cols, instead of converting to rows\n",
    "        y = np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return exp_x / y\n",
    "\n",
    "    def loss_and_gradient(self, x, y_off, y_one_off):\n",
    "        # calc the loss and gradient.  forward prop, get softmax, calc the neg loss loss, and total loss.\n",
    "        # calc dW by taking the residual, then dot with X,  + regularization\n",
    "        # find average for both\n",
    "\n",
    "        n_samples = x.shape[0]  # num of examples\n",
    "\n",
    "        # forward prop\n",
    "        f = np.dot(x, self.weights)  # mult X by W\n",
    "        probs = self.softmax(f)  # pass f to softmax\n",
    "\n",
    "        # take neg log of the highest prob. for that row\n",
    "        neg_log_loss = -np.log(probs[np.arange(n_samples), np.argmax(probs, axis=1)])\n",
    "        loss = np.sum(neg_log_loss)  # sum to get total loss across all samples\n",
    "        # calc the regularization loss too\n",
    "        reg_loss = 0.5 * self.regularization * np.sum(self.weights * self.weights)\n",
    "        total_loss = (loss / n_samples) + reg_loss  # sum to get total, divide for avg\n",
    "\n",
    "        # calc derivative of loss (including regularization derivative)\n",
    "        dW = x.T.dot( (probs - y_one_off) ) + (self.regularization * self.weights) \n",
    "        dW /= n_samples  # compute average dW\n",
    "\n",
    "        return total_loss, dW, probs\n",
    "\n",
    "    def train_phase(self, x_train, y_train_off, y_train_one_off):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        print('TRAINING PHASE --')\n",
    "        \n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        probs_arr = []\n",
    "\n",
    "        # Randomize the data\n",
    "        x_train, y_train_off, y_train_one_off = shuffle(x_train, y_train_off, y_train_one_off)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_off = y_train_off[i:i + self.batch_size]\n",
    "            y_train_batch_one_off = y_train_one_off[i:i + self.batch_size]\n",
    "\n",
    "            # forward prop\n",
    "            loss, dW, probs = self.loss_and_gradient(x_train_batch, y_train_batch_off, y_train_batch_one_off)  # calc loss and dW\n",
    "            \n",
    "            probs_arr.extend(probs)\n",
    "            \n",
    "            # calc velocity\n",
    "            self.velocity = (self.momentum * self.velocity) + (self.learning_rate * dW)\n",
    "            self.weights -= self.velocity  # update the weights\n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses), np.asarray(probs_arr)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_test_off, y_test_one_off):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "\n",
    "        print('Test PHASE --')\n",
    "        \n",
    "        loss, _, probs = self.loss_and_gradient(x, y_test_off, y_test_one_off)  # calc loss and dW (don't need)\n",
    "        return loss, probs\n",
    "\n",
    "    def run_epochs(self, x_train, y_train_off, y_train_one_off, x_test, y_test_off, y_test_one_off):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        num_dim = x_train.shape[1]  # num of dimensions\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(num_dim, self.num_classes)\n",
    "        self.velocity = np.zeros(self.weights.shape)\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Ephoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss, train_probs = self.train_phase(x_train, y_train_off, y_train_one_off)\n",
    "            test_loss, test_probs = self.test_phase(x_test, y_test_off, y_test_one_off)\n",
    "            train_mse = self.calc_mse(train_probs, y_train_off)\n",
    "            test_mse = self.calc_mse(test_probs, y_test_off)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr  # return all the vals\n",
    "\n",
    "    def plot_graph(self, train_losses, test_losses, train_mse_arr, test_mse_arr):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Softmax Class. Loss vs Epochs\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse_arr, label=\"Train MSE\")\n",
    "        plt.plot(test_mse_arr, label=\"Test MSE\")\n",
    "        # plt.legend(loc='best')\n",
    "        plt.title(\"Softmax Class. MSE vs Epochs\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= preprocessing code ========================\n",
    "\n",
    "def normalize_feat(x, mean=None, std=None):\n",
    "    # normalize the feature data.  test data must pass mean and std\n",
    "\n",
    "    # calc feature-wise mean\n",
    "    if mean is None:\n",
    "        mean = np.mean(x, axis=0)\n",
    "\n",
    "    # calc feature-wise std\n",
    "    if std is None:\n",
    "        std = np.std(x, axis=0)\n",
    "\n",
    "    # sub the mean per column\n",
    "    x_norm = x - mean\n",
    "\n",
    "    # div by the standard dev.\n",
    "    x_norm = x_norm / std\n",
    "\n",
    "    return x_norm, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loader\n",
    "\n",
    "def load_data(fname, bias=1):\n",
    "\n",
    "    data = loadtxt(fname, delimiter=',')\n",
    "\n",
    "    # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "    TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "    # process training data\n",
    "    x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "\n",
    "    x_train, train_mean, train_std = normalize_feat(x_train)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_train = np.hstack((x_train, col_bias))\n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "    y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # process test data\n",
    "    x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "    x_test, _, _ = normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "    y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_labels(y):\n",
    "    OFFSET = 1900 # starting the index 0 with year 1923\n",
    "    return y - OFFSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "\n",
    "# # # note, features are normalized\n",
    "# x_train, y_train, x_test, y_test = load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset to make labels start at index 0\n",
    "y_train_off = offset_labels(y_train)\n",
    "y_test_off = offset_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot on y labels\n",
    "\n",
    "def one_hot_vary(y_train, y_test):\n",
    "\n",
    "    train_size = len(y_train)\n",
    "    test_size = len(y_test)\n",
    "\n",
    "    stacked = np.vstack((y_train, y_test))\n",
    "    one_h = OneHotEncoder().fit_transform(stacked).toarray()\n",
    "    \n",
    "    y_train = one_h[0:train_size, :]\n",
    "    y_test = one_h[train_size:, :]\n",
    "    return y_train, y_test  # new one hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# obtain one hots on the variable number of classes\n",
    "y_train_one_off, y_test_one_off = one_hot_vary(y_train_off, y_test_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of years represented by list indexes\n",
    "train_years_key = np.unique(y_train_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters here\n",
    "num_classes = y_train_one_off.shape[1]\n",
    "epochs = 100\n",
    "learning_rate = 0.00001  # [0.1, 0.01, 0.001]\n",
    "batch_size = 100  # try powers of 2\n",
    "regularization = 0.5  # L2 weight decay, range [1, 0.1, 0.01, 0.001]\n",
    "momentum = 0.05  # started with 0 to 1, tried 2\n",
    "\n",
    "smc = SoftmaxClassifier(epochs, learning_rate, batch_size, regularization, momentum, num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ephoch 1 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.48783757775553\n",
      "test loss:  4.487395084245813\n",
      "train MSE:  1405.82803\n",
      "test MSE:  475.8257442234317\n",
      "Ephoch 2 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4869205895598405\n",
      "test loss:  4.486331482773622\n",
      "train MSE:  464.35923\n",
      "test MSE:  426.32811682903684\n",
      "Ephoch 3 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.485854354467739\n",
      "test loss:  4.485229842966816\n",
      "train MSE:  444.78239\n",
      "test MSE:  422.32180279289577\n",
      "Ephoch 4 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.484774471810193\n",
      "test loss:  4.484119937011817\n",
      "train MSE:  441.54105\n",
      "test MSE:  421.2082082469834\n",
      "Ephoch 5 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.483682908282748\n",
      "test loss:  4.483008411580978\n",
      "train MSE:  439.36078\n",
      "test MSE:  420.718986655304\n",
      "Ephoch 6 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.482607144493733\n",
      "test loss:  4.481897560075243\n",
      "train MSE:  437.9719\n",
      "test MSE:  420.5070403439794\n",
      "Ephoch 7 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4815149300256305\n",
      "test loss:  4.480788323787383\n",
      "train MSE:  437.10447\n",
      "test MSE:  420.2232573453933\n",
      "Ephoch 8 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.480436482960816\n",
      "test loss:  4.479681316883013\n",
      "train MSE:  437.39886\n",
      "test MSE:  419.86409327729467\n",
      "Ephoch 9 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.479363922899479\n",
      "test loss:  4.478576917946514\n",
      "train MSE:  437.67037\n",
      "test MSE:  419.7451337374833\n",
      "Ephoch 10 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4782660684264775\n",
      "test loss:  4.477475191849023\n",
      "train MSE:  437.5673\n",
      "test MSE:  419.67854583486667\n",
      "Ephoch 11 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.477204506921173\n",
      "test loss:  4.476376273148775\n",
      "train MSE:  437.58307\n",
      "test MSE:  419.5812786891596\n",
      "Ephoch 12 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4761113429863855\n",
      "test loss:  4.475280306845088\n",
      "train MSE:  437.94788\n",
      "test MSE:  419.5326644845151\n",
      "Ephoch 13 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.475063358962567\n",
      "test loss:  4.47418722824184\n",
      "train MSE:  436.93769\n",
      "test MSE:  419.4758768956635\n",
      "Ephoch 14 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.473998707238054\n",
      "test loss:  4.47309728700747\n",
      "train MSE:  437.13468\n",
      "test MSE:  419.4410334876334\n",
      "Ephoch 15 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.472922589301195\n",
      "test loss:  4.472010234438476\n",
      "train MSE:  436.32224\n",
      "test MSE:  419.39923689256455\n",
      "Ephoch 16 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.471878481365098\n",
      "test loss:  4.470926211197363\n",
      "train MSE:  436.8629\n",
      "test MSE:  419.49086788944624\n",
      "Ephoch 17 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.470811922050982\n",
      "test loss:  4.469845262958904\n",
      "train MSE:  436.63462\n",
      "test MSE:  419.4738626019252\n",
      "Ephoch 18 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4697432727314315\n",
      "test loss:  4.468767433885535\n",
      "train MSE:  436.95289\n",
      "test MSE:  419.4028587476516\n",
      "Ephoch 19 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.468704568765842\n",
      "test loss:  4.4676926609148655\n",
      "train MSE:  436.42302\n",
      "test MSE:  419.3751234723325\n",
      "Ephoch 20 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.467637981817865\n",
      "test loss:  4.466621034499749\n",
      "train MSE:  436.70837\n",
      "test MSE:  419.24411690650965\n",
      "Ephoch 21 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.466612543060043\n",
      "test loss:  4.465552522341876\n",
      "train MSE:  436.30568\n",
      "test MSE:  419.203404931146\n",
      "Ephoch 22 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.46557544601751\n",
      "test loss:  4.464487035914177\n",
      "train MSE:  436.88476\n",
      "test MSE:  419.1570761751661\n",
      "Ephoch 23 / 100...\n",
      "TRAINING PHASE --\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-6ec73cb63ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_off\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-266-c6e1285f55d0>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, x_train, y_train_off, y_train_one_off, x_test, y_test_off, y_test_one_off)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# calc loss and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_off\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_off\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_off\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_off\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_off\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subsample = 100000\n",
    "train_losses, test_losses, train_mse, test_mse = smc.run_epochs(x_train[:subsample], y_train_off[:subsample], y_train_one_off[:subsample], x_test[:subsample], y_test_off[:subsample], y_test_one_off[:subsample])\n",
    "smc.plot_graph(train_losses, test_losses, train_mse, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_off[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
