{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# part 5, 5 - softmax classifier on music data\n",
    "\n",
    "class SoftmaxClassifier:\n",
    "\n",
    "    def __init__(self, epochs, learning_rate, batch_size, regularization, momentum, num_classes):\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.offset = 1900\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.regularization = regularization\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        self.weights = None\n",
    "\n",
    "#     def one_hot(self, y):\n",
    "#         # get a vector of labels, convert into 1 hot\n",
    "\n",
    "#         y = np.asarray(y, dtype='int32')  # convert type to int\n",
    "#         y = y.reshape(-1)  # convert into a list of numbers\n",
    "#         y_one_hot = np.zeros((len(y), self.num_classes))  # init shape of len y, and out 3 (num of classes)\n",
    "#         y_one_hot[np.arange(len(y)), y] = 1  # set the right indexes to 1, based on y (a list)\n",
    "        \n",
    "# #         print('y one hot index: ', np.argmax(y_one_hot))\n",
    "        \n",
    "#         return y_one_hot  # shape N by num_classes\n",
    "    \n",
    "#     def one_hot_vary(self, y_train, y_test):\n",
    "        \n",
    "#         train_size = len(y_train)\n",
    "#         test_size = len(y_test)\n",
    "        \n",
    "#         stacked = np.vstack((y_train, y_test))\n",
    "#         one_h = OneHotEncoder(categories='auto').fit_transform(stacked).toarray()\n",
    "#         y_train = one_h[0:train_size, :]\n",
    "#         y_test = one_h[train_size:, :]\n",
    "#         return y_train, y_test\n",
    "    \n",
    "    def calc_mse(self, x, yt_off):\n",
    "        \n",
    "        # make prediction\n",
    "        pred = np.argmax(x.dot(self.weights), 1)  # predict\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "#         diff = pred - yt_off + self.offset\n",
    "        diff = pred - yt_off\n",
    "        mse = (np.square(diff)).mean()\n",
    "        \n",
    "#         print('pred: ', pred)\n",
    "#         print('yt_off: ', yt_off)\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # calc the softmax\n",
    "        exp_x = np.exp(x - np.max(x))  # make sure it doesn't blow up by sub max\n",
    "\n",
    "        # make sure sum along columns, and keep dims keeps the exact same dim when summing\n",
    "        # ie keep cols, instead of converting to rows\n",
    "        y = np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return exp_x / y\n",
    "\n",
    "    def loss_and_gradient(self, x, y_off, y_one_off):\n",
    "        # calc the loss and gradient.  forward prop, get softmax, calc the neg loss loss, and total loss.\n",
    "        # calc dW by taking the residual, then dot with X,  + regularization\n",
    "        # find average for both\n",
    "\n",
    "        n_samples = x.shape[0]  # num of examples\n",
    "\n",
    "        # forward prop\n",
    "        f = np.dot(x, self.weights)  # mult X by W\n",
    "        probs = self.softmax(f)  # pass f to softmax\n",
    "\n",
    "        # take neg log of the highest prob. for that row\n",
    "        neg_log_loss = -np.log(probs[np.arange(n_samples), np.argmax(probs, axis=1)])\n",
    "        loss = np.sum(neg_log_loss)  # sum to get total loss across all samples\n",
    "        # calc the regularization loss too\n",
    "        reg_loss = 0.5 * self.regularization * np.sum(self.weights * self.weights)\n",
    "        total_loss = (loss / n_samples) + reg_loss  # sum to get total, divide for avg\n",
    "\n",
    "        \n",
    "#         print('y off shape: ', y_off.shape)\n",
    "#         print('y one off shape: ', y_one_off.shape)\n",
    "        \n",
    "        # calc dW\n",
    "#         y_one_hot_off = self.one_hot(y_off)  # need one hot\n",
    "\n",
    "        # calc derivative of loss (including regularization derivative)\n",
    "        dW = x.T.dot( (probs - y_one_off) ) + (self.regularization * self.weights) \n",
    "        dW /= n_samples  # compute average dW\n",
    "\n",
    "        return total_loss, dW\n",
    "\n",
    "    def train_phase(self, x_train, y_train_off, y_train_one_off):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data\n",
    "        x_train, y_train_off, y_train_one_off = shuffle(x_train, y_train_off, y_train_one_off)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_off = y_train_off[i:i + self.batch_size]\n",
    "            y_train_batch_one_off = y_train_one_off[i:i + self.batch_size]\n",
    "\n",
    "            # forward prop\n",
    "            loss, dW = self.loss_and_gradient(x_train_batch, y_train_batch_off, y_train_batch_one_off)  # calc loss and dW\n",
    "            # calc velocity\n",
    "            self.velocity = (self.momentum * self.velocity) + (self.learning_rate * dW)\n",
    "            self.weights -= self.velocity  # update the weights\n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_test_off, y_test_one_off):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "\n",
    "        loss, _ = self.loss_and_gradient(x, y_test_off, y_test_one_off)  # calc loss and dW (don't need)\n",
    "        return loss\n",
    "\n",
    "    def run_epochs(self, x_train, y_train_off, y_train_one_off, x_test, y_test_off, y_test_one_off):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        num_dim = x_train.shape[1]  # num of dimensions\n",
    "        \n",
    "#         # create one hots of y labels\n",
    "#         y_train_one_off, y_test_one_off = self.one_hot_vary(y_train_off, y_test_off)\n",
    "\n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.001 * np.random.rand(num_dim, self.num_classes)\n",
    "        self.velocity = np.zeros(self.weights.shape)\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Ephoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_off, y_train_one_off)\n",
    "            test_loss = self.test_phase(x_test, y_test_off, y_test_one_off)\n",
    "            train_mse = self.calc_mse(x_train, y_train_off)\n",
    "            test_mse = self.calc_mse(x_test, y_test_off)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr  # return all the vals\n",
    "\n",
    "    def plot_graph(self, train_losses, test_losses, train_mse_arr, test_mse_arr):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs. Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "        plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "        # plt.legend(loc='best')\n",
    "        plt.title(\"Softmax Class. Epochs vs Accuracy\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= preprocessing code ========================\n",
    "\n",
    "def normalize_feat(x, mean=None, std=None):\n",
    "    # normalize the feature data.  test data must pass mean and std\n",
    "\n",
    "    # calc feature-wise mean\n",
    "    if mean is None:\n",
    "        mean = np.mean(x, axis=0)\n",
    "\n",
    "    # calc feature-wise std\n",
    "    if std is None:\n",
    "        std = np.std(x, axis=0)\n",
    "\n",
    "    # sub the mean per column\n",
    "    x_norm = x - mean\n",
    "\n",
    "    # div by the standard dev.\n",
    "    x_norm = x_norm / std\n",
    "\n",
    "    return x_norm, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loader\n",
    "\n",
    "def load_data(fname, bias=1):\n",
    "\n",
    "    data = loadtxt(fname, delimiter=',')\n",
    "\n",
    "    # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "    TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "    # process training data\n",
    "    x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "\n",
    "    x_train, train_mean, train_std = normalize_feat(x_train)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_train = np.hstack((x_train, col_bias))\n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "    y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # process test data\n",
    "    x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "    x_test, _, _ = normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "    y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_labels(y):\n",
    "    OFFSET = 1900 # starting the index 0 with year 1923\n",
    "    return y - OFFSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "fname = 'YearPredictionMSD.txt'\n",
    "\n",
    "# # note, features are normalized\n",
    "x_train, y_train, x_test, y_test = load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset to make labels start at index 0\n",
    "y_train_off = offset_labels(y_train)\n",
    "y_test_off = offset_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot on y labels\n",
    "\n",
    "def one_hot_vary(y_train, y_test):\n",
    "\n",
    "    train_size = len(y_train)\n",
    "    test_size = len(y_test)\n",
    "    \n",
    "#     print('y_train:', y_train[0])\n",
    "\n",
    "    stacked = np.vstack((y_train, y_test))\n",
    "    one_h = OneHotEncoder().fit_transform(stacked).toarray()\n",
    "    \n",
    "    y_train = one_h[0:train_size, :]\n",
    "    y_test = one_h[train_size:, :]\n",
    "    return y_train, y_test  # new one hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# obtain one hots on the variable number of classes\n",
    "y_train_one_off, y_test_one_off = one_hot_vary(y_train_off, y_test_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of years represented by list indexes\n",
    "train_years_key = np.unique(y_train_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters here\n",
    "num_classes = y_train_one_off.shape[1]\n",
    "epochs = 100\n",
    "learning_rate = 0.0001  # [0.1, 0.01, 0.001]\n",
    "batch_size = 100  # try powers of 2\n",
    "regularization = 1  # L2 weight decay, range [1, 0.1, 0.01, 0.001]\n",
    "momentum = 0.9  # started with 0 to 1, tried 2\n",
    "\n",
    "smc = SoftmaxClassifier(epochs, learning_rate, batch_size, regularization, momentum, num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ephoch 1 / 100...\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, train_mse, test_mse = smc.run_epochs(x_train, y_train_off, y_train_one_off, x_test, y_test_off, y_test_one_off)\n",
    "smc.plot_graph(train_losses, test_losses, train_mse, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_off[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
