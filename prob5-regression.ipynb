{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate = 0.00000001\n",
    "        self.epochs = 10\n",
    "        self.batch_size = 8\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.01  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "\n",
    "#         print('x before single ', x[0])\n",
    "        \n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x -= mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x /= std\n",
    "        \n",
    "#         print('x after single ', x[0])\n",
    "\n",
    "        return x, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], bias))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], bias))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "#         print('y test single', y_test[0])\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred.astype(int) - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        label -= self.y_mean\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    def label_add_mean(self, label):\n",
    "        \n",
    "        label += self.y_mean\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    def train_loss(self, x, yt):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]        \n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        residual = np.square(pred_y - yt) \n",
    "        sq_residual = np.linalg.norm(residual)\n",
    "        \n",
    "        loss = sq_residual + self.alpha + np.square( np.linalg.norm(self.weights) )\n",
    "    \n",
    "        return loss / n_samples\n",
    "    \n",
    "    def test_loss(self, x, yt):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "        \n",
    "        # need to add the mean back to label\n",
    "        yt = yt + self.y_mean\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # need to add the y mean back\n",
    "        pred_y += self.y_mean\n",
    "        \n",
    "        residual = np.square(pred_y - yt) \n",
    "        sq_residual = np.linalg.norm(residual)\n",
    "        \n",
    "        loss = sq_residual + self.alpha + np.square( np.linalg.norm(self.weights) )\n",
    "    \n",
    "        return loss / n_samples\n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        residual = np.square(pred_y - yt_sm) \n",
    "        dW = np.dot(x.T, residual) + 2 * self.weights * self.alpha\n",
    "        \n",
    "        return dW / n_samples\n",
    "\n",
    "    def calc_accuracy(self, x, y):\n",
    "        #  predict the class, then compare with the correct label.  return the average correct %\n",
    "        preds = np.argmax(x.dot(self.weights), 1)  # get prediction\n",
    "#         pred = pred.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "#         correct_count = np.equal(y, preds).sum()\n",
    "        avg = np.mean(np.equal(y, preds))\n",
    "        \n",
    "        return avg, preds   # return average over all the 1's (over the total)\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            # calc gradient (dont forget the regularization !)\n",
    "            \n",
    "            # ----------- need to add regularization!!! -------------\n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_acc_arr = []\n",
    "        test_acc_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Ephoch {} / {}...'.format(e + 1, self.epochs))\n",
    "            \n",
    "#             print('weights: ', self.weights)\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            \n",
    "\n",
    "#             print('y test', y_test)\n",
    "            \n",
    "            # pass\n",
    "            test_loss = self.test_phase(x_test, y_test)\n",
    "            \n",
    "#             print('train_loss: ', train_loss)\n",
    "#             train_acc, train_preds = self.calc_accuracy(x_train, y_train_sm)\n",
    "#             test_acc, test_preds = self.calc_accuracy(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "#             train_acc_arr.append(train_acc)\n",
    "#             test_acc_arr.append(test_acc)\n",
    "        \n",
    "        return train_losses, test_losses\n",
    "\n",
    "        # return all the vals\n",
    "#         return train_losses, test_losses, train_acc_arr, test_acc_arr, test_preds\n",
    "\n",
    "\n",
    "    def plot_graph(self, train_losses, test_losses, train_acc, test_acc):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs. Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "        plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "        # plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs Accuracy\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()\n",
    "\n",
    "    def make_mesh_grid(self, x, y, h=0.02):\n",
    "        # make a mesh grid for the decision boundary\n",
    "        \n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "        x_x, y_y = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return x_x, y_y  # matrix of x-axis and y-axis\n",
    "\n",
    "    def plot_contours(self, plt, x_x, y_y, **params):\n",
    "        # plot contours    \n",
    "\n",
    "        array = np.array([x_x.ravel(), y_y.ravel()])\n",
    "        f = np.dot(array.T, self.weights)\n",
    "        prob = self.softmax(f)\n",
    "        Q = np.argmax(prob, axis=1) + 1\n",
    "        Q = Q.reshape(x_x.shape)\n",
    "        plt.contourf(x_x, y_y, Q, **params)  # takes in variable number of params\n",
    "\n",
    "    def plot_decision_boundary(self, x, y):\n",
    "        # plot decision boundary\n",
    "\n",
    "        markers = ('o', '.', 'x')\n",
    "        colors = ('yellow', 'grey', 'green')\n",
    "        cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "        x_x, y_y = self.make_mesh_grid(x, y)\n",
    "        self.plot_contours(plt, x_x, y_y, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        \n",
    "        # plot training points\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            xBasedOnLabel = x[np.where(y[:,0] == cl)]\n",
    "            plt.scatter(x=xBasedOnLabel[:, 0], y=xBasedOnLabel[:, 1], c=cmap(idx),\n",
    "                        cmap=plt.cm.coolwarm, marker=markers[idx], label=cl)\n",
    "        plt.xlim(x_x.min(), x_x.max())\n",
    "        plt.ylim(y_y.min(), y_y.max())\n",
    "        plt.xlabel(\"x1\")\n",
    "        plt.ylabel(\"x2\")\n",
    "        plt.title(\"Decision Boundary - Softmax Classifier\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fname = 'YearPredictionMSD.txt'\n",
    "x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss the properties of the data\n",
    "x_min = np.min(x_train)\n",
    "x_max = np.max(x_train)\n",
    "\n",
    "y_min = np.min(y_train)\n",
    "y_max = np.max(y_train)\n",
    "\n",
    "\n",
    "# the range of the x feature values is huge, we from -14,000 to 65,000, with a wide\n",
    "# range in scales too, from 1000s to decimals, so we'll need to normalize\n",
    "\n",
    "# for the y labels, it's in years, from 1922-2011, and roughly the same in the test,\n",
    "# though slightly wider range.\n",
    "\n",
    "# The 90's and 2000's are much more over represented than the rest of the \n",
    "# years, expecially the earlier you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(1900, 2020, 20)\n",
    "\n",
    "plt.hist(y_train, bins, alpha=0.5, label='train')\n",
    "plt.hist(y_test, bins, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the most common year (in test data)\n",
    "\n",
    "years_arr, count = np.unique(y_test, return_counts=True)\n",
    "year_count_dict = dict(zip(years_arr, count))\n",
    "# year_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the loss if every prediction is most commmon \n",
    "n_samples = x_train.shape[0]\n",
    "most_common_year = 2007\n",
    "common_pred = np.full((n_samples, 1), most_common_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_loss = regr.musicMSE(common_pred, y_train)\n",
    "most_common_loss\n",
    "#  most common year loss = 193.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find loss for 1998\n",
    "n_samples = x_train.shape[0]\n",
    "most_common_year = 1998\n",
    "common_pred = np.full((n_samples, 1), most_common_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1998 = regr.musicMSE(common_pred, y_train)\n",
    "loss_1998\n",
    "#  1998 year loss = 119.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  Ridge Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "# print('y train', y_train)\n",
    "# print('y sub mean: ', y_train_sm)\n",
    "\n",
    "train_losses, test_losses = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n",
    "# train_losses, test_losses, train_acc_arr, test_acc_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
