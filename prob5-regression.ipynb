{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate = 0.00000001\n",
    "        self.epochs = 10\n",
    "        self.batch_size = 8\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.01  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "\n",
    "#         print('x before single ', x[0])\n",
    "        \n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x -= mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x /= std\n",
    "        \n",
    "#         print('x after single ', x[0])\n",
    "\n",
    "        return x, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], bias))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], bias))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "#         print('y test single', y_test[0])\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred.astype(int) - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        label -= self.y_mean\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    def label_add_mean(self, label):\n",
    "        \n",
    "        label += self.y_mean\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    def train_loss(self, x, yt):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]        \n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        residual = np.square(pred_y - yt) \n",
    "        sq_residual = np.linalg.norm(residual)\n",
    "        \n",
    "        loss = sq_residual + self.alpha + np.square( np.linalg.norm(self.weights) )\n",
    "    \n",
    "        return loss / n_samples\n",
    "    \n",
    "    def test_loss(self, x, yt):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "        \n",
    "        # need to add the mean back to label\n",
    "        yt = yt + self.y_mean\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # need to add the y mean back\n",
    "        pred_y += self.y_mean\n",
    "        \n",
    "        residual = np.square(pred_y - yt) \n",
    "        sq_residual = np.linalg.norm(residual)\n",
    "        \n",
    "        loss = sq_residual + self.alpha + np.square( np.linalg.norm(self.weights) )\n",
    "    \n",
    "        return loss / n_samples\n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        residual = np.square(pred_y - yt_sm) \n",
    "        dW = np.dot(x.T, residual) + 2 * self.weights * self.alpha\n",
    "        \n",
    "        return dW / n_samples\n",
    "\n",
    "    def calc_accuracy(self, x, y):\n",
    "        #  predict the class, then compare with the correct label.  return the average correct %\n",
    "        preds = np.argmax(x.dot(self.weights), 1)  # get prediction\n",
    "#         pred = pred.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "#         correct_count = np.equal(y, preds).sum()\n",
    "        avg = np.mean(np.equal(y, preds))\n",
    "        \n",
    "        return avg, preds   # return average over all the 1's (over the total)\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            # calc gradient (dont forget the regularization !)\n",
    "            \n",
    "            # ----------- need to add regularization!!! -------------\n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_acc_arr = []\n",
    "        test_acc_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Ephoch {} / {}...'.format(e + 1, self.epochs))\n",
    "            \n",
    "#             print('weights: ', self.weights)\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            \n",
    "\n",
    "#             print('y test', y_test)\n",
    "            \n",
    "            # pass\n",
    "            test_loss = self.test_phase(x_test, y_test)\n",
    "            \n",
    "#             print('train_loss: ', train_loss)\n",
    "            train_acc, train_preds = self.calc_accuracy(x_train, y_train_sm)\n",
    "            test_acc, test_preds = self.calc_accuracy(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_acc_arr.append(train_acc)\n",
    "            test_acc_arr.append(test_acc)\n",
    "        \n",
    "#         return train_losses, test_losses\n",
    "\n",
    "        return   # return all the vals\n",
    "\n",
    "\n",
    "    def plot_graph(self, train_losses, test_losses, train_acc, test_acc):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs. Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "        plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "        # plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs Accuracy\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()\n",
    "\n",
    "    def make_mesh_grid(self, x, y, h=0.02):\n",
    "        # make a mesh grid for the decision boundary\n",
    "        \n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "        x_x, y_y = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return x_x, y_y  # matrix of x-axis and y-axis\n",
    "\n",
    "    def plot_contours(self, plt, x_x, y_y, **params):\n",
    "        # plot contours    \n",
    "\n",
    "        array = np.array([x_x.ravel(), y_y.ravel()])\n",
    "        f = np.dot(array.T, self.weights)\n",
    "        prob = self.softmax(f)\n",
    "        Q = np.argmax(prob, axis=1) + 1\n",
    "        Q = Q.reshape(x_x.shape)\n",
    "        plt.contourf(x_x, y_y, Q, **params)  # takes in variable number of params\n",
    "\n",
    "    def plot_decision_boundary(self, x, y):\n",
    "        # plot decision boundary\n",
    "\n",
    "        markers = ('o', '.', 'x')\n",
    "        colors = ('yellow', 'grey', 'green')\n",
    "        cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "        x_x, y_y = self.make_mesh_grid(x, y)\n",
    "        self.plot_contours(plt, x_x, y_y, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        \n",
    "        # plot training points\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            xBasedOnLabel = x[np.where(y[:,0] == cl)]\n",
    "            plt.scatter(x=xBasedOnLabel[:, 0], y=xBasedOnLabel[:, 1], c=cmap(idx),\n",
    "                        cmap=plt.cm.coolwarm, marker=markers[idx], label=cl)\n",
    "        plt.xlim(x_x.min(), x_x.max())\n",
    "        plt.ylim(y_y.min(), y_y.max())\n",
    "        plt.xlabel(\"x1\")\n",
    "        plt.ylabel(\"x2\")\n",
    "        plt.title(\"Decision Boundary - Softmax Classifier\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fname = 'YearPredictionMSD.txt'\n",
    "x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss the properties of the data\n",
    "x_min = np.min(x_train)\n",
    "x_max = np.max(x_train)\n",
    "\n",
    "y_min = np.min(y_train)\n",
    "y_max = np.max(y_train)\n",
    "\n",
    "\n",
    "# the range of the x feature values is huge, we from -14,000 to 65,000, with a wide\n",
    "# range in scales too, from 1000s to decimals, so we'll need to normalize\n",
    "\n",
    "# for the y labels, it's in years, from 1922-2011, and roughly the same in the test,\n",
    "# though slightly wider range.\n",
    "\n",
    "# The 90's and 2000's are much more over represented than the rest of the \n",
    "# years, expecially the earlier you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHEFJREFUeJzt3X2UVdWZ5/HvL4C8+MarLkLJQHrQFTUGtaKk8zIaRwXsJTi+jNq2dHRWmUTXMumYESaTmDdXa7oTHKcjWdhWC50gsbUdmZWyBR2MmYmohdIKvlVBbClhAYIajYKBPPPH2SWH4lbVqXuLureK32etu+65z9n73L05devh7L3vKUUEZmZmRXyk2g0wM7P+w0nDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK2xwtRvQ28aOHRuTJk2qdjPMzPqV1atXvxER47orN+CSxqRJk2hubq52M8zM+hVJ/1aknIenzMysMCcNMzMrzEnDzMwK63ZOQ1Ij8GfA1og4McV+ARyXiowE3oqIqZImAS8CL6d9qyLiS6nOqcDdwHCgCbg+IkLSaOAXwCTgVeCSiHhTkoD/AcwE3gP+MiKeKaeTf/jDH2hra2Pnzp3lVO83hg0bRl1dHUOGDKl2U8xsgCoyEX438HfA4vZARPzn9m1JPwLezpVfHxFTSxxnAdAArCJLGtOBh4C5wKMRcYukuen1jcAMYEp6nJ7qn160Y3ltbW0cfvjhTJo0iSwXDTwRwfbt22lra2Py5MnVbo6ZDVDdDk9FxOPAjlL70tXAJcA9XR1D0njgiIh4IrK/+rQYmJ12zwIWpe1FHeKLI7MKGJmO02M7d+5kzJgxAzZhAEhizJgxA/5qysyqq9I5jc8BWyKiJRebLOlZSb+S9LkUmwC05cq0pRjA0RGxGSA9H5Wrs7GTOj02kBNGu4Ohj2ZWXZV+T+My9r3K2AxMjIjtaQ7jf0k6ASj126y7vzNbuI6kBrKhLyZOnNhto83MrDxlJw1Jg4H/BJzaHouIXcCutL1a0nrgWLKrhLpc9TpgU9reIml8RGxOw09bU7wNOKaTOvuIiIXAQoD6+vpu/+j5/BWvdNu/nvja2cd2uf+tt95iyZIlfOUrX+nRcWfOnMmSJUsYOXJkJc0zM+s1lVxp/EfgpYj4cNhJ0jhgR0TskfQxsknsDRGxQ9I7kqYBTwJXAv8zVVsGzAFuSc8P5uLXSVpKNgH+dvswVn/z1ltvcccdd+yXNPbs2cOgQYM6rdfU1HSgm2ZmOZX+h7K7/0AOBEWW3N4DnAGMldQG3BQRdwGXsv8E+OeB70naDewBvhQR7ZPoX2bvktuH0gOyZHGvpKuB14CLU7yJbLltK9mS2y+W0b+aMHfuXNavX8/UqVMZMmQIhx12GOPHj2fNmjW88MILzJ49m40bN7Jz506uv/56GhoagL23RHn33XeZMWMGn/3sZ/nNb37DhAkTePDBBxk+fHiVe2ZmB5tuk0ZEXNZJ/C9LxO4H7u+kfDNwYon4duCsEvEAru2uff3BLbfcwtq1a1mzZg2PPfYY5513HmvXrv1waWxjYyOjR4/m/fff51Of+hQXXnghY8aM2ecYLS0t3HPPPdx5551ccskl3H///VxxxRXV6I6ZHcQG3A0L+4PTTjttn+9S3H777TzwwAMAbNy4kZaWlv2SxuTJk5k6Nfv6y6mnnsqrr77aZ+01M2vnpFEFhx566Ifbjz32GI888ghPPPEEI0aM4Iwzzij5XYuhQ4d+uD1o0CDef//9PmmrmVme7z3VBw4//HDeeeedkvvefvttRo0axYgRI3jppZdYtWpVH7fOzKy4g/JKo69XOIwZM4bPfOYznHjiiQwfPpyjjz76w33Tp0/npz/9KSeddBLHHXcc06ZN69O2mZn1xEGZNKphyZIlJeNDhw7loYceKrmvfd5i7NixrF279sP4DTfc0OvtMzMrwsNTZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRV2cC65XfnXvXu8M+d1ubvcW6MD3HbbbTQ0NDBixIhyW2dm1mt8pdEH2m+NXo7bbruN9957r5dbZGZWnoPzSqOP5W+NfvbZZ3PUUUdx7733smvXLi644AK++93v8vvf/55LLrmEtrY29uzZw7e+9S22bNnCpk2bOPPMMxk7diwrV66sdlfM7CDnpNEH8rdGX758Offddx9PPfUUEcH555/P448/zrZt2/joRz/KL3/5SyC7J9WRRx7Jj3/8Y1auXMnYsWOr3AszMw9P9bnly5ezfPlyTj75ZE455RReeuklWlpa+MQnPsEjjzzCjTfeyK9//WuOPPLIajfVzGw/vtLoYxHBvHnzuOaaa/bbt3r1apqampg3bx7nnHMO3/72t6vQQjOzzvlKow/kb41+7rnn0tjYyLvvvgvA66+/ztatW9m0aRMjRozgiiuu4IYbbuCZZ57Zr66ZWbUdnFca3SyR7W35W6PPmDGDyy+/nE9/+tMAHHbYYfzsZz+jtbWVb3zjG3zkIx9hyJAhLFiwAICGhgZmzJjB+PHjPRFuZlWn7E9xDxz19fXR3Ny8T+zFF1/k4x//eJVa1LcOpr6a9bb5K16pqH5f/62e3iRpdUTUd1fOw1NmZlZYt0lDUqOkrZLW5mLfkfS6pDXpMTO3b56kVkkvSzo3F5+eYq2S5ubikyU9KalF0i8kHZLiQ9Pr1rR/Um912szMylPkSuNuYHqJ+PyImJoeTQCSjgcuBU5Ide6QNEjSIOAnwAzgeOCyVBbg1nSsKcCbwNUpfjXwZkT8e2B+Kle2gTYMV8rB0Eczq65uk0ZEPA7sKHi8WcDSiNgVEb8FWoHT0qM1IjZExAfAUmCWJAFfAO5L9RcBs3PHWpS27wPOSuV7bNiwYWzfvn1A/1KNCLZv386wYcOq3RQzG8AqWT11naQrgWbg6xHxJjABWJUr05ZiABs7xE8HxgBvRcTuEuUntNeJiN2S3k7l3+jYEEkNQAPAxIkT92toXV0dbW1tbNu2rYxu9h/Dhg2jrq6u2s0wswGs3KSxAPg+EOn5R8BVQKkrgaD0FU10UZ5u9u0bjFgILIRs9VTH/UOGDGHy5MmlqpqZWQ+UtXoqIrZExJ6I+CNwJ9nwE2RXCsfkitYBm7qIvwGMlDS4Q3yfY6X9R1J8mMzMzA6AspKGpPG5lxcA7SurlgGXppVPk4EpwFPA08CUtFLqELLJ8mWRTTKsBC5K9ecAD+aONSdtXwT8nxjIkxJmZv1At8NTku4BzgDGSmoDbgLOkDSVbLjoVeAagIhYJ+le4AVgN3BtROxJx7kOeBgYBDRGxLr0FjcCSyX9AHgWuCvF7wL+UVIr2RXGpRX31szMKtJt0oiIy0qE7yoRay9/M3BziXgT0FQivoG9w1v5+E7g4u7aZ2ZmfcffCDczs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCuk0akholbZW0Nhf7G0kvSXpO0gOSRqb4JEnvS1qTHj/N1TlV0vOSWiXdLkkpPlrSCkkt6XlUiiuVa03vc0rvd9/MzHqiyJXG3cD0DrEVwIkRcRLwCjAvt299RExNjy/l4guABmBKerQfcy7waERMAR5NrwFm5Mo2pPpmZlZF3SaNiHgc2NEhtjwidqeXq4C6ro4haTxwREQ8EREBLAZmp92zgEVpe1GH+OLIrAJGpuOYmVmV9MacxlXAQ7nXkyU9K+lXkj6XYhOAtlyZthQDODoiNgOk56NydTZ2UmcfkhokNUtq3rZtW2W9MTOzTlWUNCR9E9gN/DyFNgMTI+Jk4K+AJZKOAFSienR3+KJ1ImJhRNRHRP24ceOKNd7MzHpscLkVJc0B/gw4Kw05ERG7gF1pe7Wk9cCxZFcJ+SGsOmBT2t4iaXxEbE7DT1tTvA04ppM6ZmZWBWVdaUiaDtwInB8R7+Xi4yQNStsfI5vE3pCGnd6RNC2tmroSeDBVWwbMSdtzOsSvTKuopgFvtw9jmZlZdXR7pSHpHuAMYKykNuAmstVSQ4EVaeXsqrRS6vPA9yTtBvYAX4qI9kn0L5OtxBpONgfSPg9yC3CvpKuB14CLU7wJmAm0Au8BX6yko2ZmVrluk0ZEXFYifFcnZe8H7u9kXzNwYon4duCsEvEAru2ufWZm1nf8jXAzMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCuv2z72amfUX81e8Uu0mDHi+0jAzs8KcNMzMrLBCSUNSo6StktbmYqMlrZDUkp5Hpbgk3S6pVdJzkk7J1ZmTyrdImpOLnyrp+VTndknq6j3MzKw6il5p3A1M7xCbCzwaEVOAR9NrgBnAlPRoABZAlgCAm4DTgdOAm3JJYEEq215vejfvYWZmVVAoaUTE48CODuFZwKK0vQiYnYsvjswqYKSk8cC5wIqI2BERbwIrgOlp3xER8UREBLC4w7FKvYeZmVVBJXMaR0fEZoD0fFSKTwA25sq1pVhX8bYS8a7eYx+SGiQ1S2retm1bBV0yM7OuHIiJcJWIRRnxwiJiYUTUR0T9uHHjelLVzMx6oJKksSUNLZGet6Z4G3BMrlwdsKmbeF2JeFfvYWZmVVBJ0lgGtK+AmgM8mItfmVZRTQPeTkNLDwPnSBqVJsDPAR5O+96RNC2tmrqyw7FKvYeZmVVBoW+ES7oHOAMYK6mNbBXULcC9kq4GXgMuTsWbgJlAK/Ae8EWAiNgh6fvA06nc9yKifXL9y2QrtIYDD6UHXbyHmZlVQaGkERGXdbLrrBJlA7i2k+M0Ao0l4s3AiSXi20u9h5mZVYe/EW5mZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkVVnbSkHScpDW5x+8kfVXSdyS9novPzNWZJ6lV0suSzs3Fp6dYq6S5ufhkSU9KapH0C0mHlN9VMzOrVNlJIyJejoipETEVOBV4D3gg7Z7fvi8imgAkHQ9cCpwATAfukDRI0iDgJ8AM4HjgslQW4NZ0rCnAm8DV5bbXzMwq11vDU2cB6yPi37ooMwtYGhG7IuK3QCtwWnq0RsSGiPgAWArMkiTgC8B9qf4iYHYvtdfMzMrQW0njUuCe3OvrJD0nqVHSqBSbAGzMlWlLsc7iY4C3ImJ3h7iZmVVJxUkjzTOcD/xTCi0A/gSYCmwGftRetET1KCNeqg0NkpolNW/btq0HrTczs57ojSuNGcAzEbEFICK2RMSeiPgjcCfZ8BNkVwrH5OrVAZu6iL8BjJQ0uEN8PxGxMCLqI6J+3LhxvdAlMzMrpTeSxmXkhqYkjc/tuwBYm7aXAZdKGippMjAFeAp4GpiSVkodQjbUtSwiAlgJXJTqzwEe7IX2mplZmQZ3X6RzkkYAZwPX5MI/lDSVbCjp1fZ9EbFO0r3AC8Bu4NqI2JOOcx3wMDAIaIyIdelYNwJLJf0AeBa4q5L2mplZZSpKGhHxHtmEdT72F12Uvxm4uUS8CWgqEd/A3uEtMzOrMn8j3MzMCnPSMDOzwpw0zMysMCcNMzMrrKKJcDMz22v+ilcqqv+1s4/tpZYcOL7SMDOzwpw0zMysMCcNMzMrzEnDzMwKc9IwM7PCnDTMzKwwJw0zMyvMScPMzApz0jAzs8KcNMzMrDAnDTMzK8xJw8zMCnPSMDOzwpw0zMysMCcNMzMrzEnDzMwKqzhpSHpV0vOS1khqTrHRklZIaknPo1Jckm6X1CrpOUmn5I4zJ5VvkTQnFz81Hb811VWlbTYzs/L01pXGmRExNSLq0+u5wKMRMQV4NL0GmAFMSY8GYAFkSQa4CTgdOA24qT3RpDINuXrTe6nNZmbWQwdqeGoWsChtLwJm5+KLI7MKGClpPHAusCIidkTEm8AKYHrad0REPBERASzOHcvMzPpYbySNAJZLWi2pIcWOjojNAOn5qBSfAGzM1W1Lsa7ibSXiZmZWBYN74RifiYhNko4CVkh6qYuypeYjooz4vgfNklUDwMSJE7tvsZmZlaXipBERm9LzVkkPkM1JbJE0PiI2pyGmral4G3BMrnodsCnFz+gQfyzF60qU79iGhcBCgPr6+v2Sipn1D/NXvFLtJlg3KhqeknSopMPbt4FzgLXAMqB9BdQc4MG0vQy4Mq2imga8nYavHgbOkTQqTYCfAzyc9r0jaVpaNXVl7lhmZtbHKr3SOBp4IK2CHQwsiYh/kfQ0cK+kq4HXgItT+SZgJtAKvAd8ESAidkj6PvB0Kve9iNiRtr8M3A0MBx5KDzMzq4KKkkZEbAA+WSK+HTirRDyAazs5ViPQWCLeDJxYSTvNzKx3+BvhZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlaYk4aZmRXmpGFmZoU5aZiZWWFOGmZmVljZSUPSMZJWSnpR0jpJ16f4dyS9LmlNeszM1ZknqVXSy5LOzcWnp1irpLm5+GRJT0pqkfQLSYeU214zM6tcJVcau4GvR8THgWnAtZKOT/vmR8TU9GgCSPsuBU4ApgN3SBokaRDwE2AGcDxwWe44t6ZjTQHeBK6uoL1mZlahspNGRGyOiGfS9jvAi8CELqrMApZGxK6I+C3QCpyWHq0RsSEiPgCWArMkCfgCcF+qvwiYXW57zcyscr0ypyFpEnAy8GQKXSfpOUmNkkal2ARgY65aW4p1Fh8DvBURuzvEzcysSipOGpIOA+4HvhoRvwMWAH8CTAU2Az9qL1qiepQRL9WGBknNkpq3bdvWwx6YmVlRFSUNSUPIEsbPI+KfASJiS0TsiYg/AneSDT9BdqVwTK56HbCpi/gbwEhJgzvE9xMRCyOiPiLqx40bV0mXzMysC5WsnhJwF/BiRPw4Fx+fK3YBsDZtLwMulTRU0mRgCvAU8DQwJa2UOoRssnxZRASwErgo1Z8DPFhue83MrHKDuy/Sqc8AfwE8L2lNiv03stVPU8mGkl4FrgGIiHWS7gVeIFt5dW1E7AGQdB3wMDAIaIyIdel4NwJLJf0AeJYsSZlZjZq/4pVqN8EOsLKTRkT8X0rPOzR1Uedm4OYS8aZS9SJiA3uHt8zMrMr8jXAzMyvMScPMzAqrZE7DzGxAmfbaworqr5rY0EstqV1OGmb2IU9kW3c8PGVmZoU5aZiZWWFOGmZmVpiThpmZFeakYWZmhTlpmJlZYV5yazaAHOxLZiv9noV1z1caZmZWmJOGmZkV5qRhZmaFOWmYmVlhngg3qxEH+yS29Q9OGmZmvaTy1Vt/2yvtOJA8PGVmZoX5SsPMaoa/Z1H7fKVhZmaF+UrDrJd4ItsOBjV/pSFpuqSXJbVKmlvt9piZHcxq+kpD0iDgJ8DZQBvwtKRlEfFCdVtmNjB5TsG6U9NJAzgNaI2IDQCSlgKzACcN63UDYXjJv/TtQKv1pDEB2Jh73QacXqW2DHiV/tL82tnH9lJLylPtX/r+hW0Hg1pPGioRi/0KSQ1AQ3r5rqSXy3y/scAbZdatNX3el786cIceKOdloPQD3JcD47/8qNIjVNKXf1ekUK0njTbgmNzrOmBTx0IRsRCo+L95kpojor7S49QC96X2DJR+gPtSq/qiL7W+euppYIqkyZIOAS4FllW5TWZmB62avtKIiN2SrgMeBgYBjRGxrsrNMjM7aNV00gCIiCagqY/ebiDNZLovtWeg9APcl1p1wPuiiP3mlc3MzEqq9TkNMzOrIQM+aUhqlLRV0tpc7JOSnpD0vKT/LemI3L556ZYlL0s6Nxev6u1MetIPSWdLWp3iqyV9IVfn1BRvlXS7pFLLmmumL7n9EyW9K+mGXKzqt5gp4+frpLRvXdo/LMX71XmRNETSohR/UdK8XJ1qf1aOkbQytWudpOtTfLSkFZJa0vOoFFf6N2+V9JykU3LHmpPKt0ia0w/68uepD89J+o2kT+aO1TvnJSIG9AP4PHAKsDYXexr4D2n7KuD7aft44F+BocBkYD3ZBPygtP0x4JBU5vga7sfJwEfT9onA67k6TwGfJvsOzEPAjFo+J7n99wP/BNyQXlf9nJRxXgYDzwGfTK/HAIP643kBLgeWpu0RwKvApFo4L8B44JS0fTjwSvps/xCYm+JzgVvT9sz0by5gGvBkio8GNqTnUWl7VI335U/b2wjMyPWl187LgL/SiIjHgR0dwscBj6ftFcCFaXsW2QdhV0T8Fmglu5XJh7cziYgPgPbbmfSZnvQjIp6NiPbvs6wDhkkaKmk8cEREPBHZT9JiYPaBb/2+enhOkDSb7AObXzlX9XMCPe7LOcBzEfGvqe72iNjTT89LAIdKGgwMBz4AfkcNnJeI2BwRz6Ttd4AXye4uMQtYlIotYu+/8SxgcWRWASPTOTkXWBEROyLiTbL+T+/DrvS4LxHxm9RWgFVk322DXjwvAz5pdGItcH7avpi9XyAsdduSCV3Eq62zfuRdCDwbEbvI2tyW21cr/YBO+iLpUOBG4LsdytfqOYHOz8uxQEh6WNIzkv5rive78wLcB/we2Ay8BvxtROygxs6LpElkV95PAkdHxGbIfhkDR6Vi/eJzX7AveVeTXUFBL/blYE0aVwHXSlpNdsn3QYp3dtuSQrczqYLO+gGApBOAW4Fr2kMljlEL/YDO+/JdYH5EvNuhfH/sy2Dgs8Cfp+cLJJ1F/+zLacAe4KNkQ7lfl/Qxaqgvkg4jG9b8akT8rquiJWI19bnvQV/ay59JljRubA+VKFZWX2r+exoHQkS8RDZUgKRjgfPSrq5uW9Lt7Uz6Whf9QFId8ABwZUSsT+E29l6uQo30A7rsy+nARZJ+CIwE/ihpJ7CaGjwn0O3P168i4o20r4lsDuFn9L/zcjnwLxHxB2CrpP8H1JP9b7bq50XSELJfsj+PiH9O4S2SxkfE5jT8tDXFO/vctwFndIg/diDbXUoP+4Kkk4C/J5sX257ChW7JVEhfTupU60E2QZef3DsqPX+EbPz4qvT6BPadCN9ANoE0OG1PZu8k0gk13I+RqY0XljjG02STfe0TrjNr+Zx0qPMd9k6E18Q56eF5GQU8QzZxPBh4BDivP54Xsv/B/kNq76Fkf67gpFo4L6lNi4HbOsT/hn0nj3+Yts9j34nwp1J8NPDbdN5Gpe3RNd6XiWRzsX/aoXyvnZc+/6Hs6wdwD9m46x/Isu3VwPVkqxBeAW4hfckxlf8m2SqDl8mtYCFbYfFK2vfNWu4H8N/JxpvX5B7tH/56snHq9cDf5ftei33pUO87pKRRC+ekzJ+vK8gm9Ne2f9D743kBDiNbzbaOLGF8o1bOC9nQX5CtVGv/+Z9JtlrtUaAlPY9O5UX2x97WA88D9bljXUX2S7gV+GI/6MvfA2/myjb39nnxN8LNzKywg3Ui3MzMyuCkYWZmhTlpmJlZYU4aZmZWmJOGmZkV5qRhZmaFOWmYmVlhThpmZlbY/weAdkEh2Vb8fAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(1900, 2020, 20)\n",
    "\n",
    "plt.hist(y_train, bins, alpha=0.5, label='train')\n",
    "plt.hist(y_test, bins, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the most common year (in test data)\n",
    "\n",
    "years_arr, count = np.unique(y_test, return_counts=True)\n",
    "year_count_dict = dict(zip(years_arr, count))\n",
    "# year_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the loss if every prediction is most commmon \n",
    "n_samples = x_train.shape[0]\n",
    "most_common_year = 2007\n",
    "common_pred = np.full((n_samples, 1), most_common_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.87802179791854"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_loss = regr.musicMSE(common_pred, y_train)\n",
    "most_common_loss\n",
    "#  most common year loss = 193.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find loss for 1998\n",
    "n_samples = x_train.shape[0]\n",
    "most_common_year = 1998\n",
    "common_pred = np.full((n_samples, 1), most_common_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.82739576549339"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_1998 = regr.musicMSE(common_pred, y_train)\n",
    "loss_1998\n",
    "#  1998 year loss = 119.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ephoch 1 / 10...\n"
     ]
    }
   ],
   "source": [
    "# ==========  Ridge Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "# print('y train', y_train)\n",
    "# print('y sub mean: ', y_train_sm)\n",
    "\n",
    "train_losses, test_losses, train_acc_arr, test_acc_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
