{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate = 0.00001\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 100\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.2  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "        \n",
    "        # calc feature-wise mean\n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        # calc feature-wise std\n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x_norm = x - mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x_norm = x_norm / std\n",
    "\n",
    "        return x_norm, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        \n",
    "#         print('x before norm: ', x_train[0])\n",
    "        \n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        temp = label - self.y_mean\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def train_loss(self, x, yt_sm):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]        \n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        residual = np.linalg.norm(pred_y - yt_sm, ord=2, axis=0) \n",
    "        sq_residual = np.square(residual)\n",
    "        \n",
    "        loss = (sq_residual / n_samples) + self.alpha * np.square( np.linalg.norm(self.weights, ord=2, axis=0) )\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def test_loss(self, x, yt_sm):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "        \n",
    "        # need to add the mean back to label\n",
    "        yt = yt_sm + self.y_mean\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # need to add the y mean back\n",
    "        pred_y = pred_y + self.y_mean\n",
    "        \n",
    "        residual = np.linalg.norm(pred_y - yt, ord=2, axis=0) \n",
    "        sq_residual = np.square(residual)\n",
    "        \n",
    "        loss = (sq_residual / n_samples) + self.alpha * np.square( np.linalg.norm(self.weights, ord=2, axis=0) )\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        residual = pred_y - yt_sm\n",
    "        dW = 2 * (np.dot(x.T, residual) / n_samples) + 2 * self.weights * self.alpha\n",
    "        \n",
    "        return dW\n",
    "\n",
    "    def calc_mse(self, x, y_sm):\n",
    "        # preprocesses (adds the y_mean back to both x and y, and calls musicMSE)\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # add the y mean to the pred and convert to int to round\n",
    "        pred_y += self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        pred_y = pred_y.astype(int)\n",
    "        \n",
    "        # add the y mean back to the labels\n",
    "        y_labels = y_sm + self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        y_labels = y_labels.astype(int)\n",
    "        \n",
    "        # calc the MSE\n",
    "        mse = self.musicMSE(pred_y, y_labels)\n",
    "        \n",
    "        print('MSE: ', mse)\n",
    "        \n",
    "        return mse, pred_y\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            \n",
    "#             print('weights: ', self.weights)\n",
    "            \n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Epoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            test_loss = self.test_phase(x_test, y_test_sm)\n",
    "            \n",
    "            train_mse, train_preds = self.calc_mse(x_train, y_train_sm)\n",
    "            test_mse, test_preds = self.calc_mse(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "        \n",
    "#         return train_losses, test_losses\n",
    "\n",
    "        # return all the vals\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds\n",
    "\n",
    "    def closed_form(self, x, yt):\n",
    "        # yt is regular labels\n",
    "        # returns the weights w that allow you to find the prediction\n",
    "\n",
    "        xt = np.transpose(x)\n",
    "        alpha_identity = self.alpha * np.identity(len(xt))\n",
    "\n",
    "        theInverse = np.linalg.inv(np.dot(xt, x) + alpha_identity)\n",
    "        w = np.dot(np.dot(theInverse, xt), yt)\n",
    "        return w\n",
    "    \n",
    "    \n",
    "    def plot_graph(self, train_losses, test_losses, train_mse, test_mse):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs. Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse, label=\"Train MSE\")\n",
    "        plt.plot(test_mse, label=\"Test MSE\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs MSE\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()\n",
    "        # save plot \n",
    "        plt.savefig('./regression_loss_acc')\n",
    "\n",
    "    def make_mesh_grid(self, x, y, h=0.02):\n",
    "        # make a mesh grid for the decision boundary\n",
    "        \n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "        x_x, y_y = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return x_x, y_y  # matrix of x-axis and y-axis\n",
    "\n",
    "    def plot_contours(self, plt, x_x, y_y, **params):\n",
    "        # plot contours    \n",
    "\n",
    "        array = np.array([x_x.ravel(), y_y.ravel()])\n",
    "        f = np.dot(array.T, self.weights)\n",
    "        prob = self.softmax(f)\n",
    "        Q = np.argmax(prob, axis=1) + 1\n",
    "        Q = Q.reshape(x_x.shape)\n",
    "        plt.contourf(x_x, y_y, Q, **params)  # takes in variable number of params\n",
    "\n",
    "    def plot_decision_boundary(self, x, y):\n",
    "        # plot decision boundary\n",
    "\n",
    "        markers = ('o', '.', 'x')\n",
    "        colors = ('yellow', 'grey', 'green')\n",
    "        cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "        x_x, y_y = self.make_mesh_grid(x, y)\n",
    "        self.plot_contours(plt, x_x, y_y, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        \n",
    "        # plot training points\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            xBasedOnLabel = x[np.where(y[:,0] == cl)]\n",
    "            plt.scatter(x=xBasedOnLabel[:, 0], y=xBasedOnLabel[:, 1], c=cmap(idx),\n",
    "                        cmap=plt.cm.coolwarm, marker=markers[idx], label=cl)\n",
    "        plt.xlim(x_x.min(), x_x.max())\n",
    "        plt.ylim(y_y.min(), y_y.max())\n",
    "        plt.xlabel(\"x1\")\n",
    "        plt.ylabel(\"x2\")\n",
    "        plt.title(\"Decision Boundary - Softmax Classifier\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_weights(self):\n",
    "        \n",
    "        plt.hist(self.weights, bins=12)\n",
    "        plt.xlabel('bins')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Ridge Regression Weights Histogram')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load the data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "# x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # discuss the properties of the data\n",
    "# x_min = np.min(x_train)\n",
    "# x_max = np.max(x_train)\n",
    "\n",
    "# y_min = np.min(y_train)\n",
    "# y_max = np.max(y_train)\n",
    "\n",
    "\n",
    "# the range of the x feature values is huge, we from -14,000 to 65,000, with a wide\n",
    "# range in scales too, from 1000s to decimals, so we'll need to normalize\n",
    "\n",
    "# for the y labels, it's in years, from 1922-2011, and roughly the same in the test,\n",
    "# though slightly wider range.\n",
    "\n",
    "# The 90's and 2000's are much more over represented than the rest of the \n",
    "# years, expecially the earlier you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(1900, 2020, 20)\n",
    "\n",
    "# plt.hist(y_train, bins, alpha=0.5, label='train')\n",
    "# plt.hist(y_test, bins, alpha=0.5, label='test')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  find the most common year (in test data)\n",
    "\n",
    "# years_arr, count = np.unique(y_test, return_counts=True)\n",
    "# year_count_dict = dict(zip(years_arr, count))\n",
    "# # year_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the loss if every prediction is most commmon \n",
    "# n_samples = x_train.shape[0]\n",
    "# most_common_year = 2007\n",
    "# common_pred = np.full((n_samples, 1), most_common_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_loss = regr.musicMSE(common_pred, y_train)\n",
    "# most_common_loss\n",
    "# #  most common year loss = 193.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find loss for 1998\n",
    "# n_samples = x_train.shape[0]\n",
    "# most_common_year = 1998\n",
    "# common_pred = np.full((n_samples, 1), most_common_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_1998 = regr.musicMSE(common_pred, y_train)\n",
    "# loss_1998\n",
    "# #  1998 year loss = 119.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100...\n",
      "floor of pred y [[1998.]\n",
      " [1998.]\n",
      " [1998.]\n",
      " ...\n",
      " [1999.]\n",
      " [1998.]\n",
      " [1999.]]\n",
      "MSE:  113.8080735108278\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1998.]\n",
      " ...\n",
      " [1998.]\n",
      " [1998.]\n",
      " [1998.]]\n",
      "MSE:  112.14669481513045\n",
      "Epoch 2 / 100...\n",
      "floor of pred y [[1998.]\n",
      " [1998.]\n",
      " [1998.]\n",
      " ...\n",
      " [2000.]\n",
      " [1997.]\n",
      " [1999.]]\n",
      "MSE:  110.11345786411452\n",
      "floor of pred y [[1997.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1998.]\n",
      " [1999.]\n",
      " [1999.]]\n",
      "MSE:  108.49172009064321\n",
      "Epoch 3 / 100...\n",
      "floor of pred y [[1998.]\n",
      " [1998.]\n",
      " [1998.]\n",
      " ...\n",
      " [2000.]\n",
      " [1997.]\n",
      " [2000.]]\n",
      "MSE:  107.75588401471597\n",
      "floor of pred y [[1996.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1998.]\n",
      " [1999.]\n",
      " [1999.]]\n",
      "MSE:  106.2270341461525\n",
      "Epoch 4 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1998.]\n",
      " ...\n",
      " [2000.]\n",
      " [1997.]\n",
      " [2000.]]\n",
      "MSE:  105.98565710761375\n",
      "floor of pred y [[1996.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1998.]\n",
      " [2000.]\n",
      " [1999.]]\n",
      "MSE:  104.55840483430497\n",
      "Epoch 5 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1998.]\n",
      " ...\n",
      " [2000.]\n",
      " [1996.]\n",
      " [2000.]]\n",
      "MSE:  104.64161530598602\n",
      "floor of pred y [[1996.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1998.]\n",
      " [2000.]\n",
      " [1999.]]\n",
      "MSE:  103.23652456857315\n",
      "Epoch 6 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1996.]\n",
      " [2001.]]\n",
      "MSE:  103.53008104133151\n",
      "floor of pred y [[1995.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1998.]\n",
      " [2000.]\n",
      " [1999.]]\n",
      "MSE:  102.24678971935465\n",
      "Epoch 7 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1996.]\n",
      " [2001.]]\n",
      "MSE:  102.64291998947627\n",
      "floor of pred y [[1995.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2000.]\n",
      " [1999.]]\n",
      "MSE:  101.3463035773082\n",
      "Epoch 8 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1996.]\n",
      " [2001.]]\n",
      "MSE:  101.88488594262843\n",
      "floor of pred y [[1995.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  100.58794135306309\n",
      "Epoch 9 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1996.]\n",
      " [2001.]]\n",
      "MSE:  101.22155682166162\n",
      "floor of pred y [[1995.]\n",
      " [1997.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  99.96757761809765\n",
      "Epoch 10 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2001.]]\n",
      "MSE:  100.64822282700112\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  99.43040034088048\n",
      "Epoch 11 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2001.]]\n",
      "MSE:  100.15003644487766\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  98.9672677267533\n",
      "Epoch 12 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  99.71443174025369\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  98.60293234684589\n",
      "Epoch 13 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  99.3325670564184\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  98.2507602022041\n",
      "Epoch 14 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  98.97463522774814\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  97.90298464101025\n",
      "Epoch 15 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  98.64910267966893\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  97.56435087447464\n",
      "Epoch 16 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  98.35072480020013\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [1999.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  97.32073754139955\n",
      "Epoch 17 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  98.07775051001263\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  97.10983711336213\n",
      "Epoch 18 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1995.]\n",
      " [2002.]]\n",
      "MSE:  97.83678948662322\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  96.90505703937556\n",
      "Epoch 19 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2001.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  97.60616888858219\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  96.68195463965448\n",
      "Epoch 20 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  97.40112655645505\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  96.51234723325135\n",
      "Epoch 21 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  97.21391417986086\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  96.33954407235963\n",
      "Epoch 22 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  97.04244857821847\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  96.13172318955667\n",
      "Epoch 23 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  96.87508679919088\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  95.97561542484166\n",
      "Epoch 24 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  96.72869699858103\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  95.81997249714318\n",
      "Epoch 25 / 100...\n",
      "floor of pred y [[1997.]\n",
      " [1998.]\n",
      " [1997.]\n",
      " ...\n",
      " [2000.]\n",
      " [1994.]\n",
      " [2002.]]\n",
      "MSE:  96.59327516529585\n",
      "floor of pred y [[1995.]\n",
      " [1996.]\n",
      " [2000.]\n",
      " ...\n",
      " [1997.]\n",
      " [2001.]\n",
      " [1999.]]\n",
      "MSE:  95.69868877224923\n",
      "Epoch 26 / 100...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-2787803d47f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my_test_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_sub_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-151-7279cc9d084a>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, x_train, y_train_sm, x_test, y_test_sm)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;31m# calc loss and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-7279cc9d084a>\u001b[0m in \u001b[0;36mtrain_phase\u001b[0;34m(self, x_train, y_train_sm)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m  \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-7279cc9d084a>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, yt_sm)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myt_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========  Ridge Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_mse_arr, test_mse_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed form solution to ridge regression, obtain w from training data\n",
    "w_closed_form = regr.closed_form(x_train, y_train)\n",
    "\n",
    "# apply w to test data\n",
    "y_pred = np.dot(x_test, w_closed_form)\n",
    "\n",
    "# calc MSE \n",
    "mse_closed_form = regr.musicMSE(y_pred, y_test)\n",
    "mse_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the low of ~93 for the iterative solution I got, the closed form is much lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "regr.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1996.86316457],\n",
       "       [1994.88400408],\n",
       "       [2002.62592175],\n",
       "       ...,\n",
       "       [1996.98506823],\n",
       "       [2001.89154874],\n",
       "       [1999.71070333]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
