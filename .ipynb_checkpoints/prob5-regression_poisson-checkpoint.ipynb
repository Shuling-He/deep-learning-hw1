{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate =  0.000001\n",
    "        self.epochs = 500\n",
    "        self.batch_size = 100\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.5  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "        \n",
    "        # calc feature-wise mean\n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        # calc feature-wise std\n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x_norm = x - mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x_norm = x_norm / std\n",
    "\n",
    "        return x_norm, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        \n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        temp = label - self.y_mean\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def train_loss(self, x, yt_sm):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.clip(np.exp(np.dot(x, self.weights)), 0, 120)\n",
    "        \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt_sm\n",
    "    \n",
    "        # calc the diff, and divide\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def test_loss(self, x, yt_sm):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        # this function adds the y mean back\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "\n",
    "        # predict\n",
    "#         pred_y = np.exp(np.dot(x, self.weights)) + self.y_mean\n",
    "        pred_y = np.clip(np.exp(np.dot(x, self.weights)), 0, 120)\n",
    "    \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # need to add the mean back to label\n",
    "#         yt = yt_sm + self.y_mean\n",
    "        yt = yt_sm\n",
    "    \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt\n",
    "        \n",
    "    \n",
    "        # calc the diff, divide, and add the mean\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "#         pred_y = np.exp(np.dot(x, self.weights))\n",
    "#         pred_y = np.clip(np.exp(np.dot(x, self.weights)), 0, 2020)\n",
    "        \n",
    "#         x_trans_dot_pred_y = np.dot(x.T, pred_y)\n",
    "        \n",
    "#         dW = x_trans_dot_pred_y - np.dot(x.T, yt_sm)\n",
    "\n",
    "        y_pred = np.clip(np.exp(np.dot(x, self.weights)), 0, 2020)\n",
    "\n",
    "        dW = np.dot(x.T, (y_pred - yt_sm).reshape(-1)).reshape(-1, 1)\n",
    "        \n",
    "#         print('dW shape: ', dW.shape)\n",
    "        \n",
    "        # return the avg dW\n",
    "        return dW / n_samples\n",
    "\n",
    "    def calc_mse(self, x, y_sm):\n",
    "        # preprocesses (adds the y_mean back to both x and y, and calls musicMSE)\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # add the y mean to the pred and convert to int to round\n",
    "#         pred_y += self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        pred_y = pred_y\n",
    "        \n",
    "        # add the y mean back to the labels\n",
    "#         y_labels = y_sm + self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        y_labels = y_sm\n",
    "        \n",
    "        # calc the MSE\n",
    "        mse = self.musicMSE(pred_y, y_labels)\n",
    "        \n",
    "        return mse, pred_y\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            \n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Epoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            test_loss = self.test_phase(x_test, y_test_sm)\n",
    "            \n",
    "            train_mse, train_preds = self.calc_mse(x_train, y_train_sm)\n",
    "            test_mse, test_preds = self.calc_mse(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "        \n",
    "#         return train_losses, test_losses\n",
    "\n",
    "        # return all the vals\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds\n",
    "    \n",
    "    def plot_graph(self, train_losses, test_losses, train_mse, test_mse):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Poisson: Loss vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse, label=\"Train MSE\")\n",
    "        plt.plot(test_mse, label=\"Test MSE\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Poisson: MSE vs. Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_weights(self):\n",
    "        \n",
    "        plt.hist(self.weights, bins=12)\n",
    "        plt.xlabel('bins')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Lasso Regression Weights Histogram')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "# x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 500...\n",
      "train loss:  0.8919079975055711\n",
      "test loss:  0.7884479304661206\n",
      "train MSE:  111.80767887102826\n",
      "test MSE:  109.93081675737444\n",
      "Epoch 2 / 500...\n",
      "train loss:  0.6798943947518468\n",
      "test loss:  0.5839644158632266\n",
      "train MSE:  111.80732736126147\n",
      "test MSE:  109.92502566287695\n",
      "Epoch 3 / 500...\n",
      "train loss:  0.4747946681231526\n",
      "test loss:  0.38626840260302076\n",
      "train MSE:  111.80125249615064\n",
      "test MSE:  109.91584513180067\n",
      "Epoch 4 / 500...\n",
      "train loss:  0.2763720844839729\n",
      "test loss:  0.1951262441051476\n",
      "train MSE:  111.77412154905825\n",
      "test MSE:  109.89041467335515\n",
      "Epoch 5 / 500...\n",
      "train loss:  0.08435510717273548\n",
      "test loss:  0.01033618605718645\n",
      "train MSE:  111.70623487753227\n",
      "test MSE:  109.83310414285991\n",
      "Epoch 6 / 500...\n",
      "train loss:  -0.10154769784259449\n",
      "test loss:  -0.1681809688134471\n",
      "train MSE:  111.59225298352001\n",
      "test MSE:  109.72191125486626\n",
      "Epoch 7 / 500...\n",
      "train loss:  -0.28101979018011625\n",
      "test loss:  -0.3404392433415526\n",
      "train MSE:  111.4430748262938\n",
      "test MSE:  109.598574499816\n",
      "Epoch 8 / 500...\n",
      "train loss:  -0.45521130668349086\n",
      "test loss:  -0.5065782527941347\n",
      "train MSE:  111.26755500157425\n",
      "test MSE:  109.41786910964343\n",
      "Epoch 9 / 500...\n",
      "train loss:  -0.6226768745704168\n",
      "test loss:  -0.6665016299104004\n",
      "train MSE:  111.08301237400639\n",
      "test MSE:  109.2355948945401\n",
      "Epoch 10 / 500...\n",
      "train loss:  -0.7839299266826752\n",
      "test loss:  -0.8202190929599009\n",
      "train MSE:  110.87369369913351\n",
      "test MSE:  109.01901958125931\n",
      "Epoch 11 / 500...\n",
      "train loss:  -0.9402759229694634\n",
      "test loss:  -0.9678396142293759\n",
      "train MSE:  110.67043910686328\n",
      "test MSE:  108.81232205457961\n",
      "Epoch 12 / 500...\n",
      "train loss:  -1.0901689916163617\n",
      "test loss:  -1.1094814025831958\n",
      "train MSE:  110.46453633058307\n",
      "test MSE:  108.6150762139025\n",
      "Epoch 13 / 500...\n",
      "train loss:  -1.2331827452760846\n",
      "test loss:  -1.2450846967152251\n",
      "train MSE:  110.25970749211798\n",
      "test MSE:  108.39782301330595\n",
      "Epoch 14 / 500...\n",
      "train loss:  -1.3714742982130128\n",
      "test loss:  -1.3750161071721774\n",
      "train MSE:  110.05155117162734\n",
      "test MSE:  108.19755573202146\n",
      "Epoch 15 / 500...\n",
      "train loss:  -1.504044029880592\n",
      "test loss:  -1.4994076626882875\n",
      "train MSE:  109.8413375485752\n",
      "test MSE:  107.99405395983034\n",
      "Epoch 16 / 500...\n",
      "train loss:  -1.6313230381585602\n",
      "test loss:  -1.6187402612625368\n",
      "train MSE:  109.62861591411948\n",
      "test MSE:  107.80350951947473\n",
      "Epoch 17 / 500...\n",
      "train loss:  -1.7538441769742996\n",
      "test loss:  -1.7335809682387002\n",
      "train MSE:  109.42472946686966\n",
      "test MSE:  107.6019639363948\n",
      "Epoch 18 / 500...\n",
      "train loss:  -1.872528461552695\n",
      "test loss:  -1.8438336652556586\n",
      "train MSE:  109.22527463048344\n",
      "test MSE:  107.41752048188104\n",
      "Epoch 19 / 500...\n",
      "train loss:  -1.9856432492578693\n",
      "test loss:  -1.9498381744146978\n",
      "train MSE:  109.02812509434695\n",
      "test MSE:  107.23638899110999\n",
      "Epoch 20 / 500...\n",
      "train loss:  -2.09523636260023\n",
      "test loss:  -2.0517982286280496\n",
      "train MSE:  108.83507722432361\n",
      "test MSE:  107.04665801553331\n",
      "Epoch 21 / 500...\n",
      "train loss:  -2.2020527513558976\n",
      "test loss:  -2.1500796096995756\n",
      "train MSE:  108.65123977279099\n",
      "test MSE:  106.89033720051907\n",
      "Epoch 22 / 500...\n",
      "train loss:  -2.30399005434488\n",
      "test loss:  -2.2450720941848665\n",
      "train MSE:  108.4821484794507\n",
      "test MSE:  106.73053010788092\n",
      "Epoch 23 / 500...\n",
      "train loss:  -2.404067072274069\n",
      "test loss:  -2.3372222626932158\n",
      "train MSE:  108.31142902737463\n",
      "test MSE:  106.55296236756988\n",
      "Epoch 24 / 500...\n",
      "train loss:  -2.4983034150367796\n",
      "test loss:  -2.426353942569347\n",
      "train MSE:  108.15221235502918\n",
      "test MSE:  106.38254149638783\n",
      "Epoch 25 / 500...\n",
      "train loss:  -2.593556992455699\n",
      "test loss:  -2.5127771567007198\n",
      "train MSE:  107.99103757919752\n",
      "test MSE:  106.22784761093142\n",
      "Epoch 26 / 500...\n",
      "train loss:  -2.6846017609144797\n",
      "test loss:  -2.5966610200531286\n",
      "train MSE:  107.84007168211441\n",
      "test MSE:  106.07979702116945\n",
      "Epoch 27 / 500...\n",
      "train loss:  -2.771716668238627\n",
      "test loss:  -2.6783409944773706\n",
      "train MSE:  107.69056358013776\n",
      "test MSE:  105.96837171466754\n",
      "Epoch 28 / 500...\n",
      "train loss:  -2.8580680812930623\n",
      "test loss:  -2.7578645913665447\n",
      "train MSE:  107.55211617505617\n",
      "test MSE:  105.85109720904109\n",
      "Epoch 29 / 500...\n",
      "train loss:  -2.9416893989193715\n",
      "test loss:  -2.8353664677283983\n",
      "train MSE:  107.40366475888155\n",
      "test MSE:  105.70016075613488\n",
      "Epoch 30 / 500...\n",
      "train loss:  -3.0232681481208914\n",
      "test loss:  -2.9110035519778332\n",
      "train MSE:  107.25423429096382\n",
      "test MSE:  105.58908407739537\n",
      "Epoch 31 / 500...\n",
      "train loss:  -3.104877692046516\n",
      "test loss:  -2.9850285886835266\n",
      "train MSE:  107.11522188245341\n",
      "test MSE:  105.48844686331854\n",
      "Epoch 32 / 500...\n",
      "train loss:  -3.1839838297489926\n",
      "test loss:  -3.057434499000837\n",
      "train MSE:  106.98037367860363\n",
      "test MSE:  105.36913869574481\n",
      "Epoch 33 / 500...\n",
      "train loss:  -3.260333012612308\n",
      "test loss:  -3.1285292559498226\n",
      "train MSE:  106.85876855130533\n",
      "test MSE:  105.2541496387829\n",
      "Epoch 34 / 500...\n",
      "train loss:  -3.33688242614744\n",
      "test loss:  -3.1981395075077823\n",
      "train MSE:  106.74250939156462\n",
      "test MSE:  105.1225620266894\n",
      "Epoch 35 / 500...\n",
      "train loss:  -3.409786452843632\n",
      "test loss:  -3.266564566165805\n",
      "train MSE:  106.62370771639415\n",
      "test MSE:  104.99635877670391\n",
      "Epoch 36 / 500...\n",
      "train loss:  -3.4836405079142745\n",
      "test loss:  -3.3336633407567393\n",
      "train MSE:  106.49732378146875\n",
      "test MSE:  104.87321570374388\n",
      "Epoch 37 / 500...\n",
      "train loss:  -3.555257746219068\n",
      "test loss:  -3.3996703442197984\n",
      "train MSE:  106.38362654567254\n",
      "test MSE:  104.77180376130619\n",
      "Epoch 38 / 500...\n",
      "train loss:  -3.625830779218704\n",
      "test loss:  -3.464440117367806\n",
      "train MSE:  106.27734767550689\n",
      "test MSE:  104.69190989909163\n",
      "Epoch 39 / 500...\n",
      "train loss:  -3.695123424058903\n",
      "test loss:  -3.5283489781202975\n",
      "train MSE:  106.1625549368792\n",
      "test MSE:  104.58530727663613\n",
      "Epoch 40 / 500...\n",
      "train loss:  -3.762638366292475\n",
      "test loss:  -3.591232340710239\n",
      "train MSE:  106.0548614016398\n",
      "test MSE:  104.49940926962483\n",
      "Epoch 41 / 500...\n",
      "train loss:  -3.83036288830483\n",
      "test loss:  -3.653237384095339\n",
      "train MSE:  105.94695868574165\n",
      "test MSE:  104.38802269954097\n",
      "Epoch 42 / 500...\n",
      "train loss:  -3.895130585932956\n",
      "test loss:  -3.7144838655325607\n",
      "train MSE:  105.84070353709399\n",
      "test MSE:  104.29890956983208\n",
      "Epoch 43 / 500...\n",
      "train loss:  -3.9618668513449635\n",
      "test loss:  -3.7747546592111565\n",
      "train MSE:  105.73912584049651\n",
      "test MSE:  104.22496174778718\n",
      "Epoch 44 / 500...\n",
      "train loss:  -4.027194206708512\n",
      "test loss:  -3.8343068942732788\n",
      "train MSE:  105.65144464044648\n",
      "test MSE:  104.13089035656873\n",
      "Epoch 45 / 500...\n",
      "train loss:  -4.09694494203541\n",
      "test loss:  -3.8932128708823703\n",
      "train MSE:  105.56541100764696\n",
      "test MSE:  104.02918789099571\n",
      "Epoch 46 / 500...\n",
      "train loss:  -4.153553649243757\n",
      "test loss:  -3.9512970433335535\n",
      "train MSE:  105.47539431632428\n",
      "test MSE:  103.95109527222017\n",
      "Epoch 47 / 500...\n",
      "train loss:  -4.215723114757957\n",
      "test loss:  -4.008812793645465\n",
      "train MSE:  105.38477380454331\n",
      "test MSE:  103.87081404582518\n",
      "Epoch 48 / 500...\n",
      "train loss:  -4.277709097403991\n",
      "test loss:  -4.065632603759735\n",
      "train MSE:  105.29485846879757\n",
      "test MSE:  103.77654897251651\n",
      "Epoch 49 / 500...\n",
      "train loss:  -4.337901554044943\n",
      "test loss:  -4.121797453699286\n",
      "train MSE:  105.20792126181223\n",
      "test MSE:  103.71761151246345\n",
      "Epoch 50 / 500...\n",
      "train loss:  -4.403685149291014\n",
      "test loss:  -4.177550937988105\n",
      "train MSE:  105.11309125883626\n",
      "test MSE:  103.63713660397823\n",
      "Epoch 51 / 500...\n",
      "train loss:  -4.457164847630601\n",
      "test loss:  -4.232580299740949\n",
      "train MSE:  105.02675571580758\n",
      "test MSE:  103.5809106931882\n",
      "Epoch 52 / 500...\n",
      "train loss:  -4.517010115561519\n",
      "test loss:  -4.287083760598957\n",
      "train MSE:  104.93834561820432\n",
      "test MSE:  103.50895779667255\n",
      "Epoch 53 / 500...\n",
      "train loss:  -4.573966521516825\n",
      "test loss:  -4.3410149814721875\n",
      "train MSE:  104.84877963572374\n",
      "test MSE:  103.44012318180938\n",
      "Epoch 54 / 500...\n",
      "train loss:  -4.631436342333722\n",
      "test loss:  -4.394604049708959\n",
      "train MSE:  104.77476203004439\n",
      "test MSE:  103.3582537622746\n",
      "Epoch 55 / 500...\n",
      "train loss:  -4.689744199514844\n",
      "test loss:  -4.44756866757813\n",
      "train MSE:  104.70331713081771\n",
      "test MSE:  103.29557823788035\n",
      "Epoch 56 / 500...\n",
      "train loss:  -4.745471073527059\n",
      "test loss:  -4.4999994324451125\n",
      "train MSE:  104.6268583652855\n",
      "test MSE:  103.23359996901087\n",
      "Epoch 57 / 500...\n",
      "train loss:  -4.800208551381126\n",
      "test loss:  -4.552202715494656\n",
      "train MSE:  104.55289898515034\n",
      "test MSE:  103.17030466192791\n",
      "Epoch 58 / 500...\n",
      "train loss:  -4.856847060330123\n",
      "test loss:  -4.603903546375556\n",
      "train MSE:  104.48095809054719\n",
      "test MSE:  103.09418760047258\n",
      "Epoch 59 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -4.914507920911446\n",
      "test loss:  -4.655231496974609\n",
      "train MSE:  104.4054503422368\n",
      "test MSE:  103.0366640196781\n",
      "Epoch 60 / 500...\n",
      "train loss:  -4.967462183004678\n",
      "test loss:  -4.705885874207935\n",
      "train MSE:  104.33625251771566\n",
      "test MSE:  102.9726133524433\n",
      "Epoch 61 / 500...\n",
      "train loss:  -5.020981075708221\n",
      "test loss:  -4.756221718813949\n",
      "train MSE:  104.27216775857534\n",
      "test MSE:  102.90374000116209\n",
      "Epoch 62 / 500...\n",
      "train loss:  -5.074758330903722\n",
      "test loss:  -4.806248399376758\n",
      "train MSE:  104.20126629776112\n",
      "test MSE:  102.85909627936705\n",
      "Epoch 63 / 500...\n",
      "train loss:  -5.127412008235636\n",
      "test loss:  -4.855938522550043\n",
      "train MSE:  104.13859620369452\n",
      "test MSE:  102.7952780306405\n",
      "Epoch 64 / 500...\n",
      "train loss:  -5.180241846492393\n",
      "test loss:  -4.9052768425640725\n",
      "train MSE:  104.07324342159176\n",
      "test MSE:  102.74757413182003\n",
      "Epoch 65 / 500...\n",
      "train loss:  -5.233536322914425\n",
      "test loss:  -4.954194105381451\n",
      "train MSE:  104.00528558551176\n",
      "test MSE:  102.68945013654587\n",
      "Epoch 66 / 500...\n",
      "train loss:  -5.2844947013039105\n",
      "test loss:  -5.002823544802373\n",
      "train MSE:  103.9406013189164\n",
      "test MSE:  102.62933121574248\n",
      "Epoch 67 / 500...\n",
      "train loss:  -5.336980784591243\n",
      "test loss:  -5.05112500894977\n",
      "train MSE:  103.87691766908051\n",
      "test MSE:  102.58238267707385\n",
      "Epoch 68 / 500...\n",
      "train loss:  -5.389053332416725\n",
      "test loss:  -5.099027410809968\n",
      "train MSE:  103.81260432076668\n",
      "test MSE:  102.52853905599349\n",
      "Epoch 69 / 500...\n",
      "train loss:  -5.439344276627994\n",
      "test loss:  -5.146819809039327\n",
      "train MSE:  103.75571365108667\n",
      "test MSE:  102.4675291975751\n",
      "Epoch 70 / 500...\n",
      "train loss:  -5.4908190159908745\n",
      "test loss:  -5.194275488059441\n",
      "train MSE:  103.69574996657423\n",
      "test MSE:  102.40442757258236\n",
      "Epoch 71 / 500...\n",
      "train loss:  -5.540283039408795\n",
      "test loss:  -5.241577784247269\n",
      "train MSE:  103.63066459067443\n",
      "test MSE:  102.35728535182352\n",
      "Epoch 72 / 500...\n",
      "train loss:  -5.5919265900147455\n",
      "test loss:  -5.288585880133688\n",
      "train MSE:  103.56618087873129\n",
      "test MSE:  102.3344308651779\n",
      "Epoch 73 / 500...\n",
      "train loss:  -5.641285043693232\n",
      "test loss:  -5.335149668923633\n",
      "train MSE:  103.5141466507373\n",
      "test MSE:  102.27229765063625\n",
      "Epoch 74 / 500...\n",
      "train loss:  -5.690159977777346\n",
      "test loss:  -5.381597401383907\n",
      "train MSE:  103.45259146801692\n",
      "test MSE:  102.23251534930564\n",
      "Epoch 75 / 500...\n",
      "train loss:  -5.7387769573355145\n",
      "test loss:  -5.427916682050014\n",
      "train MSE:  103.40007202715466\n",
      "test MSE:  102.18364935794386\n",
      "Epoch 76 / 500...\n",
      "train loss:  -5.7900899144594815\n",
      "test loss:  -5.473963220279315\n",
      "train MSE:  103.3348249136321\n",
      "test MSE:  102.12676492804711\n",
      "Epoch 77 / 500...\n",
      "train loss:  -5.83941178367898\n",
      "test loss:  -5.519696227462393\n",
      "train MSE:  103.27698107022863\n",
      "test MSE:  102.05909240572524\n",
      "Epoch 78 / 500...\n",
      "train loss:  -5.887307218547544\n",
      "test loss:  -5.565306286788784\n",
      "train MSE:  103.22416187563887\n",
      "test MSE:  101.99633940849489\n",
      "Epoch 79 / 500...\n",
      "train loss:  -5.937366794811989\n",
      "test loss:  -5.61062365840094\n",
      "train MSE:  103.1721750906809\n",
      "test MSE:  101.9458464875753\n",
      "Epoch 80 / 500...\n",
      "train loss:  -5.987232402888445\n",
      "test loss:  -5.655990044857547\n",
      "train MSE:  103.11865287655753\n",
      "test MSE:  101.89213844395809\n",
      "Epoch 81 / 500...\n",
      "train loss:  -6.033954086999319\n",
      "test loss:  -5.700744523647887\n",
      "train MSE:  103.07297170238552\n",
      "test MSE:  101.86256318878193\n",
      "Epoch 82 / 500...\n",
      "train loss:  -6.079469979786335\n",
      "test loss:  -5.745705532325186\n",
      "train MSE:  103.02409027978452\n",
      "test MSE:  101.81019155158722\n",
      "Epoch 83 / 500...\n",
      "train loss:  -6.127116289146135\n",
      "test loss:  -5.790449360177939\n",
      "train MSE:  102.98121471424197\n",
      "test MSE:  101.7773430690864\n",
      "Epoch 84 / 500...\n",
      "train loss:  -6.177192376174584\n",
      "test loss:  -5.8347980024357415\n",
      "train MSE:  102.9324346472179\n",
      "test MSE:  101.71393155274932\n",
      "Epoch 85 / 500...\n",
      "train loss:  -6.220637316474877\n",
      "test loss:  -5.8791871200221495\n",
      "train MSE:  102.88244909577887\n",
      "test MSE:  101.67145707036471\n",
      "Epoch 86 / 500...\n",
      "train loss:  -6.268734460115151\n",
      "test loss:  -5.9231750167634445\n",
      "train MSE:  102.83292719219173\n",
      "test MSE:  101.64275338459453\n",
      "Epoch 87 / 500...\n",
      "train loss:  -6.31634848614537\n",
      "test loss:  -5.96691998978054\n",
      "train MSE:  102.78723092250827\n",
      "test MSE:  101.59725746160252\n",
      "Epoch 88 / 500...\n",
      "train loss:  -6.363287879108725\n",
      "test loss:  -6.01075552147107\n",
      "train MSE:  102.73941912471912\n",
      "test MSE:  101.5670817919467\n",
      "Epoch 89 / 500...\n",
      "train loss:  -6.409493098245559\n",
      "test loss:  -6.054190988359733\n",
      "train MSE:  102.6962761529736\n",
      "test MSE:  101.52811295539502\n",
      "Epoch 90 / 500...\n",
      "train loss:  -6.4566964180443716\n",
      "test loss:  -6.0975738793518595\n",
      "train MSE:  102.65108881767641\n",
      "test MSE:  101.49363754333636\n",
      "Epoch 91 / 500...\n",
      "train loss:  -6.505024640444893\n",
      "test loss:  -6.140727014427226\n",
      "train MSE:  102.60122834333231\n",
      "test MSE:  101.4363076446321\n",
      "Epoch 92 / 500...\n",
      "train loss:  -6.548220809654811\n",
      "test loss:  -6.18367113195234\n",
      "train MSE:  102.55631919674627\n",
      "test MSE:  101.41229106544517\n",
      "Epoch 93 / 500...\n",
      "train loss:  -6.595858541948\n",
      "test loss:  -6.226554290598264\n",
      "train MSE:  102.50588724946843\n",
      "test MSE:  101.36210803586992\n",
      "Epoch 94 / 500...\n",
      "train loss:  -6.641613184010561\n",
      "test loss:  -6.269311422645369\n",
      "train MSE:  102.45839892692479\n",
      "test MSE:  101.32447560574073\n",
      "Epoch 95 / 500...\n",
      "train loss:  -6.682845595019061\n",
      "test loss:  -6.311923787016882\n",
      "train MSE:  102.41362995294513\n",
      "test MSE:  101.28291142918015\n",
      "Epoch 96 / 500...\n",
      "train loss:  -6.737966340918682\n",
      "test loss:  -6.354357394365096\n",
      "train MSE:  102.37431045860164\n",
      "test MSE:  101.24274176366912\n",
      "Epoch 97 / 500...\n",
      "train loss:  -6.775926312089281\n",
      "test loss:  -6.39677660460846\n",
      "train MSE:  102.32838991274794\n",
      "test MSE:  101.21113284654568\n",
      "Epoch 98 / 500...\n",
      "train loss:  -6.819621749252147\n",
      "test loss:  -6.438855157083032\n",
      "train MSE:  102.28261600900555\n",
      "test MSE:  101.18025992136508\n",
      "Epoch 99 / 500...\n",
      "train loss:  -6.868039446439127\n",
      "test loss:  -6.480833235991649\n",
      "train MSE:  102.23806699819285\n",
      "test MSE:  101.1688714144603\n",
      "Epoch 100 / 500...\n",
      "train loss:  -6.9103283117890175\n",
      "test loss:  -6.522889154330231\n",
      "train MSE:  102.19270067325981\n",
      "test MSE:  101.13679766032035\n",
      "Epoch 101 / 500...\n",
      "train loss:  -6.953579123385637\n",
      "test loss:  -6.5646873949354365\n",
      "train MSE:  102.15695450212847\n",
      "test MSE:  101.10834576126746\n",
      "Epoch 102 / 500...\n",
      "train loss:  -6.999470632080919\n",
      "test loss:  -6.60633121172091\n",
      "train MSE:  102.1170678478545\n",
      "test MSE:  101.08659526253608\n",
      "Epoch 103 / 500...\n",
      "train loss:  -7.045621812622309\n",
      "test loss:  -6.647831198468676\n",
      "train MSE:  102.08027146042603\n",
      "test MSE:  101.04152544014255\n",
      "Epoch 104 / 500...\n",
      "train loss:  -7.087193581031433\n",
      "test loss:  -6.689345565982333\n",
      "train MSE:  102.04056810879119\n",
      "test MSE:  101.01574635393465\n",
      "Epoch 105 / 500...\n",
      "train loss:  -7.132990002369946\n",
      "test loss:  -6.730586842153916\n",
      "train MSE:  101.99936167551552\n",
      "test MSE:  100.98189072456469\n",
      "Epoch 106 / 500...\n",
      "train loss:  -7.176928247040048\n",
      "test loss:  -6.771963978170875\n",
      "train MSE:  101.95629849433055\n",
      "test MSE:  100.9363173287366\n",
      "Epoch 107 / 500...\n",
      "train loss:  -7.217358312028734\n",
      "test loss:  -6.81303748166572\n",
      "train MSE:  101.91826211846094\n",
      "test MSE:  100.89107319246189\n",
      "Epoch 108 / 500...\n",
      "train loss:  -7.262233752432977\n",
      "test loss:  -6.853992611709827\n",
      "train MSE:  101.88991921744869\n",
      "test MSE:  100.8571013538378\n",
      "Epoch 109 / 500...\n",
      "train loss:  -7.307999147009\n",
      "test loss:  -6.894754238839727\n",
      "train MSE:  101.85061913161992\n",
      "test MSE:  100.8382754546687\n",
      "Epoch 110 / 500...\n",
      "train loss:  -7.34743173266437\n",
      "test loss:  -6.93576501789801\n",
      "train MSE:  101.81509076715389\n",
      "test MSE:  100.81545970444112\n",
      "Epoch 111 / 500...\n",
      "train loss:  -7.392315046491224\n",
      "test loss:  -6.976349695381395\n",
      "train MSE:  101.78209413560946\n",
      "test MSE:  100.77552245743836\n",
      "Epoch 112 / 500...\n",
      "train loss:  -7.434816384068331\n",
      "test loss:  -7.016975888913385\n",
      "train MSE:  101.74174383348357\n",
      "test MSE:  100.73473300923864\n",
      "Epoch 113 / 500...\n",
      "train loss:  -7.477955246100789\n",
      "test loss:  -7.057108828513206\n",
      "train MSE:  101.71412336051964\n",
      "test MSE:  100.71426081230268\n",
      "Epoch 114 / 500...\n",
      "train loss:  -7.519640167750193\n",
      "test loss:  -7.0975189636851495\n",
      "train MSE:  101.68155371629928\n",
      "test MSE:  100.6972361565726\n",
      "Epoch 115 / 500...\n",
      "train loss:  -7.562752796247306\n",
      "test loss:  -7.137487577127235\n",
      "train MSE:  101.64695696054034\n",
      "test MSE:  100.6822451627898\n",
      "Epoch 116 / 500...\n",
      "train loss:  -7.605998894558629\n",
      "test loss:  -7.177535633658697\n",
      "train MSE:  101.6147000090573\n",
      "test MSE:  100.66436830586275\n",
      "Epoch 117 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -7.650041234010527\n",
      "test loss:  -7.2172368397742925\n",
      "train MSE:  101.57971508300375\n",
      "test MSE:  100.65979740853363\n",
      "Epoch 118 / 500...\n",
      "train loss:  -7.690826185821224\n",
      "test loss:  -7.256756428690002\n",
      "train MSE:  101.54627421212213\n",
      "test MSE:  100.6437024268366\n",
      "Epoch 119 / 500...\n",
      "train loss:  -7.733858553271764\n",
      "test loss:  -7.296509172478547\n",
      "train MSE:  101.51136045062258\n",
      "test MSE:  100.61862059615348\n",
      "Epoch 120 / 500...\n",
      "train loss:  -7.77214203215409\n",
      "test loss:  -7.33647894638378\n",
      "train MSE:  101.47732869829248\n",
      "test MSE:  100.58910344560438\n",
      "Epoch 121 / 500...\n",
      "train loss:  -7.814978097298825\n",
      "test loss:  -7.376079246720503\n",
      "train MSE:  101.43692664012733\n",
      "test MSE:  100.57054870136159\n",
      "Epoch 122 / 500...\n",
      "train loss:  -7.859973546051393\n",
      "test loss:  -7.415520477955329\n",
      "train MSE:  101.4104793040538\n",
      "test MSE:  100.55375646413977\n",
      "Epoch 123 / 500...\n",
      "train loss:  -7.899358560751558\n",
      "test loss:  -7.454934138236503\n",
      "train MSE:  101.38233264469048\n",
      "test MSE:  100.54217427514477\n",
      "Epoch 124 / 500...\n",
      "train loss:  -7.940446216811763\n",
      "test loss:  -7.49431827305065\n",
      "train MSE:  101.34778548846919\n",
      "test MSE:  100.49448974453333\n",
      "Epoch 125 / 500...\n",
      "train loss:  -7.9825668766846105\n",
      "test loss:  -7.533480757533501\n",
      "train MSE:  101.3144049996334\n",
      "test MSE:  100.45933644515891\n",
      "Epoch 126 / 500...\n",
      "train loss:  -8.026236066233038\n",
      "test loss:  -7.572383137830747\n",
      "train MSE:  101.28924725153867\n",
      "test MSE:  100.44697952780307\n",
      "Epoch 127 / 500...\n",
      "train loss:  -8.065766030161384\n",
      "test loss:  -7.6110672854514645\n",
      "train MSE:  101.25471087782556\n",
      "test MSE:  100.43038097267146\n",
      "Epoch 128 / 500...\n",
      "train loss:  -8.103278556547398\n",
      "test loss:  -7.649841916962418\n",
      "train MSE:  101.22511073635904\n",
      "test MSE:  100.4230985260793\n",
      "Epoch 129 / 500...\n",
      "train loss:  -8.152480796959225\n",
      "test loss:  -7.688410460287959\n",
      "train MSE:  101.19684762590735\n",
      "test MSE:  100.38780964924173\n",
      "Epoch 130 / 500...\n",
      "train loss:  -8.187840137152454\n",
      "test loss:  -7.727407736634775\n",
      "train MSE:  101.16744588259142\n",
      "test MSE:  100.36249540005036\n",
      "Epoch 131 / 500...\n",
      "train loss:  -8.232679600163713\n",
      "test loss:  -7.765892265662243\n",
      "train MSE:  101.14739688687423\n",
      "test MSE:  100.33061532800062\n",
      "Epoch 132 / 500...\n",
      "train loss:  -8.273218768403309\n",
      "test loss:  -7.804656719700489\n",
      "train MSE:  101.11661282600913\n",
      "test MSE:  100.3042745637311\n",
      "Epoch 133 / 500...\n",
      "train loss:  -8.314061318493872\n",
      "test loss:  -7.84283614533309\n",
      "train MSE:  101.09129118379\n",
      "test MSE:  100.28595223799655\n",
      "Epoch 134 / 500...\n",
      "train loss:  -8.354649280645752\n",
      "test loss:  -7.881167080632844\n",
      "train MSE:  101.07124003157118\n",
      "test MSE:  100.28033545738026\n",
      "Epoch 135 / 500...\n",
      "train loss:  -8.390765443623016\n",
      "test loss:  -7.919473590459026\n",
      "train MSE:  101.04403791992478\n",
      "test MSE:  100.28289206097112\n",
      "Epoch 136 / 500...\n",
      "train loss:  -8.435248225878656\n",
      "test loss:  -7.9573592068464345\n",
      "train MSE:  101.02275109226808\n",
      "test MSE:  100.25372353818443\n",
      "Epoch 137 / 500...\n",
      "train loss:  -8.473921603777008\n",
      "test loss:  -7.995790843058775\n",
      "train MSE:  101.0035000021565\n",
      "test MSE:  100.2361178361837\n",
      "Epoch 138 / 500...\n",
      "train loss:  -8.51337230677437\n",
      "test loss:  -8.033943399340624\n",
      "train MSE:  100.97906468211009\n",
      "test MSE:  100.20017044023939\n",
      "Epoch 139 / 500...\n",
      "train loss:  -8.555003755356033\n",
      "test loss:  -8.071862300246984\n",
      "train MSE:  100.95691956680196\n",
      "test MSE:  100.17363599387964\n",
      "Epoch 140 / 500...\n",
      "train loss:  -8.59407412221463\n",
      "test loss:  -8.109639980174443\n",
      "train MSE:  100.93383852978344\n",
      "test MSE:  100.14570703647034\n",
      "Epoch 141 / 500...\n",
      "train loss:  -8.637654052748257\n",
      "test loss:  -8.147485442631844\n",
      "train MSE:  100.90817184730243\n",
      "test MSE:  100.1317619259747\n",
      "Epoch 142 / 500...\n",
      "train loss:  -8.674327675545001\n",
      "test loss:  -8.184976889468345\n",
      "train MSE:  100.88827596320145\n",
      "test MSE:  100.11359454591233\n",
      "Epoch 143 / 500...\n",
      "train loss:  -8.719102634651637\n",
      "test loss:  -8.22268023929493\n",
      "train MSE:  100.86252302065498\n",
      "test MSE:  100.09077879568477\n",
      "Epoch 144 / 500...\n",
      "train loss:  -8.752622434982548\n",
      "test loss:  -8.260248526020556\n",
      "train MSE:  100.84218505371845\n",
      "test MSE:  100.08539443357672\n",
      "Epoch 145 / 500...\n",
      "train loss:  -8.79132605958368\n",
      "test loss:  -8.297881408387779\n",
      "train MSE:  100.81733999836106\n",
      "test MSE:  100.06525149619415\n",
      "Epoch 146 / 500...\n",
      "train loss:  -8.837085958174907\n",
      "test loss:  -8.335248737548884\n",
      "train MSE:  100.79282273125246\n",
      "test MSE:  100.05568360093743\n",
      "Epoch 147 / 500...\n",
      "train loss:  -8.873827758710865\n",
      "test loss:  -8.372514611024817\n",
      "train MSE:  100.77436523374321\n",
      "test MSE:  100.04921461912417\n",
      "Epoch 148 / 500...\n",
      "train loss:  -8.912811756126846\n",
      "test loss:  -8.409683945437509\n",
      "train MSE:  100.75440681109477\n",
      "test MSE:  100.02818074412659\n",
      "Epoch 149 / 500...\n",
      "train loss:  -8.949665169696003\n",
      "test loss:  -8.44693205869023\n",
      "train MSE:  100.74133625467422\n",
      "test MSE:  99.9972303461099\n",
      "Epoch 150 / 500...\n",
      "train loss:  -8.988574813673349\n",
      "test loss:  -8.4842610867961\n",
      "train MSE:  100.72000414048314\n",
      "test MSE:  99.96823613720439\n",
      "Epoch 151 / 500...\n",
      "train loss:  -9.031365649228363\n",
      "test loss:  -8.521452444680463\n",
      "train MSE:  100.6972789262347\n",
      "test MSE:  99.954504077008\n",
      "Epoch 152 / 500...\n",
      "train loss:  -9.069604203421497\n",
      "test loss:  -8.558402245438327\n",
      "train MSE:  100.68074502818547\n",
      "test MSE:  99.93908698262672\n",
      "Epoch 153 / 500...\n",
      "train loss:  -9.107314960845924\n",
      "test loss:  -8.59538280288976\n",
      "train MSE:  100.6602625756393\n",
      "test MSE:  99.91321105537371\n",
      "Epoch 154 / 500...\n",
      "train loss:  -9.15000646010389\n",
      "test loss:  -8.632122688770478\n",
      "train MSE:  100.64559836450915\n",
      "test MSE:  99.90557998101916\n",
      "Epoch 155 / 500...\n",
      "train loss:  -9.186371379283724\n",
      "test loss:  -8.668930162644893\n",
      "train MSE:  100.61860759002316\n",
      "test MSE:  99.8977745927834\n",
      "Epoch 156 / 500...\n",
      "train loss:  -9.226274672706353\n",
      "test loss:  -8.705669402735873\n",
      "train MSE:  100.60277024200262\n",
      "test MSE:  99.89165423873254\n",
      "Epoch 157 / 500...\n",
      "train loss:  -9.26229252102858\n",
      "test loss:  -8.742369660929532\n",
      "train MSE:  100.58836049806561\n",
      "test MSE:  99.87863880226996\n",
      "Epoch 158 / 500...\n",
      "train loss:  -9.302294138816011\n",
      "test loss:  -8.778894261123376\n",
      "train MSE:  100.56958383831413\n",
      "test MSE:  99.87114330537855\n",
      "Epoch 159 / 500...\n",
      "train loss:  -9.340300599657093\n",
      "test loss:  -8.815414616122704\n",
      "train MSE:  100.5501300370487\n",
      "test MSE:  99.86142046444965\n",
      "Epoch 160 / 500...\n",
      "train loss:  -9.377322187550801\n",
      "test loss:  -8.852073563834558\n",
      "train MSE:  100.53150864541506\n",
      "test MSE:  99.8440665491662\n",
      "Epoch 161 / 500...\n",
      "train loss:  -9.418190101917725\n",
      "test loss:  -8.888522727859423\n",
      "train MSE:  100.52124801062725\n",
      "test MSE:  99.83537022331545\n",
      "Epoch 162 / 500...\n",
      "train loss:  -9.456808871385805\n",
      "test loss:  -8.924814941405966\n",
      "train MSE:  100.50225570071208\n",
      "test MSE:  99.8170478975809\n",
      "Epoch 163 / 500...\n",
      "train loss:  -9.496879369923267\n",
      "test loss:  -8.961164504019997\n",
      "train MSE:  100.47318390214659\n",
      "test MSE:  99.82008870639731\n",
      "Epoch 164 / 500...\n",
      "train loss:  -9.53133626959971\n",
      "test loss:  -8.997393569073866\n",
      "train MSE:  100.45696485333633\n",
      "test MSE:  99.82018554744243\n",
      "Epoch 165 / 500...\n",
      "train loss:  -9.569495213542245\n",
      "test loss:  -9.033780817691339\n",
      "train MSE:  100.43257913282756\n",
      "test MSE:  99.83002459762545\n",
      "Epoch 166 / 500...\n",
      "train loss:  -9.608537034666476\n",
      "test loss:  -9.069888430127307\n",
      "train MSE:  100.41208805427483\n",
      "test MSE:  99.80025566035908\n",
      "Epoch 167 / 500...\n",
      "train loss:  -9.649362715001457\n",
      "test loss:  -9.106008131616557\n",
      "train MSE:  100.40097344483884\n",
      "test MSE:  99.80548507679495\n",
      "Epoch 168 / 500...\n",
      "train loss:  -9.682684973488131\n",
      "test loss:  -9.141943644985892\n",
      "train MSE:  100.39369956481796\n",
      "test MSE:  99.79469698436985\n",
      "Epoch 169 / 500...\n",
      "train loss:  -9.728021526380596\n",
      "test loss:  -9.177526501117924\n",
      "train MSE:  100.38844417032912\n",
      "test MSE:  99.79917104065387\n",
      "Epoch 170 / 500...\n",
      "train loss:  -9.766372384392309\n",
      "test loss:  -9.21326037575498\n",
      "train MSE:  100.37433418011965\n",
      "test MSE:  99.77120334682652\n",
      "Epoch 171 / 500...\n",
      "train loss:  -9.796814623946531\n",
      "test loss:  -9.249060708618446\n",
      "train MSE:  100.36924267975519\n",
      "test MSE:  99.75352017198969\n",
      "Epoch 172 / 500...\n",
      "train loss:  -9.836259319248873\n",
      "test loss:  -9.284607740800224\n",
      "train MSE:  100.35569122346963\n",
      "test MSE:  99.74422343165928\n",
      "Epoch 173 / 500...\n",
      "train loss:  -9.877480267434368\n",
      "test loss:  -9.320293768074336\n",
      "train MSE:  100.34470816063349\n",
      "test MSE:  99.70808235362476\n",
      "Epoch 174 / 500...\n",
      "train loss:  -9.911815030268665\n",
      "test loss:  -9.355931292055526\n",
      "train MSE:  100.33648541989244\n",
      "test MSE:  99.71673994305746\n",
      "Epoch 175 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -9.950600368230496\n",
      "test loss:  -9.391423215102025\n",
      "train MSE:  100.33268135100515\n",
      "test MSE:  99.7137378706591\n",
      "Epoch 176 / 500...\n",
      "train loss:  -9.99149329187845\n",
      "test loss:  -9.427302705271334\n",
      "train MSE:  100.31880641947407\n",
      "test MSE:  99.69894055896651\n",
      "Epoch 177 / 500...\n",
      "train loss:  -10.023970864222163\n",
      "test loss:  -9.46280253400568\n",
      "train MSE:  100.30561078595859\n",
      "test MSE:  99.69338188297728\n",
      "Epoch 178 / 500...\n",
      "train loss:  -10.06337716458234\n",
      "test loss:  -9.498046021151904\n",
      "train MSE:  100.29677991175595\n",
      "test MSE:  99.69863066762217\n",
      "Epoch 179 / 500...\n",
      "train loss:  -10.099523654865637\n",
      "test loss:  -9.533410136917647\n",
      "train MSE:  100.2887102826311\n",
      "test MSE:  99.70529333152564\n",
      "Epoch 180 / 500...\n",
      "train loss:  -10.134692543167015\n",
      "test loss:  -9.568785197362015\n",
      "train MSE:  100.27854884691857\n",
      "test MSE:  99.69524123104337\n",
      "Epoch 181 / 500...\n",
      "train loss:  -10.170847270698555\n",
      "test loss:  -9.604087928785736\n",
      "train MSE:  100.27505531426698\n",
      "test MSE:  99.70742383451802\n",
      "Epoch 182 / 500...\n",
      "train loss:  -10.209226732717475\n",
      "test loss:  -9.639363183606921\n",
      "train MSE:  100.27333011295755\n",
      "test MSE:  99.7223954600918\n",
      "Epoch 183 / 500...\n",
      "train loss:  -10.248570443607935\n",
      "test loss:  -9.674491596915711\n",
      "train MSE:  100.26664280138189\n",
      "test MSE:  99.70426681644749\n",
      "Epoch 184 / 500...\n",
      "train loss:  -10.285107694090106\n",
      "test loss:  -9.709489451546242\n",
      "train MSE:  100.25999646333732\n",
      "test MSE:  99.70957370571944\n",
      "Epoch 185 / 500...\n",
      "train loss:  -10.319271029135278\n",
      "test loss:  -9.744675066336127\n",
      "train MSE:  100.24769146499783\n",
      "test MSE:  99.70870213631345\n",
      "Epoch 186 / 500...\n",
      "train loss:  -10.363534286598586\n",
      "test loss:  -9.779447046959763\n",
      "train MSE:  100.23867944465769\n",
      "test MSE:  99.70436365749259\n",
      "Epoch 187 / 500...\n",
      "train loss:  -10.393053401899918\n",
      "test loss:  -9.814171688482233\n",
      "train MSE:  100.23029927929716\n",
      "test MSE:  99.69752667970793\n",
      "Epoch 188 / 500...\n",
      "train loss:  -10.43279910857532\n",
      "test loss:  -9.848903323346738\n",
      "train MSE:  100.21906606227114\n",
      "test MSE:  99.70473165346401\n",
      "Epoch 189 / 500...\n",
      "train loss:  -10.470875404178928\n",
      "test loss:  -9.883500311265802\n",
      "train MSE:  100.21456544335517\n",
      "test MSE:  99.71888981425887\n",
      "Epoch 190 / 500...\n",
      "train loss:  -10.50366093021314\n",
      "test loss:  -9.91837982779596\n",
      "train MSE:  100.2106513928844\n",
      "test MSE:  99.719006023513\n",
      "Epoch 191 / 500...\n",
      "train loss:  -10.543200942096398\n",
      "test loss:  -9.952882860347751\n",
      "train MSE:  100.20645052769595\n",
      "test MSE:  99.7107939028878\n",
      "Epoch 192 / 500...\n",
      "train loss:  -10.57808736742349\n",
      "test loss:  -9.987621097995055\n",
      "train MSE:  100.1999810227856\n",
      "test MSE:  99.7416474598594\n",
      "Epoch 193 / 500...\n",
      "train loss:  -10.618851073766187\n",
      "test loss:  -10.022114867584449\n",
      "train MSE:  100.19867849579698\n",
      "test MSE:  99.74565667912688\n",
      "Epoch 194 / 500...\n",
      "train loss:  -10.652486903791562\n",
      "test loss:  -10.056357094816647\n",
      "train MSE:  100.18690830986341\n",
      "test MSE:  99.73616625670624\n",
      "Epoch 195 / 500...\n",
      "train loss:  -10.68768289025225\n",
      "test loss:  -10.09081367901351\n",
      "train MSE:  100.18309777147121\n",
      "test MSE:  99.74027231701885\n",
      "Epoch 196 / 500...\n",
      "train loss:  -10.724695300896672\n",
      "test loss:  -10.12498370345363\n",
      "train MSE:  100.18198501662663\n",
      "test MSE:  99.73111115415158\n",
      "Epoch 197 / 500...\n",
      "train loss:  -10.76304848835038\n",
      "test loss:  -10.159346991498326\n",
      "train MSE:  100.17406418611472\n",
      "test MSE:  99.73529468730027\n",
      "Epoch 198 / 500...\n",
      "train loss:  -10.797793459452416\n",
      "test loss:  -10.19353625452511\n",
      "train MSE:  100.1736415117939\n",
      "test MSE:  99.71732098932812\n",
      "Epoch 199 / 500...\n",
      "train loss:  -10.834329985268807\n",
      "test loss:  -10.22791127558785\n",
      "train MSE:  100.16638488378612\n",
      "test MSE:  99.72508764114582\n",
      "Epoch 200 / 500...\n",
      "train loss:  -10.869680844567982\n",
      "test loss:  -10.2621442262721\n",
      "train MSE:  100.16363318769758\n",
      "test MSE:  99.719896961128\n",
      "Epoch 201 / 500...\n",
      "train loss:  -10.904324643620809\n",
      "test loss:  -10.296244217722629\n",
      "train MSE:  100.16262178842993\n",
      "test MSE:  99.73517847804614\n",
      "Epoch 202 / 500...\n",
      "train loss:  -10.941809592159064\n",
      "test loss:  -10.330481776873466\n",
      "train MSE:  100.15544495098271\n",
      "test MSE:  99.73804497298134\n",
      "Epoch 203 / 500...\n",
      "train loss:  -10.977976565716792\n",
      "test loss:  -10.364774247495433\n",
      "train MSE:  100.1570558577054\n",
      "test MSE:  99.74342933508939\n",
      "Epoch 204 / 500...\n",
      "train loss:  -11.011510016394816\n",
      "test loss:  -10.398596682551124\n",
      "train MSE:  100.16075210151085\n",
      "test MSE:  99.749336638841\n",
      "Epoch 205 / 500...\n",
      "train loss:  -11.045217811471208\n",
      "test loss:  -10.432690615595202\n",
      "train MSE:  100.15172498565926\n",
      "test MSE:  99.76932463055141\n",
      "Epoch 206 / 500...\n",
      "train loss:  -11.083661003636301\n",
      "test loss:  -10.46668745979913\n",
      "train MSE:  100.14912855768857\n",
      "test MSE:  99.78154597044411\n",
      "Epoch 207 / 500...\n",
      "train loss:  -11.12099634577481\n",
      "test loss:  -10.500044161399249\n",
      "train MSE:  100.15548161151054\n",
      "test MSE:  99.79554918556681\n",
      "Epoch 208 / 500...\n",
      "train loss:  -11.155371833629959\n",
      "test loss:  -10.533859143905397\n",
      "train MSE:  100.15500718115045\n",
      "test MSE:  99.78691096434312\n",
      "Epoch 209 / 500...\n",
      "train loss:  -11.190773565139287\n",
      "test loss:  -10.567751287818218\n",
      "train MSE:  100.14829614805677\n",
      "test MSE:  99.80054618349442\n",
      "Epoch 210 / 500...\n",
      "train loss:  -11.227993198982034\n",
      "test loss:  -10.601464546600083\n",
      "train MSE:  100.14225578697214\n",
      "test MSE:  99.80709263814376\n",
      "Epoch 211 / 500...\n",
      "train loss:  -11.262256029897063\n",
      "test loss:  -10.634984816898566\n",
      "train MSE:  100.14682757044213\n",
      "test MSE:  99.82198679088145\n",
      "Epoch 212 / 500...\n",
      "train loss:  -11.295833999155953\n",
      "test loss:  -10.668596091443694\n",
      "train MSE:  100.1488072389447\n",
      "test MSE:  99.82192868625438\n",
      "Epoch 213 / 500...\n",
      "train loss:  -11.337989402551049\n",
      "test loss:  -10.702250611903159\n",
      "train MSE:  100.14923422626877\n",
      "test MSE:  99.84365981677674\n",
      "Epoch 214 / 500...\n",
      "train loss:  -11.372775201871319\n",
      "test loss:  -10.735733716288843\n",
      "train MSE:  100.15593232035263\n",
      "test MSE:  99.84284635199784\n",
      "Epoch 215 / 500...\n",
      "train loss:  -11.404643486555422\n",
      "test loss:  -10.769378428669437\n",
      "train MSE:  100.15882203254593\n",
      "test MSE:  99.84007669810772\n",
      "Epoch 216 / 500...\n",
      "train loss:  -11.450985383991096\n",
      "test loss:  -10.80304190793484\n",
      "train MSE:  100.15855462634296\n",
      "test MSE:  99.84892796963065\n",
      "Epoch 217 / 500...\n",
      "train loss:  -11.474139289968262\n",
      "test loss:  -10.836640351327876\n",
      "train MSE:  100.1651750863679\n",
      "test MSE:  99.85165888710272\n",
      "Epoch 218 / 500...\n",
      "train loss:  -11.511779692091698\n",
      "test loss:  -10.869864376241713\n",
      "train MSE:  100.17316708143382\n",
      "test MSE:  99.86198214251128\n",
      "Epoch 219 / 500...\n",
      "train loss:  -11.542912063883174\n",
      "test loss:  -10.903341726030177\n",
      "train MSE:  100.17693880279656\n",
      "test MSE:  99.89407526486026\n",
      "Epoch 220 / 500...\n",
      "train loss:  -11.578648758652923\n",
      "test loss:  -10.936620809602354\n",
      "train MSE:  100.17515106293966\n",
      "test MSE:  99.9280083670663\n",
      "Epoch 221 / 500...\n",
      "train loss:  -11.620713212881128\n",
      "test loss:  -10.969767437537259\n",
      "train MSE:  100.18589044109085\n",
      "test MSE:  99.92849257229184\n",
      "Epoch 222 / 500...\n",
      "train loss:  -11.651606368356621\n",
      "test loss:  -11.003145953287108\n",
      "train MSE:  100.19257559616487\n",
      "test MSE:  99.92260463674924\n",
      "Epoch 223 / 500...\n",
      "train loss:  -11.687034279964248\n",
      "test loss:  -11.03638072618737\n",
      "train MSE:  100.20232082706151\n",
      "test MSE:  99.92146191241696\n",
      "Epoch 224 / 500...\n",
      "train loss:  -11.721554893303795\n",
      "test loss:  -11.069699873761087\n",
      "train MSE:  100.2108562605399\n",
      "test MSE:  99.93025507931281\n",
      "Epoch 225 / 500...\n",
      "train loss:  -11.755709257972974\n",
      "test loss:  -11.102601515411491\n",
      "train MSE:  100.21788645587581\n",
      "test MSE:  99.93004202901358\n",
      "Epoch 226 / 500...\n",
      "train loss:  -11.787491653983649\n",
      "test loss:  -11.135636831613546\n",
      "train MSE:  100.22613938763979\n",
      "test MSE:  99.95839708702137\n",
      "Epoch 227 / 500...\n",
      "train loss:  -11.826210028970484\n",
      "test loss:  -11.168318800654756\n",
      "train MSE:  100.23731653562325\n",
      "test MSE:  99.96653173481049\n",
      "Epoch 228 / 500...\n",
      "train loss:  -11.87018832100937\n",
      "test loss:  -11.201122793827832\n",
      "train MSE:  100.24537106923664\n",
      "test MSE:  99.96201894210843\n",
      "Epoch 229 / 500...\n",
      "train loss:  -11.892628770593726\n",
      "test loss:  -11.23411935609083\n",
      "train MSE:  100.24771087351256\n",
      "test MSE:  99.96122484553854\n",
      "Epoch 230 / 500...\n",
      "train loss:  -11.9299452332738\n",
      "test loss:  -11.267040490683543\n",
      "train MSE:  100.24939941429416\n",
      "test MSE:  99.98105789157677\n",
      "Epoch 231 / 500...\n",
      "train loss:  -11.966661841478636\n",
      "test loss:  -11.299670302986833\n",
      "train MSE:  100.25810089839858\n",
      "test MSE:  100.00112335612326\n",
      "Epoch 232 / 500...\n",
      "train loss:  -12.00507340450927\n",
      "test loss:  -11.332238857137737\n",
      "train MSE:  100.26600663339903\n",
      "test MSE:  100.00011620925413\n",
      "Epoch 233 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -12.030256239398547\n",
      "test loss:  -11.36509757811041\n",
      "train MSE:  100.28149678465606\n",
      "test MSE:  100.02264143634638\n",
      "Epoch 234 / 500...\n",
      "train loss:  -12.068798891848775\n",
      "test loss:  -11.397403698914827\n",
      "train MSE:  100.29024139879323\n",
      "test MSE:  100.01603687706998\n",
      "Epoch 235 / 500...\n",
      "train loss:  -12.105498580740075\n",
      "test loss:  -11.430079010591209\n",
      "train MSE:  100.29775249399414\n",
      "test MSE:  100.02651507815072\n",
      "Epoch 236 / 500...\n",
      "train loss:  -12.136797687428151\n",
      "test loss:  -11.462845525310302\n",
      "train MSE:  100.30458213467784\n",
      "test MSE:  100.03232554085724\n",
      "Epoch 237 / 500...\n",
      "train loss:  -12.173821113902603\n",
      "test loss:  -11.494962977111445\n",
      "train MSE:  100.31691948054188\n",
      "test MSE:  100.0293428366679\n",
      "Epoch 238 / 500...\n",
      "train loss:  -12.207084658017848\n",
      "test loss:  -11.52760419836066\n",
      "train MSE:  100.3168267509715\n",
      "test MSE:  100.06073870349209\n",
      "Epoch 239 / 500...\n",
      "train loss:  -12.239068047387276\n",
      "test loss:  -11.560227460943718\n",
      "train MSE:  100.33136804150834\n",
      "test MSE:  100.0771629447425\n",
      "Epoch 240 / 500...\n",
      "train loss:  -12.279645712054494\n",
      "test loss:  -11.592668176585672\n",
      "train MSE:  100.3438649684935\n",
      "test MSE:  100.08994596269683\n",
      "Epoch 241 / 500...\n",
      "train loss:  -12.314247489316124\n",
      "test loss:  -11.625071572294644\n",
      "train MSE:  100.34866534113699\n",
      "test MSE:  100.088319033139\n",
      "Epoch 242 / 500...\n",
      "train loss:  -12.343105045305354\n",
      "test loss:  -11.657333136095625\n",
      "train MSE:  100.35120570006512\n",
      "test MSE:  100.11183203889136\n",
      "Epoch 243 / 500...\n",
      "train loss:  -12.38114231576556\n",
      "test loss:  -11.68983676005814\n",
      "train MSE:  100.35551654683707\n",
      "test MSE:  100.12757839282601\n",
      "Epoch 244 / 500...\n",
      "train loss:  -12.408327197415032\n",
      "test loss:  -11.722279900104432\n",
      "train MSE:  100.36504828407165\n",
      "test MSE:  100.15114950320545\n",
      "Epoch 245 / 500...\n",
      "train loss:  -12.44759233853895\n",
      "test loss:  -11.754666128123773\n",
      "train MSE:  100.37809727547584\n",
      "test MSE:  100.16759311266487\n",
      "Epoch 246 / 500...\n",
      "train loss:  -12.475266210460411\n",
      "test loss:  -11.78671814427361\n",
      "train MSE:  100.38893585270232\n",
      "test MSE:  100.19769130948461\n",
      "Epoch 247 / 500...\n",
      "train loss:  -12.511644267386556\n",
      "test loss:  -11.819190416881\n",
      "train MSE:  100.3999922365941\n",
      "test MSE:  100.23146946601848\n",
      "Epoch 248 / 500...\n",
      "train loss:  -12.543175766517484\n",
      "test loss:  -11.85122104693465\n",
      "train MSE:  100.41850149014263\n",
      "test MSE:  100.25585404117682\n",
      "Epoch 249 / 500...\n",
      "train loss:  -12.577359825031627\n",
      "test loss:  -11.883374969324349\n",
      "train MSE:  100.428337294108\n",
      "test MSE:  100.26015378357964\n",
      "Epoch 250 / 500...\n",
      "train loss:  -12.612832257645925\n",
      "test loss:  -11.915280271413177\n",
      "train MSE:  100.43981419581897\n",
      "test MSE:  100.27477678139103\n",
      "Epoch 251 / 500...\n",
      "train loss:  -12.646572405794215\n",
      "test loss:  -11.947243955725664\n",
      "train MSE:  100.45559331829533\n",
      "test MSE:  100.31858767019814\n",
      "Epoch 252 / 500...\n",
      "train loss:  -12.685458261030421\n",
      "test loss:  -11.979309956417676\n",
      "train MSE:  100.46564692892602\n",
      "test MSE:  100.31252542077434\n",
      "Epoch 253 / 500...\n",
      "train loss:  -12.714088260287552\n",
      "test loss:  -12.011145125607602\n",
      "train MSE:  100.4751139711115\n",
      "test MSE:  100.32914334411497\n",
      "Epoch 254 / 500...\n",
      "train loss:  -12.747409763155007\n",
      "test loss:  -12.042600523318738\n",
      "train MSE:  100.49324583687358\n",
      "test MSE:  100.3572659836145\n",
      "Epoch 255 / 500...\n",
      "train loss:  -12.781508835749097\n",
      "test loss:  -12.074671630867872\n",
      "train MSE:  100.50313555337989\n",
      "test MSE:  100.36964226917937\n",
      "Epoch 256 / 500...\n",
      "train loss:  -12.818752968444175\n",
      "test loss:  -12.106552949585495\n",
      "train MSE:  100.51598614663348\n",
      "test MSE:  100.39160581821\n",
      "Epoch 257 / 500...\n",
      "train loss:  -12.848974085873909\n",
      "test loss:  -12.13805243995453\n",
      "train MSE:  100.52751696088538\n",
      "test MSE:  100.41050919021518\n",
      "Epoch 258 / 500...\n",
      "train loss:  -12.880436390158694\n",
      "test loss:  -12.170132380294616\n",
      "train MSE:  100.54237310066118\n",
      "test MSE:  100.42151033293952\n",
      "Epoch 259 / 500...\n",
      "train loss:  -12.915056094395082\n",
      "test loss:  -12.201855154862914\n",
      "train MSE:  100.55344889306771\n",
      "test MSE:  100.45598574499816\n",
      "Epoch 260 / 500...\n",
      "train loss:  -12.950468478205963\n",
      "test loss:  -12.233654109276678\n",
      "train MSE:  100.56772493390322\n",
      "test MSE:  100.48689740659681\n",
      "Epoch 261 / 500...\n",
      "train loss:  -12.987000748417897\n",
      "test loss:  -12.265037672155378\n",
      "train MSE:  100.58244305757428\n",
      "test MSE:  100.50212081888787\n",
      "Epoch 262 / 500...\n",
      "train loss:  -13.018909222810851\n",
      "test loss:  -12.296549765008407\n",
      "train MSE:  100.5974113354352\n",
      "test MSE:  100.5393465166276\n",
      "Epoch 263 / 500...\n",
      "train loss:  -13.047612570015117\n",
      "test loss:  -12.328013976440209\n",
      "train MSE:  100.61608448310812\n",
      "test MSE:  100.57182700315701\n",
      "Epoch 264 / 500...\n",
      "train loss:  -13.082537067270668\n",
      "test loss:  -12.359583851017655\n",
      "train MSE:  100.63279090128829\n",
      "test MSE:  100.60934322403207\n",
      "Epoch 265 / 500...\n",
      "train loss:  -13.119323555723959\n",
      "test loss:  -12.390842357315405\n",
      "train MSE:  100.6491479662033\n",
      "test MSE:  100.63932521159768\n",
      "Epoch 266 / 500...\n",
      "train loss:  -13.147971361683412\n",
      "test loss:  -12.42259996671077\n",
      "train MSE:  100.66215167107312\n",
      "test MSE:  100.65751195986907\n",
      "Epoch 267 / 500...\n",
      "train loss:  -13.18342156373725\n",
      "test loss:  -12.454286993118453\n",
      "train MSE:  100.68437442044018\n",
      "test MSE:  100.68921771803761\n",
      "Epoch 268 / 500...\n",
      "train loss:  -13.216500942175628\n",
      "test loss:  -12.485704831327975\n",
      "train MSE:  100.70459593628831\n",
      "test MSE:  100.70461544420988\n",
      "Epoch 269 / 500...\n",
      "train loss:  -13.267095148190396\n",
      "test loss:  -12.517313294813544\n",
      "train MSE:  100.7237650793377\n",
      "test MSE:  100.7524549204935\n",
      "Epoch 270 / 500...\n",
      "train loss:  -13.281276577918062\n",
      "test loss:  -12.549039922125889\n",
      "train MSE:  100.73916465752598\n",
      "test MSE:  100.7768007592338\n",
      "Epoch 271 / 500...\n",
      "train loss:  -13.3167053718042\n",
      "test loss:  -12.58025212124696\n",
      "train MSE:  100.75653527821028\n",
      "test MSE:  100.78993240495052\n",
      "Epoch 272 / 500...\n",
      "train loss:  -13.348740938409009\n",
      "test loss:  -12.611212145194617\n",
      "train MSE:  100.77779191484406\n",
      "test MSE:  100.80594991381147\n",
      "Epoch 273 / 500...\n",
      "train loss:  -13.37935494497905\n",
      "test loss:  -12.642124062706221\n",
      "train MSE:  100.80873555683029\n",
      "test MSE:  100.84149057736632\n",
      "Epoch 274 / 500...\n",
      "train loss:  -13.412128651819225\n",
      "test loss:  -12.673299199281827\n",
      "train MSE:  100.82801468146315\n",
      "test MSE:  100.8579922914528\n",
      "Epoch 275 / 500...\n",
      "train loss:  -13.447411991868561\n",
      "test loss:  -12.704328178213531\n",
      "train MSE:  100.84396201106716\n",
      "test MSE:  100.87185992911236\n",
      "Epoch 276 / 500...\n",
      "train loss:  -13.482859482470227\n",
      "test loss:  -12.735376988881805\n",
      "train MSE:  100.86667644280742\n",
      "test MSE:  100.89825879800895\n",
      "Epoch 277 / 500...\n",
      "train loss:  -13.510083865507738\n",
      "test loss:  -12.76655656515888\n",
      "train MSE:  100.8881379470967\n",
      "test MSE:  100.91834363076447\n",
      "Epoch 278 / 500...\n",
      "train loss:  -13.548900016184954\n",
      "test loss:  -12.797495748629236\n",
      "train MSE:  100.91645497008932\n",
      "test MSE:  100.94503302279638\n",
      "Epoch 279 / 500...\n",
      "train loss:  -13.585768679885106\n",
      "test loss:  -12.828580174609877\n",
      "train MSE:  100.93930094842942\n",
      "test MSE:  100.9796633805272\n",
      "Epoch 280 / 500...\n",
      "train loss:  -13.608929894121605\n",
      "test loss:  -12.859875583143655\n",
      "train MSE:  100.96433146292757\n",
      "test MSE:  101.00176250702097\n",
      "Epoch 281 / 500...\n",
      "train loss:  -13.657340382340871\n",
      "test loss:  -12.891127659666019\n",
      "train MSE:  100.98948058501577\n",
      "test MSE:  101.03155081249636\n",
      "Epoch 282 / 500...\n",
      "train loss:  -13.678289896808963\n",
      "test loss:  -12.92175487321778\n",
      "train MSE:  101.01430407535679\n",
      "test MSE:  101.04454688074993\n",
      "Epoch 283 / 500...\n",
      "train loss:  -13.709122482514017\n",
      "test loss:  -12.952575895199773\n",
      "train MSE:  101.03627451403236\n",
      "test MSE:  101.08121090042803\n",
      "Epoch 284 / 500...\n",
      "train loss:  -13.744224118389317\n",
      "test loss:  -12.98350635949372\n",
      "train MSE:  101.0611519169143\n",
      "test MSE:  101.10611841722996\n",
      "Epoch 285 / 500...\n",
      "train loss:  -13.774159768361162\n",
      "test loss:  -13.014528103288777\n",
      "train MSE:  101.08433862251302\n",
      "test MSE:  101.12260076310744\n",
      "Epoch 286 / 500...\n",
      "train loss:  -13.807701931957983\n",
      "test loss:  -13.045354927756751\n",
      "train MSE:  101.1022009255705\n",
      "test MSE:  101.14681102438458\n",
      "Epoch 287 / 500...\n",
      "train loss:  -13.836774685867018\n",
      "test loss:  -13.076197225205926\n",
      "train MSE:  101.12191782003562\n",
      "test MSE:  101.15818016308032\n",
      "Epoch 288 / 500...\n",
      "train loss:  -13.870021516853845\n",
      "test loss:  -13.10700449049009\n",
      "train MSE:  101.14081955688205\n",
      "test MSE:  101.16997540237455\n",
      "Epoch 289 / 500...\n",
      "train loss:  -13.901731318811617\n",
      "test loss:  -13.137786141581337\n",
      "train MSE:  101.16828476172813\n",
      "test MSE:  101.20355987681819\n",
      "Epoch 290 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -13.935141913925422\n",
      "test loss:  -13.168283441273356\n",
      "train MSE:  101.19191139366075\n",
      "test MSE:  101.23815149813097\n",
      "Epoch 291 / 500...\n",
      "train loss:  -13.967205388231488\n",
      "test loss:  -13.199114566245736\n",
      "train MSE:  101.22310518983684\n",
      "test MSE:  101.2718134454107\n",
      "Epoch 292 / 500...\n",
      "train loss:  -13.998052356042926\n",
      "test loss:  -13.230000018019293\n",
      "train MSE:  101.24939078828761\n",
      "test MSE:  101.30770273672793\n",
      "Epoch 293 / 500...\n",
      "train loss:  -14.041677138821967\n",
      "test loss:  -13.260742162137953\n",
      "train MSE:  101.26695549411922\n",
      "test MSE:  101.32933702620518\n",
      "Epoch 294 / 500...\n",
      "train loss:  -14.06716362397506\n",
      "test loss:  -13.29147622656478\n",
      "train MSE:  101.29781287603997\n",
      "test MSE:  101.35285003195754\n",
      "Epoch 295 / 500...\n",
      "train loss:  -14.09188040153146\n",
      "test loss:  -13.322374388058192\n",
      "train MSE:  101.32344721099643\n",
      "test MSE:  101.38689934341771\n",
      "Epoch 296 / 500...\n",
      "train loss:  -14.130157818530046\n",
      "test loss:  -13.352959083145732\n",
      "train MSE:  101.34925622258547\n",
      "test MSE:  101.4180240553156\n",
      "Epoch 297 / 500...\n",
      "train loss:  -14.160604515972688\n",
      "test loss:  -13.383516423512495\n",
      "train MSE:  101.37254428376112\n",
      "test MSE:  101.4698533826577\n",
      "Epoch 298 / 500...\n",
      "train loss:  -14.196166012991823\n",
      "test loss:  -13.413707305558365\n",
      "train MSE:  101.40457480257227\n",
      "test MSE:  101.5061494063644\n",
      "Epoch 299 / 500...\n",
      "train loss:  -14.22419644970685\n",
      "test loss:  -13.44450229371204\n",
      "train MSE:  101.43017894650582\n",
      "test MSE:  101.53316805794968\n",
      "Epoch 300 / 500...\n",
      "train loss:  -14.265028331642245\n",
      "test loss:  -13.47479872897818\n",
      "train MSE:  101.46230219488737\n",
      "test MSE:  101.55282679010672\n",
      "Epoch 301 / 500...\n",
      "train loss:  -14.295957777762407\n",
      "test loss:  -13.50499752804066\n",
      "train MSE:  101.49286629258552\n",
      "test MSE:  101.59843892235286\n",
      "Epoch 302 / 500...\n",
      "train loss:  -14.323335923785466\n",
      "test loss:  -13.535436771038803\n",
      "train MSE:  101.51956162634727\n",
      "test MSE:  101.64321822161105\n",
      "Epoch 303 / 500...\n",
      "train loss:  -14.353651972340732\n",
      "test loss:  -13.565552260239924\n",
      "train MSE:  101.5464186977318\n",
      "test MSE:  101.66647944064613\n",
      "Epoch 304 / 500...\n",
      "train loss:  -14.385253803132779\n",
      "test loss:  -13.595953845890527\n",
      "train MSE:  101.57460633062621\n",
      "test MSE:  101.70523522689857\n",
      "Epoch 305 / 500...\n",
      "train loss:  -14.419562874878011\n",
      "test loss:  -13.625808488090502\n",
      "train MSE:  101.60313253427759\n",
      "test MSE:  101.7402335806008\n",
      "Epoch 306 / 500...\n",
      "train loss:  -14.447079790395192\n",
      "test loss:  -13.656414990079304\n",
      "train MSE:  101.63212238578089\n",
      "test MSE:  101.75919505723306\n",
      "Epoch 307 / 500...\n",
      "train loss:  -14.488070103398284\n",
      "test loss:  -13.687003722808386\n",
      "train MSE:  101.66142061701825\n",
      "test MSE:  101.78063566462009\n",
      "Epoch 308 / 500...\n",
      "train loss:  -14.520810193818654\n",
      "test loss:  -13.71781227705617\n",
      "train MSE:  101.69073394376706\n",
      "test MSE:  101.80304468245821\n",
      "Epoch 309 / 500...\n",
      "train loss:  -14.548652739803803\n",
      "test loss:  -13.747678146992179\n",
      "train MSE:  101.71357776560552\n",
      "test MSE:  101.83757819914392\n",
      "Epoch 310 / 500...\n",
      "train loss:  -14.5771127176514\n",
      "test loss:  -13.777827836807631\n",
      "train MSE:  101.7386859141626\n",
      "test MSE:  101.87685692703995\n",
      "Epoch 311 / 500...\n",
      "train loss:  -14.614149810540251\n",
      "test loss:  -13.807470932730926\n",
      "train MSE:  101.76626325709381\n",
      "test MSE:  101.91615502314501\n",
      "Epoch 312 / 500...\n",
      "train loss:  -14.64112686255152\n",
      "test loss:  -13.837698495698756\n",
      "train MSE:  101.79779778052851\n",
      "test MSE:  101.93796362650346\n",
      "Epoch 313 / 500...\n",
      "train loss:  -14.673724895166579\n",
      "test loss:  -13.868114470332982\n",
      "train MSE:  101.82427099462168\n",
      "test MSE:  101.97598342081308\n",
      "Epoch 314 / 500...\n",
      "train loss:  -14.703406967163541\n",
      "test loss:  -13.898379518518896\n",
      "train MSE:  101.85265702566669\n",
      "test MSE:  102.01255059944607\n",
      "Epoch 315 / 500...\n",
      "train loss:  -14.738939500051446\n",
      "test loss:  -13.927985384255\n",
      "train MSE:  101.88619925212524\n",
      "test MSE:  102.061552168271\n",
      "Epoch 316 / 500...\n",
      "train loss:  -14.76753183277564\n",
      "test loss:  -13.9581932522871\n",
      "train MSE:  101.92131788128027\n",
      "test MSE:  102.09941701690845\n",
      "Epoch 317 / 500...\n",
      "train loss:  -14.796365839028827\n",
      "test loss:  -13.987908952286107\n",
      "train MSE:  101.95294297778372\n",
      "test MSE:  102.13011562820786\n",
      "Epoch 318 / 500...\n",
      "train loss:  -14.832175446013746\n",
      "test loss:  -14.017677741051797\n",
      "train MSE:  101.98219807898835\n",
      "test MSE:  102.16350642056129\n",
      "Epoch 319 / 500...\n",
      "train loss:  -14.864522805450818\n",
      "test loss:  -14.04768938169723\n",
      "train MSE:  102.01342853569226\n",
      "test MSE:  102.19246189304874\n",
      "Epoch 320 / 500...\n",
      "train loss:  -14.897024351959411\n",
      "test loss:  -14.077501070531069\n",
      "train MSE:  102.04603484044044\n",
      "test MSE:  102.23851949410238\n",
      "Epoch 321 / 500...\n",
      "train loss:  -14.928771576765115\n",
      "test loss:  -14.106985490783055\n",
      "train MSE:  102.07696554341685\n",
      "test MSE:  102.26983788809049\n",
      "Epoch 322 / 500...\n",
      "train loss:  -14.956685599878911\n",
      "test loss:  -14.136823642005643\n",
      "train MSE:  102.11054227390159\n",
      "test MSE:  102.29025198039938\n",
      "Epoch 323 / 500...\n",
      "train loss:  -14.989092767613217\n",
      "test loss:  -14.167121393724464\n",
      "train MSE:  102.14152688941891\n",
      "test MSE:  102.33696810055974\n",
      "Epoch 324 / 500...\n",
      "train loss:  -15.019368127964324\n",
      "test loss:  -14.197075323956263\n",
      "train MSE:  102.17393695251815\n",
      "test MSE:  102.37326412426643\n",
      "Epoch 325 / 500...\n",
      "train loss:  -15.051537837560558\n",
      "test loss:  -14.226600653571706\n",
      "train MSE:  102.2110438761823\n",
      "test MSE:  102.41246537932638\n",
      "Epoch 326 / 500...\n",
      "train loss:  -15.079420934505633\n",
      "test loss:  -14.256515702077731\n",
      "train MSE:  102.2443488874608\n",
      "test MSE:  102.45222831244794\n",
      "Epoch 327 / 500...\n",
      "train loss:  -15.119412682453254\n",
      "test loss:  -14.28649951013275\n",
      "train MSE:  102.27659936943893\n",
      "test MSE:  102.47924696403324\n",
      "Epoch 328 / 500...\n",
      "train loss:  -15.141438080547942\n",
      "test loss:  -14.316283922625264\n",
      "train MSE:  102.31553500649107\n",
      "test MSE:  102.51800275028567\n",
      "Epoch 329 / 500...\n",
      "train loss:  -15.1812086564543\n",
      "test loss:  -14.345805331062467\n",
      "train MSE:  102.34931013512639\n",
      "test MSE:  102.55422130115628\n",
      "Epoch 330 / 500...\n",
      "train loss:  -15.205383319563207\n",
      "test loss:  -14.375673103814798\n",
      "train MSE:  102.3820760209957\n",
      "test MSE:  102.57428676570278\n",
      "Epoch 331 / 500...\n",
      "train loss:  -15.238765929694972\n",
      "test loss:  -14.404845700560807\n",
      "train MSE:  102.4226743208098\n",
      "test MSE:  102.61333307509055\n",
      "Epoch 332 / 500...\n",
      "train loss:  -15.270334597106048\n",
      "test loss:  -14.434694252185357\n",
      "train MSE:  102.45449781546384\n",
      "test MSE:  102.65598187135636\n",
      "Epoch 333 / 500...\n",
      "train loss:  -15.30442674318858\n",
      "test loss:  -14.464298828981292\n",
      "train MSE:  102.4918980233506\n",
      "test MSE:  102.69065096550521\n",
      "Epoch 334 / 500...\n",
      "train loss:  -15.343123370111227\n",
      "test loss:  -14.493727026262825\n",
      "train MSE:  102.5273832577839\n",
      "test MSE:  102.74627646181557\n",
      "Epoch 335 / 500...\n",
      "train loss:  -15.364601524232317\n",
      "test loss:  -14.5232024893045\n",
      "train MSE:  102.56586387299068\n",
      "test MSE:  102.77658770893456\n",
      "Epoch 336 / 500...\n",
      "train loss:  -15.391618663624978\n",
      "test loss:  -14.552714495097243\n",
      "train MSE:  102.60496124766559\n",
      "test MSE:  102.82022428386047\n",
      "Epoch 337 / 500...\n",
      "train loss:  -15.434705636827942\n",
      "test loss:  -14.582222497147395\n",
      "train MSE:  102.64430015052382\n",
      "test MSE:  102.87920048033159\n",
      "Epoch 338 / 500...\n",
      "train loss:  -15.448748776647015\n",
      "test loss:  -14.611860105381787\n",
      "train MSE:  102.68192247807916\n",
      "test MSE:  102.9155158722473\n",
      "Epoch 339 / 500...\n",
      "train loss:  -15.485206337730427\n",
      "test loss:  -14.641391027040106\n",
      "train MSE:  102.71442527074878\n",
      "test MSE:  102.94731847146095\n",
      "Epoch 340 / 500...\n",
      "train loss:  -15.519155057159695\n",
      "test loss:  -14.670764196048562\n",
      "train MSE:  102.74649891959268\n",
      "test MSE:  102.9985667525324\n",
      "Epoch 341 / 500...\n",
      "train loss:  -15.553557038110519\n",
      "test loss:  -14.699892408271786\n",
      "train MSE:  102.78058027146042\n",
      "test MSE:  103.03838779028104\n",
      "Epoch 342 / 500...\n",
      "train loss:  -15.578517254507915\n",
      "test loss:  -14.729344173342666\n",
      "train MSE:  102.81550481546816\n",
      "test MSE:  103.0899265944878\n",
      "Epoch 343 / 500...\n",
      "train loss:  -15.613882909318106\n",
      "test loss:  -14.758822507066792\n",
      "train MSE:  102.85071833069522\n",
      "test MSE:  103.14702407468381\n",
      "Epoch 344 / 500...\n",
      "train loss:  -15.640662642727031\n",
      "test loss:  -14.787845747091888\n",
      "train MSE:  102.89222451769841\n",
      "test MSE:  103.19521217872983\n",
      "Epoch 345 / 500...\n",
      "train loss:  -15.673063622887824\n",
      "test loss:  -14.81708147641845\n",
      "train MSE:  102.92956650004098\n",
      "test MSE:  103.24068873351281\n",
      "Epoch 346 / 500...\n",
      "train loss:  -15.702813493598224\n",
      "test loss:  -14.846640069007737\n",
      "train MSE:  102.965142307543\n",
      "test MSE:  103.2743119443745\n",
      "Epoch 347 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -15.747580270650218\n",
      "test loss:  -14.876051639906715\n",
      "train MSE:  103.00968700535243\n",
      "test MSE:  103.30830315120761\n",
      "Epoch 348 / 500...\n",
      "train loss:  -15.770279185093138\n",
      "test loss:  -14.905180049872508\n",
      "train MSE:  103.04895258715501\n",
      "test MSE:  103.34242993550386\n",
      "Epoch 349 / 500...\n",
      "train loss:  -15.797381845008987\n",
      "test loss:  -14.934044671142637\n",
      "train MSE:  103.0885179226851\n",
      "test MSE:  103.39294222463249\n",
      "Epoch 350 / 500...\n",
      "train loss:  -15.827249664610218\n",
      "test loss:  -14.963192989654095\n",
      "train MSE:  103.12955830533475\n",
      "test MSE:  103.45542406693653\n",
      "Epoch 351 / 500...\n",
      "train loss:  -15.856927278500871\n",
      "test loss:  -14.992674462199117\n",
      "train MSE:  103.16952475016929\n",
      "test MSE:  103.50039704828494\n",
      "Epoch 352 / 500...\n",
      "train loss:  -15.902986539578686\n",
      "test loss:  -15.022053823197501\n",
      "train MSE:  103.21317018679618\n",
      "test MSE:  103.53535666556914\n",
      "Epoch 353 / 500...\n",
      "train loss:  -15.916254800248828\n",
      "test loss:  -15.051064167439144\n",
      "train MSE:  103.2597376831409\n",
      "test MSE:  103.59601789622513\n",
      "Epoch 354 / 500...\n",
      "train loss:  -15.950900497858655\n",
      "test loss:  -15.080105506906209\n",
      "train MSE:  103.30443117956327\n",
      "test MSE:  103.63508357382193\n",
      "Epoch 355 / 500...\n",
      "train loss:  -15.98872057579859\n",
      "test loss:  -15.10919388002884\n",
      "train MSE:  103.34515886947558\n",
      "test MSE:  103.68178032577327\n",
      "Epoch 356 / 500...\n",
      "train loss:  -16.009043689730067\n",
      "test loss:  -15.138548384156783\n",
      "train MSE:  103.38282001406039\n",
      "test MSE:  103.71888981425887\n",
      "Epoch 357 / 500...\n",
      "train loss:  -16.042791409331496\n",
      "test loss:  -15.167413882005153\n",
      "train MSE:  103.42525996627232\n",
      "test MSE:  103.77195870697837\n",
      "Epoch 358 / 500...\n",
      "train loss:  -16.07862512874744\n",
      "test loss:  -15.196602827830501\n",
      "train MSE:  103.47092388843123\n",
      "test MSE:  103.82311014700471\n",
      "Epoch 359 / 500...\n",
      "train loss:  -16.107337713420993\n",
      "test loss:  -15.225128515601337\n",
      "train MSE:  103.50652557395291\n",
      "test MSE:  103.88303538571789\n",
      "Epoch 360 / 500...\n",
      "train loss:  -16.13344801784007\n",
      "test loss:  -15.254124910146636\n",
      "train MSE:  103.54743009699945\n",
      "test MSE:  103.91520598090294\n",
      "Epoch 361 / 500...\n",
      "train loss:  -16.167257502284226\n",
      "test loss:  -15.282911740786252\n",
      "train MSE:  103.59454534475992\n",
      "test MSE:  103.94960392012551\n",
      "Epoch 362 / 500...\n",
      "train loss:  -16.19160119590226\n",
      "test loss:  -15.311741317151812\n",
      "train MSE:  103.6354239897868\n",
      "test MSE:  104.0019561891112\n",
      "Epoch 363 / 500...\n",
      "train loss:  -16.22552670777914\n",
      "test loss:  -15.339874077145152\n",
      "train MSE:  103.68321206605796\n",
      "test MSE:  104.05736863512232\n",
      "Epoch 364 / 500...\n",
      "train loss:  -16.25420956985522\n",
      "test loss:  -15.368509697768086\n",
      "train MSE:  103.7269372932454\n",
      "test MSE:  104.08576242954814\n",
      "Epoch 365 / 500...\n",
      "train loss:  -16.28286989233999\n",
      "test loss:  -15.397916357141582\n",
      "train MSE:  103.7730627067546\n",
      "test MSE:  104.13600356375046\n",
      "Epoch 366 / 500...\n",
      "train loss:  -16.31621868324908\n",
      "test loss:  -15.42651279953951\n",
      "train MSE:  103.81828238957634\n",
      "test MSE:  104.17816815479073\n",
      "Epoch 367 / 500...\n",
      "train loss:  -16.34649229836838\n",
      "test loss:  -15.454748129874641\n",
      "train MSE:  103.86060589069987\n",
      "test MSE:  104.23542058065891\n",
      "Epoch 368 / 500...\n",
      "train loss:  -16.379553433488773\n",
      "test loss:  -15.483169479700715\n",
      "train MSE:  103.90629784738006\n",
      "test MSE:  104.27036082973407\n",
      "Epoch 369 / 500...\n",
      "train loss:  -16.405526986354968\n",
      "test loss:  -15.511751962092562\n",
      "train MSE:  103.94594081696908\n",
      "test MSE:  104.30491371462881\n",
      "Epoch 370 / 500...\n",
      "train loss:  -16.43507973373156\n",
      "test loss:  -15.540402050633102\n",
      "train MSE:  103.98871287043306\n",
      "test MSE:  104.3520171989696\n",
      "Epoch 371 / 500...\n",
      "train loss:  -16.468829289987653\n",
      "test loss:  -15.56922676132324\n",
      "train MSE:  104.03401018731373\n",
      "test MSE:  104.42205264279212\n",
      "Epoch 372 / 500...\n",
      "train loss:  -16.492856138367106\n",
      "test loss:  -15.598463711798454\n",
      "train MSE:  104.07458692211148\n",
      "test MSE:  104.46632836861575\n",
      "Epoch 373 / 500...\n",
      "train loss:  -16.53123529969453\n",
      "test loss:  -15.626889289658475\n",
      "train MSE:  104.11531245552216\n",
      "test MSE:  104.5215083961186\n",
      "Epoch 374 / 500...\n",
      "train loss:  -16.570767607884562\n",
      "test loss:  -15.656029016065292\n",
      "train MSE:  104.15869048594608\n",
      "test MSE:  104.59067227053514\n",
      "Epoch 375 / 500...\n",
      "train loss:  -16.585000235945852\n",
      "test loss:  -15.685021985454435\n",
      "train MSE:  104.20428324355098\n",
      "test MSE:  104.63415389978888\n",
      "Epoch 376 / 500...\n",
      "train loss:  -16.627558043350824\n",
      "test loss:  -15.713373911212003\n",
      "train MSE:  104.25175647058316\n",
      "test MSE:  104.70180705390173\n",
      "Epoch 377 / 500...\n",
      "train loss:  -16.645854683165712\n",
      "test loss:  -15.74206482619805\n",
      "train MSE:  104.29236123990218\n",
      "test MSE:  104.74118262284287\n",
      "Epoch 378 / 500...\n",
      "train loss:  -16.683127807116467\n",
      "test loss:  -15.771121079221738\n",
      "train MSE:  104.32806212449916\n",
      "test MSE:  104.78584571284694\n",
      "Epoch 379 / 500...\n",
      "train loss:  -16.71158100574364\n",
      "test loss:  -15.799516963102933\n",
      "train MSE:  104.37896203263219\n",
      "test MSE:  104.83039259359687\n",
      "Epoch 380 / 500...\n",
      "train loss:  -16.743834073434794\n",
      "test loss:  -15.82756841762648\n",
      "train MSE:  104.42757604903022\n",
      "test MSE:  104.9029459045922\n",
      "Epoch 381 / 500...\n",
      "train loss:  -16.768258195549112\n",
      "test loss:  -15.8562362562181\n",
      "train MSE:  104.47906899511337\n",
      "test MSE:  104.96230946524375\n",
      "Epoch 382 / 500...\n",
      "train loss:  -16.79933512428063\n",
      "test loss:  -15.884333140459661\n",
      "train MSE:  104.52981794813182\n",
      "test MSE:  105.02312564157192\n",
      "Epoch 383 / 500...\n",
      "train loss:  -16.827288051166022\n",
      "test loss:  -15.912252367763475\n",
      "train MSE:  104.57738821773766\n",
      "test MSE:  105.06168774573415\n",
      "Epoch 384 / 500...\n",
      "train loss:  -16.85626088436817\n",
      "test loss:  -15.940357946627868\n",
      "train MSE:  104.6219286025438\n",
      "test MSE:  105.11514400263408\n",
      "Epoch 385 / 500...\n",
      "train loss:  -16.88975280612313\n",
      "test loss:  -15.969215512136492\n",
      "train MSE:  104.66926596997287\n",
      "test MSE:  105.16007824756444\n",
      "Epoch 386 / 500...\n",
      "train loss:  -16.927909109605494\n",
      "test loss:  -15.997757252180818\n",
      "train MSE:  104.71809132353131\n",
      "test MSE:  105.21190757490655\n",
      "Epoch 387 / 500...\n",
      "train loss:  -16.94959798765983\n",
      "test loss:  -16.026180571212976\n",
      "train MSE:  104.77074878049832\n",
      "test MSE:  105.25105072533943\n",
      "Epoch 388 / 500...\n",
      "train loss:  -16.977549016489732\n",
      "test loss:  -16.05439478829965\n",
      "train MSE:  104.81839452766144\n",
      "test MSE:  105.31147953748717\n",
      "Epoch 389 / 500...\n",
      "train loss:  -17.00416294742096\n",
      "test loss:  -16.082217976476837\n",
      "train MSE:  104.86627317700135\n",
      "test MSE:  105.35902849063547\n",
      "Epoch 390 / 500...\n",
      "train loss:  -17.035425270524293\n",
      "test loss:  -16.110722163833053\n",
      "train MSE:  104.90816753429915\n",
      "test MSE:  105.40171602331932\n",
      "Epoch 391 / 500...\n",
      "train loss:  -17.066385255945704\n",
      "test loss:  -16.138826043895403\n",
      "train MSE:  104.95510163592215\n",
      "test MSE:  105.4829656601654\n",
      "Epoch 392 / 500...\n",
      "train loss:  -17.09062102988094\n",
      "test loss:  -16.167231841446863\n",
      "train MSE:  105.00471195607638\n",
      "test MSE:  105.53454320079022\n",
      "Epoch 393 / 500...\n",
      "train loss:  -17.125008851547932\n",
      "test loss:  -16.195361006167637\n",
      "train MSE:  105.06084569368188\n",
      "test MSE:  105.60798744940055\n",
      "Epoch 394 / 500...\n",
      "train loss:  -17.156235052242792\n",
      "test loss:  -16.223166491512792\n",
      "train MSE:  105.10802563649146\n",
      "test MSE:  105.6736844144022\n",
      "Epoch 395 / 500...\n",
      "train loss:  -17.188872261075176\n",
      "test loss:  -16.250977811344693\n",
      "train MSE:  105.15471174042621\n",
      "test MSE:  105.73763824059189\n",
      "Epoch 396 / 500...\n",
      "train loss:  -17.211537142900415\n",
      "test loss:  -16.279574189921533\n",
      "train MSE:  105.20599981885387\n",
      "test MSE:  105.79064902868431\n",
      "Epoch 397 / 500...\n",
      "train loss:  -17.245388714446907\n",
      "test loss:  -16.307799361138184\n",
      "train MSE:  105.2597204311278\n",
      "test MSE:  105.84555790126088\n",
      "Epoch 398 / 500...\n",
      "train loss:  -17.27282834786254\n",
      "test loss:  -16.33576654251052\n",
      "train MSE:  105.31333106181827\n",
      "test MSE:  105.89674807770525\n",
      "Epoch 399 / 500...\n",
      "train loss:  -17.30545216070525\n",
      "test loss:  -16.363758691777548\n",
      "train MSE:  105.37123313076594\n",
      "test MSE:  105.94452944936182\n",
      "Epoch 400 / 500...\n",
      "train loss:  -17.333787997502196\n",
      "test loss:  -16.39191996124057\n",
      "train MSE:  105.42510254165283\n",
      "test MSE:  105.99912843059403\n",
      "Epoch 401 / 500...\n",
      "train loss:  -17.361998255942606\n",
      "test loss:  -16.419555277959915\n",
      "train MSE:  105.47820208145538\n",
      "test MSE:  106.0670721078422\n",
      "Epoch 402 / 500...\n",
      "train loss:  -17.388447714407437\n",
      "test loss:  -16.44818043979411\n",
      "train MSE:  105.5345299042082\n",
      "test MSE:  106.12045089190603\n",
      "Epoch 403 / 500...\n",
      "train loss:  -17.42348911961043\n",
      "test loss:  -16.475792105387228\n",
      "train MSE:  105.5879313542399\n",
      "test MSE:  106.18118959539811\n",
      "Epoch 404 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -17.44746500370441\n",
      "test loss:  -16.504080699560166\n",
      "train MSE:  105.63874284580582\n",
      "test MSE:  106.24272239546009\n",
      "Epoch 405 / 500...\n",
      "train loss:  -17.47510472649746\n",
      "test loss:  -16.531986832300344\n",
      "train MSE:  105.69787196418483\n",
      "test MSE:  106.28140070887645\n",
      "Epoch 406 / 500...\n",
      "train loss:  -17.505390961397005\n",
      "test loss:  -16.56021154313132\n",
      "train MSE:  105.75283687790319\n",
      "test MSE:  106.34109352908136\n",
      "Epoch 407 / 500...\n",
      "train loss:  -17.54235328826514\n",
      "test loss:  -16.588264634550708\n",
      "train MSE:  105.81248571317666\n",
      "test MSE:  106.40076698107725\n",
      "Epoch 408 / 500...\n",
      "train loss:  -17.580718568551998\n",
      "test loss:  -16.615859322288884\n",
      "train MSE:  105.86648667066338\n",
      "test MSE:  106.4601305417288\n",
      "Epoch 409 / 500...\n",
      "train loss:  -17.598348125328005\n",
      "test loss:  -16.643971410030222\n",
      "train MSE:  105.91621128540437\n",
      "test MSE:  106.51538804206776\n",
      "Epoch 410 / 500...\n",
      "train loss:  -17.633265883217597\n",
      "test loss:  -16.671901862778704\n",
      "train MSE:  105.9675920934024\n",
      "test MSE:  106.56528054850767\n",
      "Epoch 411 / 500...\n",
      "train loss:  -17.657265205590864\n",
      "test loss:  -16.69999899094304\n",
      "train MSE:  106.02454314512825\n",
      "test MSE:  106.62071236272782\n",
      "Epoch 412 / 500...\n",
      "train loss:  -17.68633805411093\n",
      "test loss:  -16.72753410879207\n",
      "train MSE:  106.0791371405651\n",
      "test MSE:  106.67585365381264\n",
      "Epoch 413 / 500...\n",
      "train loss:  -17.715163434866138\n",
      "test loss:  -16.755386719875002\n",
      "train MSE:  106.13455923263045\n",
      "test MSE:  106.72665646607658\n",
      "Epoch 414 / 500...\n",
      "train loss:  -17.744532227770495\n",
      "test loss:  -16.78275247665586\n",
      "train MSE:  106.18462026162678\n",
      "test MSE:  106.7847417249327\n",
      "Epoch 415 / 500...\n",
      "train loss:  -17.770968460997768\n",
      "test loss:  -16.810773199567006\n",
      "train MSE:  106.23662861160112\n",
      "test MSE:  106.83329782495012\n",
      "Epoch 416 / 500...\n",
      "train loss:  -17.80885949019785\n",
      "test loss:  -16.838763427907406\n",
      "train MSE:  106.29078268070405\n",
      "test MSE:  106.90358505548991\n",
      "Epoch 417 / 500...\n",
      "train loss:  -17.830180957600493\n",
      "test loss:  -16.86688366249391\n",
      "train MSE:  106.3405051389434\n",
      "test MSE:  106.9586101373206\n",
      "Epoch 418 / 500...\n",
      "train loss:  -17.86783062073023\n",
      "test loss:  -16.89482093469604\n",
      "train MSE:  106.39592507450713\n",
      "test MSE:  107.03399120683311\n",
      "Epoch 419 / 500...\n",
      "train loss:  -17.894287608873242\n",
      "test loss:  -16.92225871650189\n",
      "train MSE:  106.45283730920352\n",
      "test MSE:  107.07588464294707\n",
      "Epoch 420 / 500...\n",
      "train loss:  -17.92583117086946\n",
      "test loss:  -16.950183359490136\n",
      "train MSE:  106.50667221606422\n",
      "test MSE:  107.15632081501424\n",
      "Epoch 421 / 500...\n",
      "train loss:  -17.948096899882554\n",
      "test loss:  -16.978899477912904\n",
      "train MSE:  106.55154254562079\n",
      "test MSE:  107.21551006178458\n",
      "Epoch 422 / 500...\n",
      "train loss:  -17.991574114352055\n",
      "test loss:  -17.006185475548314\n",
      "train MSE:  106.61110727733042\n",
      "test MSE:  107.27636497453081\n",
      "Epoch 423 / 500...\n",
      "train loss:  -18.01145422504399\n",
      "test loss:  -17.03345108403161\n",
      "train MSE:  106.66976843485425\n",
      "test MSE:  107.33445023338692\n",
      "Epoch 424 / 500...\n",
      "train loss:  -18.038893404854708\n",
      "test loss:  -17.0609902726126\n",
      "train MSE:  106.72841880986988\n",
      "test MSE:  107.41343378977746\n",
      "Epoch 425 / 500...\n",
      "train loss:  -18.063764912380165\n",
      "test loss:  -17.089030876614366\n",
      "train MSE:  106.78794041154677\n",
      "test MSE:  107.48015726985726\n",
      "Epoch 426 / 500...\n",
      "train loss:  -18.096519854467854\n",
      "test loss:  -17.116357069360735\n",
      "train MSE:  106.84414315720466\n",
      "test MSE:  107.55028955472487\n",
      "Epoch 427 / 500...\n",
      "train loss:  -18.137653351476775\n",
      "test loss:  -17.143581138638716\n",
      "train MSE:  106.90253043902061\n",
      "test MSE:  107.61637388390696\n",
      "Epoch 428 / 500...\n",
      "train loss:  -18.150694710421906\n",
      "test loss:  -17.17106082305025\n",
      "train MSE:  106.96120669205588\n",
      "test MSE:  107.67765489725166\n",
      "Epoch 429 / 500...\n",
      "train loss:  -18.185325863069597\n",
      "test loss:  -17.1980897430797\n",
      "train MSE:  107.02102373445702\n",
      "test MSE:  107.7257848966706\n",
      "Epoch 430 / 500...\n",
      "train loss:  -18.21896605300341\n",
      "test loss:  -17.225839750677032\n",
      "train MSE:  107.0772027585969\n",
      "test MSE:  107.77077724622804\n",
      "Epoch 431 / 500...\n",
      "train loss:  -18.24088162797199\n",
      "test loss:  -17.253756135443847\n",
      "train MSE:  107.13560944892757\n",
      "test MSE:  107.83631926555752\n",
      "Epoch 432 / 500...\n",
      "train loss:  -18.276861734757766\n",
      "test loss:  -17.280959522122707\n",
      "train MSE:  107.19059161465904\n",
      "test MSE:  107.89694175979547\n",
      "Epoch 433 / 500...\n",
      "train loss:  -18.303218865345393\n",
      "test loss:  -17.30846297813058\n",
      "train MSE:  107.25382886865611\n",
      "test MSE:  107.96536964226918\n",
      "Epoch 434 / 500...\n",
      "train loss:  -18.323537296479778\n",
      "test loss:  -17.336045215219663\n",
      "train MSE:  107.31286741396637\n",
      "test MSE:  108.01134977048672\n",
      "Epoch 435 / 500...\n",
      "train loss:  -18.355776241894933\n",
      "test loss:  -17.363147302478538\n",
      "train MSE:  107.3689731170506\n",
      "test MSE:  108.06910576978947\n",
      "Epoch 436 / 500...\n",
      "train loss:  -18.382939287264424\n",
      "test loss:  -17.39092953487611\n",
      "train MSE:  107.43520575182117\n",
      "test MSE:  108.12660998237493\n",
      "Epoch 437 / 500...\n",
      "train loss:  -18.4179337447967\n",
      "test loss:  -17.418366229222876\n",
      "train MSE:  107.49283394506097\n",
      "test MSE:  108.18148011853344\n",
      "Epoch 438 / 500...\n",
      "train loss:  -18.448123331204076\n",
      "test loss:  -17.445581394082957\n",
      "train MSE:  107.54753576557965\n",
      "test MSE:  108.24742887025236\n",
      "Epoch 439 / 500...\n",
      "train loss:  -18.472461287877703\n",
      "test loss:  -17.47258931821464\n",
      "train MSE:  107.61412853612356\n",
      "test MSE:  108.3190912436327\n",
      "Epoch 440 / 500...\n",
      "train loss:  -18.49782262243234\n",
      "test loss:  -17.499906215921143\n",
      "train MSE:  107.67193140599593\n",
      "test MSE:  108.37899711413685\n",
      "Epoch 441 / 500...\n",
      "train loss:  -18.534102752549234\n",
      "test loss:  -17.527151644358856\n",
      "train MSE:  107.73409903518117\n",
      "test MSE:  108.44023939106351\n",
      "Epoch 442 / 500...\n",
      "train loss:  -18.55727283165306\n",
      "test loss:  -17.5545287706235\n",
      "train MSE:  107.79777405901051\n",
      "test MSE:  108.51800275028567\n",
      "Epoch 443 / 500...\n",
      "train loss:  -18.58739102327118\n",
      "test loss:  -17.581630980817188\n",
      "train MSE:  107.85867150873167\n",
      "test MSE:  108.58914218202243\n",
      "Epoch 444 / 500...\n",
      "train loss:  -18.61222572304604\n",
      "test loss:  -17.60928608553361\n",
      "train MSE:  107.9237288501102\n",
      "test MSE:  108.64955162596115\n",
      "Epoch 445 / 500...\n",
      "train loss:  -18.660197524210638\n",
      "test loss:  -17.636659782904665\n",
      "train MSE:  107.98353295350151\n",
      "test MSE:  108.7180182448529\n",
      "Epoch 446 / 500...\n",
      "train loss:  -18.67563886788748\n",
      "test loss:  -17.664397691043188\n",
      "train MSE:  108.04917039382033\n",
      "test MSE:  108.77871821192694\n",
      "Epoch 447 / 500...\n",
      "train loss:  -18.69905242057279\n",
      "test loss:  -17.691613620757856\n",
      "train MSE:  108.11127332795645\n",
      "test MSE:  108.84391160349402\n",
      "Epoch 448 / 500...\n",
      "train loss:  -18.73939988323542\n",
      "test loss:  -17.719163382966002\n",
      "train MSE:  108.17823054727698\n",
      "test MSE:  108.90408862892448\n",
      "Epoch 449 / 500...\n",
      "train loss:  -18.75896656188692\n",
      "test loss:  -17.7461348902941\n",
      "train MSE:  108.23237383387173\n",
      "test MSE:  108.9734074490132\n",
      "Epoch 450 / 500...\n",
      "train loss:  -18.803159818789\n",
      "test loss:  -17.772701124414375\n",
      "train MSE:  108.295598148859\n",
      "test MSE:  109.03586992310821\n",
      "Epoch 451 / 500...\n",
      "train loss:  -18.81522064684822\n",
      "test loss:  -17.799784347378154\n",
      "train MSE:  108.35294384038437\n",
      "test MSE:  109.10902364858322\n",
      "Epoch 452 / 500...\n",
      "train loss:  -18.841773265165784\n",
      "test loss:  -17.827054967644578\n",
      "train MSE:  108.41429415544927\n",
      "test MSE:  109.16943309252193\n",
      "Epoch 453 / 500...\n",
      "train loss:  -18.87888111248033\n",
      "test loss:  -17.854098337725116\n",
      "train MSE:  108.47720362119755\n",
      "test MSE:  109.24866843562975\n",
      "Epoch 454 / 500...\n",
      "train loss:  -18.914770040725692\n",
      "test loss:  -17.88049571591406\n",
      "train MSE:  108.54746891402891\n",
      "test MSE:  109.31833588348086\n",
      "Epoch 455 / 500...\n",
      "train loss:  -18.931854901167554\n",
      "test loss:  -17.908125557081604\n",
      "train MSE:  108.60992767093511\n",
      "test MSE:  109.38316127907652\n",
      "Epoch 456 / 500...\n",
      "train loss:  -18.960254624020635\n",
      "test loss:  -17.93505652777089\n",
      "train MSE:  108.67084668567263\n",
      "test MSE:  109.43944529449362\n",
      "Epoch 457 / 500...\n",
      "train loss:  -18.990511701671924\n",
      "test loss:  -17.962180512517385\n",
      "train MSE:  108.7396822179188\n",
      "test MSE:  109.51532993744068\n",
      "Epoch 458 / 500...\n",
      "train loss:  -19.02541155807427\n",
      "test loss:  -17.989632745332134\n",
      "train MSE:  108.80890376395796\n",
      "test MSE:  109.60492727237512\n",
      "Epoch 459 / 500...\n",
      "train loss:  -19.053994271303733\n",
      "test loss:  -18.0169463347912\n",
      "train MSE:  108.8762836575993\n",
      "test MSE:  109.6686874164746\n",
      "Epoch 460 / 500...\n",
      "train loss:  -19.071755190920115\n",
      "test loss:  -18.04406395543865\n",
      "train MSE:  108.94059053640822\n",
      "test MSE:  109.72348007979703\n",
      "Epoch 461 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -19.110427832550947\n",
      "test loss:  -18.071323641496612\n",
      "train MSE:  109.0010372772873\n",
      "test MSE:  109.81021091979625\n",
      "Epoch 462 / 500...\n",
      "train loss:  -19.130452650429234\n",
      "test loss:  -18.098576170340813\n",
      "train MSE:  109.0648395347132\n",
      "test MSE:  109.87995584048343\n",
      "Epoch 463 / 500...\n",
      "train loss:  -19.159465921963303\n",
      "test loss:  -18.12560815476494\n",
      "train MSE:  109.13494955942672\n",
      "test MSE:  109.94555596443996\n",
      "Epoch 464 / 500...\n",
      "train loss:  -19.189496092786914\n",
      "test loss:  -18.152377826593945\n",
      "train MSE:  109.20165662455737\n",
      "test MSE:  110.00995525943716\n",
      "Epoch 465 / 500...\n",
      "train loss:  -19.215252519977145\n",
      "test loss:  -18.179295207151412\n",
      "train MSE:  109.26552789003567\n",
      "test MSE:  110.07753094071391\n",
      "Epoch 466 / 500...\n",
      "train loss:  -19.24183329175106\n",
      "test loss:  -18.20626872530831\n",
      "train MSE:  109.33585787791613\n",
      "test MSE:  110.16267358757335\n",
      "Epoch 467 / 500...\n",
      "train loss:  -19.27613419945554\n",
      "test loss:  -18.233039523080603\n",
      "train MSE:  109.3950495348426\n",
      "test MSE:  110.24897832697411\n",
      "Epoch 468 / 500...\n",
      "train loss:  -19.312396984233153\n",
      "test loss:  -18.25987237044145\n",
      "train MSE:  109.45680742871684\n",
      "test MSE:  110.31419108675021\n",
      "Epoch 469 / 500...\n",
      "train loss:  -19.332347258386307\n",
      "test loss:  -18.286758138876383\n",
      "train MSE:  109.52206532474759\n",
      "test MSE:  110.39578935135869\n",
      "Epoch 470 / 500...\n",
      "train loss:  -19.36332728366538\n",
      "test loss:  -18.31360490244638\n",
      "train MSE:  109.59503055762819\n",
      "test MSE:  110.48620015107203\n",
      "Epoch 471 / 500...\n",
      "train loss:  -19.39081502061858\n",
      "test loss:  -18.3403216932325\n",
      "train MSE:  109.66433189422791\n",
      "test MSE:  110.5729697274893\n",
      "Epoch 472 / 500...\n",
      "train loss:  -19.435483311906253\n",
      "test loss:  -18.367118210378177\n",
      "train MSE:  109.73971025244008\n",
      "test MSE:  110.65185644283473\n",
      "Epoch 473 / 500...\n",
      "train loss:  -19.442918560825408\n",
      "test loss:  -18.39370859955117\n",
      "train MSE:  109.80885200791867\n",
      "test MSE:  110.71646878813117\n",
      "Epoch 474 / 500...\n",
      "train loss:  -19.475362606738983\n",
      "test loss:  -18.42041244486128\n",
      "train MSE:  109.87171834363423\n",
      "test MSE:  110.78871220778214\n",
      "Epoch 475 / 500...\n",
      "train loss:  -19.495579195884524\n",
      "test loss:  -18.44718659098999\n",
      "train MSE:  109.93667864243909\n",
      "test MSE:  110.8393407061649\n",
      "Epoch 476 / 500...\n",
      "train loss:  -19.529031367627578\n",
      "test loss:  -18.474047032283043\n",
      "train MSE:  110.00837800885891\n",
      "test MSE:  110.90513451221166\n",
      "Epoch 477 / 500...\n",
      "train loss:  -19.560035096972644\n",
      "test loss:  -18.50080907370594\n",
      "train MSE:  110.07685340533173\n",
      "test MSE:  110.99850864790533\n",
      "Epoch 478 / 500...\n",
      "train loss:  -19.58605427664452\n",
      "test loss:  -18.52709550285989\n",
      "train MSE:  110.14623237599038\n",
      "test MSE:  111.07745346787783\n",
      "Epoch 479 / 500...\n",
      "train loss:  -19.61270280546355\n",
      "test loss:  -18.553840951929036\n",
      "train MSE:  110.21556821661628\n",
      "test MSE:  111.15438399411207\n",
      "Epoch 480 / 500...\n",
      "train loss:  -19.638198253222154\n",
      "test loss:  -18.58003460270491\n",
      "train MSE:  110.2877139788749\n",
      "test MSE:  111.21206252057873\n",
      "Epoch 481 / 500...\n",
      "train loss:  -19.668665572461506\n",
      "test loss:  -18.607249162718183\n",
      "train MSE:  110.36359911497173\n",
      "test MSE:  111.29079429025198\n",
      "Epoch 482 / 500...\n",
      "train loss:  -19.694703911441533\n",
      "test loss:  -18.63431949460701\n",
      "train MSE:  110.43089706155087\n",
      "test MSE:  111.34306908640158\n",
      "Epoch 483 / 500...\n",
      "train loss:  -19.732851411514403\n",
      "test loss:  -18.660920538991967\n",
      "train MSE:  110.49934442350242\n",
      "test MSE:  111.42300168503418\n",
      "Epoch 484 / 500...\n",
      "train loss:  -19.75525652342525\n",
      "test loss:  -18.68747005554874\n",
      "train MSE:  110.56811957370276\n",
      "test MSE:  111.50647866591777\n",
      "Epoch 485 / 500...\n",
      "train loss:  -19.779463081046746\n",
      "test loss:  -18.714514140922336\n",
      "train MSE:  110.64193015522498\n",
      "test MSE:  111.57326025062463\n",
      "Epoch 486 / 500...\n",
      "train loss:  -19.810350292659045\n",
      "test loss:  -18.741060475889526\n",
      "train MSE:  110.71561565965229\n",
      "test MSE:  111.65826732002091\n",
      "Epoch 487 / 500...\n",
      "train loss:  -19.837842790986883\n",
      "test loss:  -18.76755683048972\n",
      "train MSE:  110.79576204298338\n",
      "test MSE:  111.72735372160136\n",
      "Epoch 488 / 500...\n",
      "train loss:  -19.870905087822134\n",
      "test loss:  -18.794221300606257\n",
      "train MSE:  110.86309233708708\n",
      "test MSE:  111.79117197032791\n",
      "Epoch 489 / 500...\n",
      "train loss:  -19.89622887048397\n",
      "test loss:  -18.820862870823092\n",
      "train MSE:  110.93260285434556\n",
      "test MSE:  111.86219519281052\n",
      "Epoch 490 / 500...\n",
      "train loss:  -19.923019904610758\n",
      "test loss:  -18.847524342805478\n",
      "train MSE:  111.00430222076538\n",
      "test MSE:  111.94849993221126\n",
      "Epoch 491 / 500...\n",
      "train loss:  -19.95612062127355\n",
      "test loss:  -18.8748490371571\n",
      "train MSE:  111.07313559650991\n",
      "test MSE:  111.99947705835642\n",
      "Epoch 492 / 500...\n",
      "train loss:  -19.981158652744806\n",
      "test loss:  -18.9007185039253\n",
      "train MSE:  111.14901857610509\n",
      "test MSE:  112.0587825143809\n",
      "Epoch 493 / 500...\n",
      "train loss:  -20.00776608487096\n",
      "test loss:  -18.927178771197383\n",
      "train MSE:  111.21795977693147\n",
      "test MSE:  112.12323991400515\n",
      "Epoch 494 / 500...\n",
      "train loss:  -20.040781239046524\n",
      "test loss:  -18.953508746185168\n",
      "train MSE:  111.29119241601505\n",
      "test MSE:  112.18860761945344\n",
      "Epoch 495 / 500...\n",
      "train loss:  -20.06138309522559\n",
      "test loss:  -18.979580001871014\n",
      "train MSE:  111.36677995488598\n",
      "test MSE:  112.28994208905503\n",
      "Epoch 496 / 500...\n",
      "train loss:  -20.0884565031397\n",
      "test loss:  -19.00631892787305\n",
      "train MSE:  111.43736009695631\n",
      "test MSE:  112.36203056303384\n",
      "Epoch 497 / 500...\n",
      "train loss:  -20.116803212397134\n",
      "test loss:  -19.032615291784232\n",
      "train MSE:  111.50769655434169\n",
      "test MSE:  112.44362882764231\n",
      "Epoch 498 / 500...\n",
      "train loss:  -20.149242618283505\n",
      "test loss:  -19.059421509402824\n",
      "train MSE:  111.58681428639204\n",
      "test MSE:  112.5248978326974\n",
      "Epoch 499 / 500...\n",
      "train loss:  -20.171568235791685\n",
      "test loss:  -19.08573653707861\n",
      "train MSE:  111.6612416273824\n",
      "test MSE:  112.5862563188782\n",
      "Epoch 500 / 500...\n",
      "train loss:  -20.203303938803252\n",
      "test loss:  -19.11196417524375\n",
      "train MSE:  111.7303014357988\n",
      "test MSE:  112.6755631306773\n"
     ]
    }
   ],
   "source": [
    "# ==========  Poisson Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_mse_arr, test_mse_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the weights\n",
    "regr.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3],\n",
       "       [ -6],\n",
       "       [ -5],\n",
       "       [ -2],\n",
       "       [ -6],\n",
       "       [ -6],\n",
       "       [ -6],\n",
       "       [ -1],\n",
       "       [ -2],\n",
       "       [-22],\n",
       "       [  0],\n",
       "       [ -2],\n",
       "       [  0],\n",
       "       [ -8],\n",
       "       [ -2],\n",
       "       [ -2],\n",
       "       [ -3],\n",
       "       [ -3],\n",
       "       [ -4],\n",
       "       [ -2],\n",
       "       [ -3],\n",
       "       [ -3],\n",
       "       [ -9],\n",
       "       [ -3],\n",
       "       [ -1],\n",
       "       [ -3],\n",
       "       [ -4],\n",
       "       [ -1],\n",
       "       [  0],\n",
       "       [ -1],\n",
       "       [ -4],\n",
       "       [-20],\n",
       "       [ -5],\n",
       "       [ -3],\n",
       "       [-15],\n",
       "       [ -6],\n",
       "       [ -2],\n",
       "       [ -5],\n",
       "       [ -2],\n",
       "       [ -4],\n",
       "       [ -3],\n",
       "       [ -7],\n",
       "       [ -1],\n",
       "       [ -7],\n",
       "       [ -3],\n",
       "       [-13],\n",
       "       [-16],\n",
       "       [ -7],\n",
       "       [-13],\n",
       "       [ -8],\n",
       "       [ -6],\n",
       "       [ -6],\n",
       "       [ -3],\n",
       "       [ -5],\n",
       "       [ -8],\n",
       "       [ -3],\n",
       "       [ -8],\n",
       "       [-12],\n",
       "       [ -4],\n",
       "       [ -7]])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[2050:2110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.50407701],\n",
       "       [8.50407701],\n",
       "       [4.50407701],\n",
       "       ...,\n",
       "       [7.50407701],\n",
       "       [7.50407701],\n",
       "       [6.50407701]])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
