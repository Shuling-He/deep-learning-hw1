{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, feat_dims=0):\n",
    "        # alpha is weight decay hyperparameter\n",
    "        \n",
    "        self.learning_rate = 0.000001\n",
    "        self.epochs = 500\n",
    "        self.batch_size = 100\n",
    "        \n",
    "        self.feat_dims = feat_dims\n",
    "        self.output_classes = 1\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.001 * np.random.rand(self.feat_dims, self.output_classes)\n",
    "        self.alpha = 0.2  # regularization strength\n",
    "        \n",
    "        self.y_mean = None\n",
    "        \n",
    "    def normalize_feat(self, x, mean=None, std=None):\n",
    "        # normalize the feature data.  test data must pass mean and std\n",
    "        \n",
    "        # calc feature-wise mean\n",
    "        if mean is None:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            \n",
    "        # calc feature-wise std\n",
    "        if std is None:\n",
    "            std = np.std(x, axis=0)\n",
    "        \n",
    "        # sub the mean per column\n",
    "        x_norm = x - mean\n",
    "\n",
    "        # div by the standard dev.\n",
    "        x_norm = x_norm / std\n",
    "\n",
    "        return x_norm, mean, std\n",
    "        \n",
    "    def load_data(self, fname, bias=1):\n",
    "        \n",
    "        data = loadtxt(fname, delimiter=',')\n",
    "        \n",
    "        # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "        TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "        # process training data\n",
    "        x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "        \n",
    "        x_train, train_mean, train_std = self.normalize_feat(x_train)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_train = np.hstack((x_train, col_bias))\n",
    "        \n",
    "        # convert label vals to int and to vector\n",
    "        y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "        # -------------------\n",
    "        \n",
    "        # process test data\n",
    "        x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "        x_test, _, _ = self.normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "        # create a col vector of ones\n",
    "        col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "        # append bias with hstack\n",
    "        x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "        # convert label vals to int and to vector\n",
    "        y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "        y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def musicMSE(self, pred, gt):\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "        diff = pred - gt\n",
    "        mse = (np.square(diff)).mean()\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    def label_sub_mean(self, label):\n",
    "        \n",
    "        # find the mean\n",
    "        self.y_mean = np.mean(label)\n",
    "        \n",
    "        # sub mean\n",
    "        temp = label - self.y_mean\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def train_loss(self, x, yt_sm):\n",
    "        # calc the cost\n",
    "        # yt = true label, sub mean label\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.exp(np.dot(x, self.weights))\n",
    "        \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt_sm\n",
    "    \n",
    "        # calc the diff, and divide\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def test_loss(self, x, yt_sm):\n",
    "        # calc the cost at test time\n",
    "        # yt = true label, is regular label\n",
    "        # this function adds the y mean back\n",
    "        \n",
    "        n_samples = x.shape[0]  \n",
    "\n",
    "        # predict\n",
    "#         pred_y = np.exp(np.dot(x, self.weights)) + self.y_mean\n",
    "        pred_y = np.exp(np.dot(x, self.weights))\n",
    "    \n",
    "        # (x dot w)\n",
    "        x_dot_w = np.dot(x, self.weights)\n",
    "        \n",
    "        # need to add the mean back to label\n",
    "#         yt = yt_sm + self.y_mean\n",
    "        yt = yt_sm\n",
    "    \n",
    "        # calc y dot times x_dot_w\n",
    "        x_prod_y = x_dot_w * yt\n",
    "        \n",
    "    \n",
    "        # calc the diff, divide, and add the mean\n",
    "        loss = np.sum((pred_y - x_prod_y)) / n_samples\n",
    "    \n",
    "        return loss \n",
    "    \n",
    "    def gradient(self, x, yt_sm):\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "        pred_y = np.exp(np.dot(x, self.weights))\n",
    "        \n",
    "        x_trans_dot_pred_y = np.dot(x.T, pred_y)\n",
    "        \n",
    "        dW = x_trans_dot_pred_y - np.dot(x.T, yt_sm)\n",
    "        \n",
    "        # return the avg dW\n",
    "        return dW / n_samples\n",
    "\n",
    "    def calc_mse(self, x, y_sm):\n",
    "        # preprocesses (adds the y_mean back to both x and y, and calls musicMSE)\n",
    "        \n",
    "        # predict\n",
    "        pred_y = np.dot(x, self.weights)\n",
    "        \n",
    "        # add the y mean to the pred and convert to int to round\n",
    "        pred_y += self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        pred_y = pred_y.astype(int)\n",
    "        \n",
    "        # add the y mean back to the labels\n",
    "        y_labels = y_sm + self.y_mean\n",
    "        \n",
    "        # convert to int to round\n",
    "        y_labels = y_labels.astype(int)\n",
    "        \n",
    "        # calc the MSE\n",
    "        mse = self.musicMSE(pred_y, y_labels)\n",
    "        \n",
    "#         print('MSE: ', mse)\n",
    "        \n",
    "        return mse, pred_y\n",
    "\n",
    "    def train_phase(self, x_train, y_train_sm):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        # Randomize the data (using sklearn shuffle)\n",
    "        x_train, y_train_sm = shuffle(x_train, y_train_sm)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_sm = y_train_sm[i:i + self.batch_size]\n",
    "\n",
    "            # calc loss\n",
    "            loss = self.train_loss(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            dW = self.gradient(x_train_batch, y_train_batch_sm)\n",
    "            \n",
    "            self.weights -= dW * self.learning_rate  # update the weights\n",
    "            \n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_sm):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "        \n",
    "        # calc loss\n",
    "        loss = self.test_loss(x, y_sm)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def run_epochs(self, x_train, y_train_sm, x_test, y_test_sm):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Epoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss = self.train_phase(x_train, y_train_sm)\n",
    "            test_loss = self.test_phase(x_test, y_test_sm)\n",
    "            \n",
    "            train_mse, train_preds = self.calc_mse(x_train, y_train_sm)\n",
    "            test_mse, test_preds = self.calc_mse(x_test, y_test_sm)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "        \n",
    "#         return train_losses, test_losses\n",
    "\n",
    "        # return all the vals\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds\n",
    "\n",
    "#     def closed_form(self, x, yt):\n",
    "#         # for ridge regression only\n",
    "#         # yt is regular labels\n",
    "#         # returns the weights w that allow you to find the prediction\n",
    "\n",
    "#         xt = np.transpose(x)\n",
    "#         alpha_identity = self.alpha * np.identity(len(xt))\n",
    "\n",
    "\n",
    "#         theInverse = np.linalg.inv(np.dot(xt, x) + alpha_identity)\n",
    "#         w = np.dot(np.dot(theInverse, xt), yt)\n",
    "#         return w\n",
    "    \n",
    "    \n",
    "    def plot_graph(self, train_losses, test_losses, train_mse, test_mse):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs. Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse, label=\"Train MSE\")\n",
    "        plt.plot(test_mse, label=\"Test MSE\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Epochs vs MSE\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()\n",
    "        # save plot \n",
    "        plt.savefig('./regression_loss_acc')\n",
    "\n",
    "    def make_mesh_grid(self, x, y, h=0.02):\n",
    "        # make a mesh grid for the decision boundary\n",
    "        \n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "        x_x, y_y = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return x_x, y_y  # matrix of x-axis and y-axis\n",
    "\n",
    "    def plot_contours(self, plt, x_x, y_y, **params):\n",
    "        # plot contours    \n",
    "\n",
    "        array = np.array([x_x.ravel(), y_y.ravel()])\n",
    "        f = np.dot(array.T, self.weights)\n",
    "        prob = self.softmax(f)\n",
    "        Q = np.argmax(prob, axis=1) + 1\n",
    "        Q = Q.reshape(x_x.shape)\n",
    "        plt.contourf(x_x, y_y, Q, **params)  # takes in variable number of params\n",
    "\n",
    "    def plot_decision_boundary(self, x, y):\n",
    "        # plot decision boundary\n",
    "\n",
    "        markers = ('o', '.', 'x')\n",
    "        colors = ('yellow', 'grey', 'green')\n",
    "        cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "        x_x, y_y = self.make_mesh_grid(x, y)\n",
    "        self.plot_contours(plt, x_x, y_y, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        \n",
    "        # plot training points\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            xBasedOnLabel = x[np.where(y[:,0] == cl)]\n",
    "            plt.scatter(x=xBasedOnLabel[:, 0], y=xBasedOnLabel[:, 1], c=cmap(idx),\n",
    "                        cmap=plt.cm.coolwarm, marker=markers[idx], label=cl)\n",
    "        plt.xlim(x_x.min(), x_x.max())\n",
    "        plt.ylim(y_y.min(), y_y.max())\n",
    "        plt.xlabel(\"x1\")\n",
    "        plt.ylabel(\"x2\")\n",
    "        plt.title(\"Decision Boundary - Softmax Classifier\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_weights(self):\n",
    "        \n",
    "        plt.hist(self.weights, bins=12)\n",
    "        plt.xlabel('bins')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Lasso Regression Weights Histogram')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Regression() object to load data\n",
    "regr = Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "# x_train, y_train, x_test, y_test = regr.load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 500...\n",
      "train loss:  0.8976223768246411\n",
      "test loss:  6.795063738042287\n",
      "train MSE:  119.82701190820204\n",
      "test MSE:  3994103.795045612\n",
      "Epoch 2 / 500...\n",
      "train loss:  0.6853073459504554\n",
      "test loss:  14.13523119738507\n",
      "train MSE:  119.79489944232867\n",
      "test MSE:  3994111.8862505085\n",
      "Epoch 3 / 500...\n",
      "train loss:  0.47997861275048287\n",
      "test loss:  21.545770382577388\n",
      "train MSE:  119.59691965306202\n",
      "test MSE:  3994145.925025663\n",
      "Epoch 4 / 500...\n",
      "train loss:  0.28157627933221113\n",
      "test loss:  29.061121039235587\n",
      "train MSE:  119.25164648899968\n",
      "test MSE:  3994192.7284383415\n",
      "Epoch 5 / 500...\n",
      "train loss:  0.08853140194751014\n",
      "test loss:  36.72135900906519\n",
      "train MSE:  118.84128794903755\n",
      "test MSE:  3994218.2208556873\n",
      "Epoch 6 / 500...\n",
      "train loss:  -0.09667425029544614\n",
      "test loss:  44.539474838155805\n",
      "train MSE:  118.44401721750907\n",
      "test MSE:  3994241.1872131084\n",
      "Epoch 7 / 500...\n",
      "train loss:  -0.27696031139645283\n",
      "test loss:  52.55993045016579\n",
      "train MSE:  118.03767624009626\n",
      "test MSE:  3994265.2298231684\n",
      "Epoch 8 / 500...\n",
      "train loss:  -0.45057830397394283\n",
      "test loss:  60.806700181388535\n",
      "train MSE:  117.63058048711059\n",
      "test MSE:  3994284.805310763\n",
      "Epoch 9 / 500...\n",
      "train loss:  -0.6183153550753367\n",
      "test loss:  69.32413541869249\n",
      "train MSE:  117.22226846720177\n",
      "test MSE:  3994310.9883597065\n",
      "Epoch 10 / 500...\n",
      "train loss:  -0.7799228205079242\n",
      "test loss:  78.10854532703772\n",
      "train MSE:  116.85793614167353\n",
      "test MSE:  3994332.8677151324\n",
      "Epoch 11 / 500...\n",
      "train loss:  -0.93598619602618\n",
      "test loss:  87.20259579859074\n",
      "train MSE:  116.50365527027435\n",
      "test MSE:  3994352.559392613\n",
      "Epoch 12 / 500...\n",
      "train loss:  -1.0849737839051505\n",
      "test loss:  96.61343844596728\n",
      "train MSE:  116.16602043500951\n",
      "test MSE:  3994372.357227247\n",
      "Epoch 13 / 500...\n",
      "train loss:  -1.2291947629766897\n",
      "test loss:  106.37856022727799\n",
      "train MSE:  115.8472291110469\n",
      "test MSE:  3994390.7796866223\n",
      "Epoch 14 / 500...\n",
      "train loss:  -1.3678800126228707\n",
      "test loss:  116.48824538405526\n",
      "train MSE:  115.54024679004732\n",
      "test MSE:  3994408.892487072\n",
      "Epoch 15 / 500...\n",
      "train loss:  -1.4997084053381198\n",
      "test loss:  126.96422590135502\n",
      "train MSE:  115.24673829127437\n",
      "test MSE:  3994421.223973969\n",
      "Epoch 16 / 500...\n",
      "train loss:  -1.6270822695709823\n",
      "test loss:  137.81718833295616\n",
      "train MSE:  114.97235580551806\n",
      "test MSE:  3994441.9245414576\n",
      "Epoch 17 / 500...\n",
      "train loss:  -1.7482858221137207\n",
      "test loss:  149.02645025962588\n",
      "train MSE:  114.71328016837965\n",
      "test MSE:  3994455.939493715\n",
      "Epoch 18 / 500...\n",
      "train loss:  -1.8668405865236195\n",
      "test loss:  160.61707249483223\n",
      "train MSE:  114.47593128523185\n",
      "test MSE:  3994471.194863551\n",
      "Epoch 19 / 500...\n",
      "train loss:  -1.979599187600881\n",
      "test loss:  172.55229942329984\n",
      "train MSE:  114.24839448453142\n",
      "test MSE:  3994489.171253704\n",
      "Epoch 20 / 500...\n",
      "train loss:  -2.0877348001357285\n",
      "test loss:  184.8359954389038\n",
      "train MSE:  114.02498307146215\n",
      "test MSE:  3994515.3592609093\n",
      "Epoch 21 / 500...\n",
      "train loss:  -2.1929658127551277\n",
      "test loss:  197.4544490552619\n",
      "train MSE:  113.80911294461673\n",
      "test MSE:  3994535.96236757\n",
      "Epoch 22 / 500...\n",
      "train loss:  -2.292834914424777\n",
      "test loss:  210.37233791932698\n",
      "train MSE:  113.61266211501055\n",
      "test MSE:  3994560.25835254\n",
      "Epoch 23 / 500...\n",
      "train loss:  -2.390688450587309\n",
      "test loss:  223.5829824619135\n",
      "train MSE:  113.42850334473404\n",
      "test MSE:  3994579.9221979044\n",
      "Epoch 24 / 500...\n",
      "train loss:  -2.4853838623614832\n",
      "test loss:  237.09040807563662\n",
      "train MSE:  113.2489745834717\n",
      "test MSE:  3994603.221243052\n",
      "Epoch 25 / 500...\n",
      "train loss:  -2.577435147803222\n",
      "test loss:  250.8611789826837\n",
      "train MSE:  113.08065316121575\n",
      "test MSE:  3994627.5192423156\n",
      "Epoch 26 / 500...\n",
      "train loss:  -2.666762986095654\n",
      "test loss:  264.8782942241973\n",
      "train MSE:  112.9216542955356\n",
      "test MSE:  3994649.6591969943\n",
      "Epoch 27 / 500...\n",
      "train loss:  -2.754846569791671\n",
      "test loss:  279.15499406093716\n",
      "train MSE:  112.77679992409114\n",
      "test MSE:  3994673.959752862\n",
      "Epoch 28 / 500...\n",
      "train loss:  -2.838400368445104\n",
      "test loss:  293.6240346902556\n",
      "train MSE:  112.62795818112026\n",
      "test MSE:  3994702.3116344833\n",
      "Epoch 29 / 500...\n",
      "train loss:  -2.921029559126561\n",
      "test loss:  308.3000165404699\n",
      "train MSE:  112.49356068611256\n",
      "test MSE:  3994727.646510817\n",
      "Epoch 30 / 500...\n",
      "train loss:  -3.001415319080631\n",
      "test loss:  323.1817886142348\n",
      "train MSE:  112.36742690537702\n",
      "test MSE:  3994757.096182526\n",
      "Epoch 31 / 500...\n",
      "train loss:  -3.081148788639782\n",
      "test loss:  338.2709044245813\n",
      "train MSE:  112.24126724662185\n",
      "test MSE:  3994784.4611183205\n",
      "Epoch 32 / 500...\n",
      "train loss:  -3.1583581024677714\n",
      "test loss:  353.5282873678573\n",
      "train MSE:  112.12360204781396\n",
      "test MSE:  3994816.473300924\n",
      "Epoch 33 / 500...\n",
      "train loss:  -3.234661300508458\n",
      "test loss:  368.9725565151152\n",
      "train MSE:  112.01338540565952\n",
      "test MSE:  3994848.026883074\n",
      "Epoch 34 / 500...\n",
      "train loss:  -3.3079507532466152\n",
      "test loss:  384.5531871864159\n",
      "train MSE:  111.90729199463462\n",
      "test MSE:  3994879.27003157\n",
      "Epoch 35 / 500...\n",
      "train loss:  -3.3808980794521557\n",
      "test loss:  400.2953715075257\n",
      "train MSE:  111.79680794627723\n",
      "test MSE:  3994910.211345897\n",
      "Epoch 36 / 500...\n",
      "train loss:  -3.45427167695697\n",
      "test loss:  416.170520988783\n",
      "train MSE:  111.68918557559185\n",
      "test MSE:  3994939.5332261627\n",
      "Epoch 37 / 500...\n",
      "train loss:  -3.523991095471714\n",
      "test loss:  432.18865898695725\n",
      "train MSE:  111.58329271921917\n",
      "test MSE:  3994975.825724855\n",
      "Epoch 38 / 500...\n",
      "train loss:  -3.5924728379451474\n",
      "test loss:  448.32885788446305\n",
      "train MSE:  111.49374398875169\n",
      "test MSE:  3995006.536421917\n",
      "Epoch 39 / 500...\n",
      "train loss:  -3.660283083145039\n",
      "test loss:  464.59181512170636\n",
      "train MSE:  111.39939704214235\n",
      "test MSE:  3995039.524955937\n",
      "Epoch 40 / 500...\n",
      "train loss:  -3.7273525186052487\n",
      "test loss:  480.9855837192118\n",
      "train MSE:  111.30256580564745\n",
      "test MSE:  3995076.200228545\n",
      "Epoch 41 / 500...\n",
      "train loss:  -3.79319974944553\n",
      "test loss:  497.5239350262749\n",
      "train MSE:  111.21845145930466\n",
      "test MSE:  3995109.9339737752\n",
      "Epoch 42 / 500...\n",
      "train loss:  -3.8597445043644165\n",
      "test loss:  514.1524885459633\n",
      "train MSE:  111.13683434185727\n",
      "test MSE:  3995141.80004261\n",
      "Epoch 43 / 500...\n",
      "train loss:  -3.923171645499902\n",
      "test loss:  530.896019075875\n",
      "train MSE:  111.05559676869795\n",
      "test MSE:  3995178.865526525\n",
      "Epoch 44 / 500...\n",
      "train loss:  -3.9884582104228405\n",
      "test loss:  547.7154249644302\n",
      "train MSE:  110.9804771906822\n",
      "test MSE:  3995208.99091631\n",
      "Epoch 45 / 500...\n",
      "train loss:  -4.049984525630141\n",
      "test loss:  564.6311838917419\n",
      "train MSE:  110.90470634917212\n",
      "test MSE:  3995238.8022312177\n",
      "Epoch 46 / 500...\n",
      "train loss:  -4.110903541224875\n",
      "test loss:  581.6833641592305\n",
      "train MSE:  110.8260544214753\n",
      "test MSE:  3995273.398965738\n",
      "Epoch 47 / 500...\n",
      "train loss:  -4.173069198876584\n",
      "test loss:  598.7979160581872\n",
      "train MSE:  110.74696256744458\n",
      "test MSE:  3995309.631171196\n",
      "Epoch 48 / 500...\n",
      "train loss:  -4.232153229188072\n",
      "test loss:  615.9961863539964\n",
      "train MSE:  110.67906295690878\n",
      "test MSE:  3995342.6968487925\n",
      "Epoch 49 / 500...\n",
      "train loss:  -4.293334160142709\n",
      "test loss:  633.2995311432855\n",
      "train MSE:  110.61129704947446\n",
      "test MSE:  3995377.850245008\n",
      "Epoch 50 / 500...\n",
      "train loss:  -4.350852724345479\n",
      "test loss:  650.6956593088237\n",
      "train MSE:  110.53616668895052\n",
      "test MSE:  3995415.32561833\n",
      "Epoch 51 / 500...\n",
      "train loss:  -4.409094280345366\n",
      "test loss:  668.1602220909945\n",
      "train MSE:  110.46527385414285\n",
      "test MSE:  3995450.6474017547\n",
      "Epoch 52 / 500...\n",
      "train loss:  -4.466734150681051\n",
      "test loss:  685.7195905465156\n",
      "train MSE:  110.4005658660295\n",
      "test MSE:  3995484.567740311\n",
      "Epoch 53 / 500...\n",
      "train loss:  -4.5235730275539865\n",
      "test loss:  703.3328052319126\n",
      "train MSE:  110.32949404158597\n",
      "test MSE:  3995522.3565299916\n",
      "Epoch 54 / 500...\n",
      "train loss:  -4.580346421886681\n",
      "test loss:  721.0297472138952\n",
      "train MSE:  110.25272473981808\n",
      "test MSE:  3995560.9272917435\n",
      "Epoch 55 / 500...\n",
      "train loss:  -4.634933102442412\n",
      "test loss:  738.7959831751529\n",
      "train MSE:  110.18960393690939\n",
      "test MSE:  3995596.3163796943\n",
      "Epoch 56 / 500...\n",
      "train loss:  -4.690734374012288\n",
      "test loss:  756.6169979698102\n",
      "train MSE:  110.12286452425417\n",
      "test MSE:  3995633.880401309\n",
      "Epoch 57 / 500...\n",
      "train loss:  -4.745827820959461\n",
      "test loss:  774.4809546627847\n",
      "train MSE:  110.06606658414454\n",
      "test MSE:  3995671.9259166005\n",
      "Epoch 58 / 500...\n",
      "train loss:  -4.800318964738409\n",
      "test loss:  792.432115054503\n",
      "train MSE:  110.01173999491066\n",
      "test MSE:  3995707.1106118415\n",
      "Epoch 59 / 500...\n",
      "train loss:  -4.853677680942487\n",
      "test loss:  810.465997227678\n",
      "train MSE:  109.96084655628253\n",
      "test MSE:  3995742.581724158\n",
      "Epoch 60 / 500...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  -4.909123492084312\n",
      "test loss:  828.5621378793177\n",
      "train MSE:  109.90347067373425\n",
      "test MSE:  3995778.9856094206\n",
      "Epoch 61 / 500...\n",
      "train loss:  -4.960381520512827\n",
      "test loss:  846.6986582684137\n",
      "train MSE:  109.85090810283926\n",
      "test MSE:  3995815.710154752\n",
      "Epoch 62 / 500...\n",
      "train loss:  -5.012169506485266\n",
      "test loss:  864.8911834438592\n",
      "train MSE:  109.79365514088425\n",
      "test MSE:  3995850.0100521003\n",
      "Epoch 63 / 500...\n",
      "train loss:  -5.066376140006855\n",
      "test loss:  883.1427735690512\n",
      "train MSE:  109.74114432602855\n",
      "test MSE:  3995887.9656407973\n",
      "Epoch 64 / 500...\n",
      "train loss:  -5.114914726334454\n",
      "test loss:  901.4084163465808\n",
      "train MSE:  109.69539198730251\n",
      "test MSE:  3995925.1279851254\n",
      "Epoch 65 / 500...\n",
      "train loss:  -5.167321854645992\n",
      "test loss:  919.7771327903274\n",
      "train MSE:  109.64309897911212\n",
      "test MSE:  3995964.4189537293\n",
      "Epoch 66 / 500...\n",
      "train loss:  -5.221930958502773\n",
      "test loss:  938.1792237766643\n",
      "train MSE:  109.59635464963318\n",
      "test MSE:  3996002.9931242857\n",
      "Epoch 67 / 500...\n",
      "train loss:  -5.268405129173583\n",
      "test loss:  956.5866687302647\n",
      "train MSE:  109.54753145257638\n",
      "test MSE:  3996039.6519532837\n",
      "Epoch 68 / 500...\n",
      "train loss:  -5.319929004711103\n",
      "test loss:  975.0790345471673\n",
      "train MSE:  109.50362507925144\n",
      "test MSE:  3996075.2691018963\n",
      "Epoch 69 / 500...\n",
      "train loss:  -5.368285992241429\n",
      "test loss:  993.5535960745106\n",
      "train MSE:  109.4582242502922\n",
      "test MSE:  3996110.053940462\n",
      "Epoch 70 / 500...\n",
      "train loss:  -5.417853231425071\n",
      "test loss:  1012.1210487381985\n",
      "train MSE:  109.41947191587919\n",
      "test MSE:  3996145.851639519\n",
      "Epoch 71 / 500...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-d49ad994d7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my_test_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_sub_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-6edacaf83119>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, x_train, y_train_sm, x_test, y_test_sm)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# calc loss and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-6edacaf83119>\u001b[0m in \u001b[0;36mtrain_phase\u001b[0;34m(self, x_train, y_train_sm)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Randomize the data (using sklearn shuffle)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# get the next batch (loop through number of training samples, step by batch size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \"\"\"\n\u001b[1;32m    342\u001b[0m     \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========  Ridge Regression Training  =============\n",
    "\n",
    "feat_dims = x_train.shape[1]\n",
    "\n",
    "# create Regression() object to run training\n",
    "regr = Regression(feat_dims)\n",
    "\n",
    "# convert labels to floats\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# sub mean from y labels\n",
    "y_train_sm = regr.label_sub_mean(y_train)\n",
    "y_test_sm = regr.label_sub_mean(y_test)\n",
    "\n",
    "train_losses, test_losses, train_mse_arr, test_mse_arr, test_preds = regr.run_epochs(x_train, y_train_sm, x_test, y_test_sm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracies graphs\n",
    "\n",
    "regr.plot_graph(train_losses, test_losses, train_mse_arr, test_mse_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "regr.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
