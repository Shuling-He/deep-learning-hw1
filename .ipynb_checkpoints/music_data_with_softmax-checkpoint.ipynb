{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# part 5, 5 - softmax classifier on music data\n",
    "\n",
    "class SoftmaxClassifier:\n",
    "\n",
    "    def __init__(self, epochs, learning_rate, batch_size, regularization, momentum, num_classes):\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.regularization = regularization\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        self.weights = None\n",
    "    \n",
    "    def calc_mse(self, probs, yt_off):\n",
    "        \n",
    "#         print('Evaluation phase --')\n",
    "        \n",
    "#         f = np.dot(x, self.weights)\n",
    "#         pred = self.softmax(f)\n",
    "        \n",
    "#         print('pred shape: ', pred.shape)\n",
    "        \n",
    "        # make prediction\n",
    "#         pred = np.argmax(np.dot(x, self.weights), 1)  # predict\n",
    "\n",
    "        # make sure to floor by converting to int()\n",
    "#         diff = pred - yt_off + self.offset\n",
    "        preds = np.argmax(probs, 1).reshape(-1, 1)\n",
    "        diff = preds - yt_off.reshape(-1, 1)\n",
    "        mse = (np.square(diff)).mean()\n",
    "        \n",
    "#         print('pred shape: ', preds.shape)\n",
    "#         print('yt_off shape: ', yt_off.shape)\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # calc the softmax\n",
    "        exp_x = np.exp(x - np.max(x))  # make sure it doesn't blow up by sub max\n",
    "\n",
    "        # make sure sum along columns, and keep dims keeps the exact same dim when summing\n",
    "        # ie keep cols, instead of converting to rows\n",
    "        y = np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return exp_x / y\n",
    "\n",
    "    def loss_and_gradient(self, x, y_off, y_one_off):\n",
    "        # calc the loss and gradient.  forward prop, get softmax, calc the neg loss loss, and total loss.\n",
    "        # calc dW by taking the residual, then dot with X,  + regularization\n",
    "        # find average for both\n",
    "\n",
    "        n_samples = x.shape[0]  # num of examples\n",
    "\n",
    "        # forward prop\n",
    "        f = np.dot(x, self.weights)  # mult X by W\n",
    "        probs = self.softmax(f)  # pass f to softmax\n",
    "\n",
    "        # take neg log of the highest prob. for that row\n",
    "        neg_log_loss = -np.log(probs[np.arange(n_samples), np.argmax(probs, axis=1)])\n",
    "        loss = np.sum(neg_log_loss)  # sum to get total loss across all samples\n",
    "        # calc the regularization loss too\n",
    "        reg_loss = 0.5 * self.regularization * np.sum(self.weights * self.weights)\n",
    "        total_loss = (loss / n_samples) + reg_loss  # sum to get total, divide for avg\n",
    "\n",
    "        # calc derivative of loss (including regularization derivative)\n",
    "        dW = x.T.dot( (probs - y_one_off) ) + (self.regularization * self.weights) \n",
    "        dW /= n_samples  # compute average dW\n",
    "\n",
    "        return total_loss, dW, probs\n",
    "\n",
    "    def train_phase(self, x_train, y_train_off, y_train_one_off):\n",
    "        # shuffle data together, and forward prop by batch size, and add momentum\n",
    "\n",
    "        print('TRAINING PHASE --')\n",
    "        \n",
    "        num_train = x_train.shape[0]\n",
    "        losses = []\n",
    "        probs_arr = []\n",
    "\n",
    "        # Randomize the data\n",
    "        x_train, y_train_off, y_train_one_off = shuffle(x_train, y_train_off, y_train_one_off)\n",
    "\n",
    "        # get the next batch (loop through number of training samples, step by batch size)\n",
    "        for i in range(0, num_train, self.batch_size):\n",
    "\n",
    "            # grab the next batch size\n",
    "            x_train_batch = x_train[i:i + self.batch_size]\n",
    "            y_train_batch_off = y_train_off[i:i + self.batch_size]\n",
    "            y_train_batch_one_off = y_train_one_off[i:i + self.batch_size]\n",
    "\n",
    "            # forward prop\n",
    "            loss, dW, probs = self.loss_and_gradient(x_train_batch, y_train_batch_off, y_train_batch_one_off)  # calc loss and dW\n",
    "            \n",
    "            probs_arr.extend(probs)\n",
    "            \n",
    "            # calc velocity\n",
    "            self.velocity = (self.momentum * self.velocity) + (self.learning_rate * dW)\n",
    "            self.weights -= self.velocity  # update the weights\n",
    "            losses.append(loss)  # save the losses\n",
    "\n",
    "        return np.average(losses), np.asarray(probs_arr)  # return the average\n",
    "\n",
    "    def test_phase(self, x, y_test_off, y_test_one_off):\n",
    "        # extra, but more explicit calc of loss and gradient during testing (no back prop)\n",
    "\n",
    "        print('Test PHASE --')\n",
    "        \n",
    "        loss, _, probs = self.loss_and_gradient(x, y_test_off, y_test_one_off)  # calc loss and dW (don't need)\n",
    "        return loss, probs\n",
    "\n",
    "    def run_epochs(self, x_train, y_train_off, y_train_one_off, x_test, y_test_off, y_test_one_off):\n",
    "        # start the training/valid by looping through epochs\n",
    "\n",
    "        num_dim = x_train.shape[1]  # num of dimensions\n",
    "        \n",
    "        # create weights array/matrix size (num features x output)\n",
    "        self.weights = 0.0001 * np.random.rand(num_dim, self.num_classes)\n",
    "        self.velocity = np.zeros(self.weights.shape)\n",
    "\n",
    "        # store losses and accuracies here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_mse_arr = []\n",
    "        test_mse_arr = []\n",
    "\n",
    "        for e in range(self.epochs): # loop through epochs\n",
    "\n",
    "            print('Ephoch {} / {}...'.format(e + 1, self.epochs))\n",
    "\n",
    "            # calc loss and accuracies\n",
    "            train_loss, train_probs = self.train_phase(x_train, y_train_off, y_train_one_off)\n",
    "            test_loss, test_probs = self.test_phase(x_test, y_test_off, y_test_one_off)\n",
    "            train_mse = self.calc_mse(train_probs, y_train_off)\n",
    "            test_mse = self.calc_mse(test_probs, y_test_off)\n",
    "            \n",
    "            print('train loss: ', train_loss)\n",
    "            print('test loss: ', test_loss)\n",
    "            print('train MSE: ', train_mse)\n",
    "            print('test MSE: ', test_mse)\n",
    "\n",
    "            # append vals to lists\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_mse_arr.append(train_mse)\n",
    "            test_mse_arr.append(test_mse)\n",
    "\n",
    "        return train_losses, test_losses, train_mse_arr, test_mse_arr  # return all the vals\n",
    "\n",
    "    def plot_graph(self, train_losses, test_losses, train_mse_arr, test_mse_arr):\n",
    "        # plot graph\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train loss\")\n",
    "        plt.plot(test_losses, label=\"Test loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"Softmax Class. Loss vs Epochs\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss (Cross entropy)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_mse_arr, label=\"Train MSE\")\n",
    "        plt.plot(test_mse_arr, label=\"Test MSE\")\n",
    "        # plt.legend(loc='best')\n",
    "        plt.title(\"Softmax Class. MSE vs Epochs\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= preprocessing code ========================\n",
    "\n",
    "def normalize_feat(x, mean=None, std=None):\n",
    "    # normalize the feature data.  test data must pass mean and std\n",
    "\n",
    "    # calc feature-wise mean\n",
    "    if mean is None:\n",
    "        mean = np.mean(x, axis=0)\n",
    "\n",
    "    # calc feature-wise std\n",
    "    if std is None:\n",
    "        std = np.std(x, axis=0)\n",
    "\n",
    "    # sub the mean per column\n",
    "    x_norm = x - mean\n",
    "\n",
    "    # div by the standard dev.\n",
    "    x_norm = x_norm / std\n",
    "\n",
    "    return x_norm, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loader\n",
    "\n",
    "def load_data(fname, bias=1):\n",
    "\n",
    "    data = loadtxt(fname, delimiter=',')\n",
    "\n",
    "    # loads data, normalizes, and appends a bias vector to the data\n",
    "\n",
    "    TRAIN_NUM = 463714  # training data up to this point\n",
    "\n",
    "    # process training data\n",
    "    x_train = data[:TRAIN_NUM,1:].astype(float)  # parse train\n",
    "\n",
    "    x_train, train_mean, train_std = normalize_feat(x_train)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_train.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_train = np.hstack((x_train, col_bias))\n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_train = data[:TRAIN_NUM,0].astype(int)\n",
    "    y_train = y_train.reshape((-1, 1))\n",
    "\n",
    "    # -------------------\n",
    "\n",
    "    # process test data\n",
    "    x_test = data[TRAIN_NUM:,1:].astype(float)  # parse test\n",
    "    x_test, _, _ = normalize_feat(x_test, train_mean, train_std)  # normalize data\n",
    "\n",
    "    # create a col vector of ones\n",
    "    col_bias = np.ones((x_test.shape[0], 1))\n",
    "\n",
    "    # append bias with hstack\n",
    "    x_test = np.hstack((x_test, col_bias))    \n",
    "\n",
    "    # convert label vals to int and to vector\n",
    "    y_test = data[TRAIN_NUM:,0].astype(int)\n",
    "    y_test = y_test.reshape((-1, 1))  # convert to column vector\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_labels(y):\n",
    "    OFFSET = 1900 # starting the index 0 with year 1923\n",
    "    return y - OFFSET\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load data\n",
    "# fname = 'YearPredictionMSD.txt'\n",
    "\n",
    "# # # note, features are normalized\n",
    "# x_train, y_train, x_test, y_test = load_data(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset to make labels start at index 0\n",
    "y_train_off = offset_labels(y_train)\n",
    "y_test_off = offset_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot on y labels\n",
    "\n",
    "def one_hot_vary(y_train, y_test):\n",
    "\n",
    "    train_size = len(y_train)\n",
    "    test_size = len(y_test)\n",
    "\n",
    "    stacked = np.vstack((y_train, y_test))\n",
    "    one_h = OneHotEncoder().fit_transform(stacked).toarray()\n",
    "    \n",
    "    y_train = one_h[0:train_size, :]\n",
    "    y_test = one_h[train_size:, :]\n",
    "    return y_train, y_test  # new one hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# obtain one hots on the variable number of classes\n",
    "y_train_one_off, y_test_one_off = one_hot_vary(y_train_off, y_test_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of years represented by list indexes\n",
    "train_years_key = np.unique(y_train_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters here\n",
    "num_classes = y_train_one_off.shape[1]\n",
    "epochs = 100\n",
    "learning_rate = 0.00001  # [0.1, 0.01, 0.001]\n",
    "batch_size = 100  # try powers of 2\n",
    "regularization = 0.5  # L2 weight decay, range [1, 0.1, 0.01, 0.001]\n",
    "momentum = 0.05  # started with 0 to 1, tried 2\n",
    "\n",
    "smc = SoftmaxClassifier(epochs, learning_rate, batch_size, regularization, momentum, num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ephoch 1 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4877758097368385\n",
      "test loss:  4.4873277124072715\n",
      "train MSE:  1420.19596\n",
      "test MSE:  505.3695841645523\n",
      "Ephoch 2 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.486847791311746\n",
      "test loss:  4.486260266201404\n",
      "train MSE:  461.41105\n",
      "test MSE:  420.7141058666305\n",
      "Ephoch 3 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.485761764878209\n",
      "test loss:  4.4851591114354\n",
      "train MSE:  434.11365\n",
      "test MSE:  416.99980631790976\n",
      "Ephoch 4 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.484700989400915\n",
      "test loss:  4.484049886352825\n",
      "train MSE:  434.52605\n",
      "test MSE:  416.6447870465418\n",
      "Ephoch 5 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4836095135623975\n",
      "test loss:  4.482938990081306\n",
      "train MSE:  433.29852\n",
      "test MSE:  416.8707946776162\n",
      "Ephoch 6 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4825142273848195\n",
      "test loss:  4.481828495080987\n",
      "train MSE:  433.62426\n",
      "test MSE:  416.86661114446747\n",
      "Ephoch 7 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.481429590588074\n",
      "test loss:  4.480719629887891\n",
      "train MSE:  433.86737\n",
      "test MSE:  417.1279076523794\n",
      "Ephoch 8 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.480347726845772\n",
      "test loss:  4.479612950806804\n",
      "train MSE:  434.03204\n",
      "test MSE:  417.31512076078326\n",
      "Ephoch 9 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.479275615368866\n",
      "test loss:  4.478508833578119\n",
      "train MSE:  434.4424\n",
      "test MSE:  417.31895566616953\n",
      "Ephoch 10 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.478184759206999\n",
      "test loss:  4.477407321129186\n",
      "train MSE:  434.27026\n",
      "test MSE:  417.32931765799617\n",
      "Ephoch 11 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.477124406625375\n",
      "test loss:  4.476308446928922\n",
      "train MSE:  434.60546\n",
      "test MSE:  417.5362282349751\n",
      "Ephoch 12 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.476062222488221\n",
      "test loss:  4.475212622146993\n",
      "train MSE:  435.22818\n",
      "test MSE:  417.7010710619589\n",
      "Ephoch 13 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.474980016719994\n",
      "test loss:  4.47411976634259\n",
      "train MSE:  435.40836\n",
      "test MSE:  417.87980089481124\n",
      "Ephoch 14 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.473914585608847\n",
      "test loss:  4.473029903341439\n",
      "train MSE:  435.69035\n",
      "test MSE:  417.89204160291297\n",
      "Ephoch 15 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4728557866547325\n",
      "test loss:  4.471942963820649\n",
      "train MSE:  435.00839\n",
      "test MSE:  417.9439096666731\n",
      "Ephoch 16 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4717967059765416\n",
      "test loss:  4.4708591740137855\n",
      "train MSE:  435.11478\n",
      "test MSE:  417.97427901841917\n",
      "Ephoch 17 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.470726345013481\n",
      "test loss:  4.469778419661292\n",
      "train MSE:  435.53777\n",
      "test MSE:  418.0361604462435\n",
      "Ephoch 18 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.469689392332504\n",
      "test loss:  4.468700639797073\n",
      "train MSE:  436.3978\n",
      "test MSE:  418.12670682342\n",
      "Ephoch 19 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.468628028197591\n",
      "test loss:  4.46762601682525\n",
      "train MSE:  435.6463\n",
      "test MSE:  418.16573376459877\n",
      "Ephoch 20 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.467578323688558\n",
      "test loss:  4.466554467703729\n",
      "train MSE:  436.01501\n",
      "test MSE:  418.20700741802403\n",
      "Ephoch 21 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.466551828613481\n",
      "test loss:  4.465485857432661\n",
      "train MSE:  435.92834\n",
      "test MSE:  418.2108810598284\n",
      "Ephoch 22 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.465504808838416\n",
      "test loss:  4.464420495559351\n",
      "train MSE:  435.11731\n",
      "test MSE:  418.2557572001317\n",
      "Ephoch 23 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.464449286251834\n",
      "test loss:  4.463358215120205\n",
      "train MSE:  435.62838\n",
      "test MSE:  418.2777982219984\n",
      "Ephoch 24 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4634297478585285\n",
      "test loss:  4.462298937035491\n",
      "train MSE:  435.42528\n",
      "test MSE:  418.25413027057385\n",
      "Ephoch 25 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.462402961003396\n",
      "test loss:  4.461242852236499\n",
      "train MSE:  435.24128\n",
      "test MSE:  418.26311711956\n",
      "Ephoch 26 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.461365048338922\n",
      "test loss:  4.460189862028897\n",
      "train MSE:  435.12524\n",
      "test MSE:  418.2652088861343\n",
      "Ephoch 27 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.460347787183292\n",
      "test loss:  4.459139925072349\n",
      "train MSE:  435.48509\n",
      "test MSE:  418.26210997269084\n",
      "Ephoch 28 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.4593249903309005\n",
      "test loss:  4.458093085933512\n",
      "train MSE:  435.31509\n",
      "test MSE:  418.23662140961824\n",
      "Ephoch 29 / 100...\n",
      "TRAINING PHASE --\n",
      "Test PHASE --\n",
      "train loss:  4.458296351157188\n",
      "test loss:  4.457049277793184\n",
      "train MSE:  435.5152\n",
      "test MSE:  418.24589878173964\n",
      "Ephoch 30 / 100...\n",
      "TRAINING PHASE --\n"
     ]
    }
   ],
   "source": [
    "subsample = 100000\n",
    "train_losses, test_losses, train_mse, test_mse = smc.run_epochs(x_train[:subsample], y_train_off[:subsample], y_train_one_off[:subsample], x_test[:subsample], y_test_off[:subsample], y_test_one_off[:subsample])\n",
    "smc.plot_graph(train_losses, test_losses, train_mse, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_off[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
